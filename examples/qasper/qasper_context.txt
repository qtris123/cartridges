Below is a panel of scientific papers.

<paper>
<title>Question Answering based Clinical Text Structuring Using Pre-trained Language Model</title>
<abstract>Clinical text structuring is a critical and fundamental task for clinical research. Traditional methods such as taskspecific end-to-end models and pipeline models usually suffer from the lack of dataset and error propagation. In this paper, we present a question answering based clinical text structuring (QA-CTS) task to unify different specific tasks and make dataset shareable. A novel model that aims to introduce domain-specific features (e.g., clinical named entity information) into pre-trained language model is also proposed for QA-CTS task. Experimental results on Chinese pathology reports collected from Ruijing Hospital demonstrate our presented QA-CTS task is very effective to improve the performance on specific tasks. Our proposed model also competes favorably with strong baseline models in specific tasks.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches.

However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor cost. To handle it, researchers turn to some rule-based structuring methods which often have lower labor cost.

Traditionally, CTS tasks can be addressed by rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2, task-specific end-to-end methods BIBREF3, BIBREF4, BIBREF5, BIBREF6 and pipeline methods BIBREF7, BIBREF8, BIBREF9. Rule and dictionary based methods suffer from costly human-designed extraction rules, while task-specific end-to-end methods have non-uniform output formats and require task-specific training dataset. Pipeline methods break down the entire process into several pieces which improves the performance and generality. However, when the pipeline depth grows, error propagation will have a greater impact on the performance.

To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.

We first present a question answering based clinical text structuring (QA-CTS) task, which unifies different specific tasks and make dataset shareable. We also propose an effective model to integrate clinical named entity information into pre-trained language model.

Experimental results show that QA-CTS task leads to significant improvement due to shared dataset. Our proposed model also achieves significantly better performance than the strong baseline methods. In addition, we also show that two-stage training mechanism has a great improvement on QA-CTS task.

The rest of the paper is organized as follows. We briefly review the related work on clinical text structuring in Section SECREF2. Then, we present question answer based clinical text structuring task in Section SECREF3. In Section SECREF4, we present an effective model for this task. Section SECREF5 is devoted to computational studies and several investigations on the key issues of our proposed model. Finally, conclusions are given in Section SECREF6.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Related Work ::: Clinical Text Structuring</section-title>
<section-number>1</section-number>
<paragraphs>
Clinical text structuring is a final problem which is highly related to practical applications. Most of existing studies are case-by-case. Few of them are developed for the general purpose structuring task. These studies can be roughly divided into three categories: rule and dictionary based methods, task-specific end-to-end methods and pipeline methods.

Rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2 rely extremely on heuristics and handcrafted extraction rules which is more of an art than a science and incurring extensive trial-and-error experiments. Fukuda et al. BIBREF0 identified protein names from biological papers by dictionaries and several features of protein names. Wang et al. BIBREF1 developed some linguistic rules (i.e. normalised/expanded term matching and substring term matching) to map specific terminology to SNOMED CT. Song et al. BIBREF2 proposed a hybrid dictionary-based bio-entity extraction technique and expands the bio-entity dictionary by combining different data sources and improves the recall rate through the shortest path edit distance algorithm. This kind of approach features its interpretability and easy modifiability. However, with the increase of the rule amount, supplementing new rules to existing system will turn to be a rule disaster.

Task-specific end-to-end methods BIBREF3, BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 constructed an automated wound information identification model with five output. Tan et al. BIBREF4 identified patients undergoing radical cystectomy for bladder cancer. Although they achieved good performance, none of their models could be used to another task due to output format difference. This makes building a new model for a new task a costly job.

Pipeline methods BIBREF7, BIBREF8, BIBREF9 break down the entire task into several basic natural language processing tasks. Bill et al. BIBREF7 focused on attributes extraction which mainly relied on dependency parsing and named entity recognition BIBREF10, BIBREF11, BIBREF12. Meanwhile, Fonferko et al. BIBREF9 used more components like noun phrase chunking BIBREF13, BIBREF14, BIBREF15, part-of-speech tagging BIBREF16, BIBREF17, BIBREF18, sentence splitter, named entity linking BIBREF19, BIBREF20, BIBREF21, relation extraction BIBREF22, BIBREF23. This kind of method focus on language itself, so it can handle tasks more general. However, as the depth of pipeline grows, it is obvious that error propagation will be more and more serious. In contrary, using less components to decrease the pipeline depth will lead to a poor performance. So the upper limit of this method depends mainly on the worst component.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Related Work ::: Pre-trained Language Model</section-title>
<section-number>2</section-number>
<paragraphs>
Recently, some works focused on pre-trained language representation models to capture language information from text and then utilizing the information to improve the performance of specific natural language processing tasks BIBREF24, BIBREF25, BIBREF26, BIBREF27 which makes language model a shared model to all natural language processing tasks. Radford et al. BIBREF24 proposed a framework for fine-tuning pre-trained language model. Peters et al. BIBREF25 proposed ELMo which concatenates forward and backward language models in a shallow manner. Devlin et al. BIBREF26 used bidirectional Transformers to model deep interactions between the two directions. Yang et al. BIBREF27 replaced the fixed forward or backward factorization order with all possible permutations of the factorization order and avoided using the [MASK] tag which causes pretrain-finetune discrepancy that BERT is subject to.

The main motivation of introducing pre-trained language model is to solve the shortage of labeled data and polysemy problem. Although polysemy problem is not a common phenomenon in biomedical domain, shortage of labeled data is always a non-trivial problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved improvement on biomedical named entity recognition, relation extraction and question answering. Kim et al. BIBREF29 adapted BioBERT into multi-type named entity recognition and discovered new entities. Both of them demonstrates the usefulness of introducing pre-trained language model into biomedical domain.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Question Answering based Clinical Text Structuring</section-title>
<section-number>3</section-number>
<paragraphs>
Given a sequence of paragraph text $X=<x_1, x_2, ..., x_n>$, clinical text structuring (CTS) can be regarded to extract or generate a key-value pair where key $Q$ is typically a query term such as proximal resection margin and value $V$ is a result of query term $Q$ according to the paragraph text $X$.

Generally, researchers solve CTS problem in two steps. Firstly, the answer-related text is pick out. And then several steps such as entity names conversion and negative words recognition are deployed to generate the final answer. While final answer varies from task to task, which truly causes non-uniform output formats, finding the answer-related text is a common action among all tasks. Traditional methods regard both the steps as a whole. In this paper, we focus on finding the answer-related substring $Xs = <X_i, X_i+1, X_i+2, ... X_j> (1 <= i < j <= n)$ from paragraph text $X$. For example, given sentence UTF8gkai“远端胃切除标本：小弯长11.5cm，大弯长17.0cm。距上切端6.0cm、下切端8.0cm" (Distal gastrectomy specimen: measuring 11.5cm in length along the lesser curvature, 17.0cm in length along the greater curvature; 6.0cm from the proximal resection margin, and 8.0cm from the distal resection margin) and query UTF8gkai“上切缘距离"(proximal resection margin), the answer should be 6.0cm which is located in original text from index 32 to 37. With such definition, it unifies the output format of CTS tasks and therefore make the training data shareable, in order to reduce the training data quantity requirement.

Since BERT BIBREF26 has already demonstrated the usefulness of shared model, we suppose extracting commonality of this problem and unifying the output format will make the model more powerful than dedicated model and meanwhile, for a specific clinical task, use the data for other tasks to supplement the training data.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>The Proposed Model for QA-CTS Task</section-title>
<section-number>4</section-number>
<paragraphs>
In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>The Proposed Model for QA-CTS Task ::: Contextualized Representation of Sentence Text and Query Text</section-title>
<section-number>5</section-number>
<paragraphs>
For any clinical free-text paragraph $X$ and query $Q$, contextualized representation is to generate the encoded vector of both of them. Here we use pre-trained language model BERT-base BIBREF26 model to capture contextual information.

The text input is constructed as `[CLS] $Q$ [SEP] $X$ [SEP]'. For Chinese sentence, each word in this input will be mapped to a pre-trained embedding $e_i$. To tell the model $Q$ and $X$ is two different sentence, a sentence type input is generated which is a binary label sequence to denote what sentence each character in the input belongs to. Positional encoding and mask matrix is also constructed automatically to bring in absolute position information and eliminate the impact of zero padding respectively. Then a hidden vector $V_s$ which contains both query and text information is generated through BERT-base model.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>The Proposed Model for QA-CTS Task ::: Clinical Named Entity Information</section-title>
<section-number>6</section-number>
<paragraphs>
Since BERT is trained on general corpus, its performance on biomedical domain can be improved by introducing biomedical domain-specific features. In this paper, we introduce clinical named entity information into the model.

The CNER task aims to identify and classify important clinical terms such as diseases, symptoms, treatments, exams, and body parts from Chinese EHRs. It can be regarded as a sequence labeling task. A CNER model typically outputs a sequence of tags. Each character of the original sentence will be tagged a label following a tag scheme. In this paper we recognize the entities by the model of our previous work BIBREF12 but trained on another corpus which has 44 entity types including operations, numbers, unit words, examinations, symptoms, negative words, etc. An illustrative example of named entity information sequence is demonstrated in Table TABREF2. In Table TABREF2, UTF8gkai“远端胃切除" is tagged as an operation, `11.5' is a number word and `cm' is an unit word. The named entity tag sequence is organized in one-hot type. We denote the sequence for clinical sentence and query term as $I_{nt}$ and $I_{nq}$, respectively.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>The Proposed Model for QA-CTS Task ::: Integration Method</section-title>
<section-number>7</section-number>
<paragraphs>
There are two ways to integrate two named entity information vectors $I_{nt}$ and $I_{nq}$ or hidden contextualized representation $V_s$ and named entity information $I_n$, where $I_n = [I_{nt}; I_{nq}]$. The first one is to concatenate them together because they have sequence output with a common dimension. The second one is to transform them into a new hidden representation. For the concatenation method, the integrated representation is described as follows.

While for the transformation method, we use multi-head attention BIBREF30 to encode the two vectors. It can be defined as follows where $h$ is the number of heads and $W_o$ is used to projects back the dimension of concatenated matrix.

$Attention$ denotes the traditional attention and it can be defined as follows.

where $d_k$ is the length of hidden vector.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>The Proposed Model for QA-CTS Task ::: Final Prediction</section-title>
<section-number>8</section-number>
<paragraphs>
The final step is to use integrated representation $H_i$ to predict the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word. We use a feed forward network (FFN) to compress and calculate the score of each word $H_f$ which makes the dimension to $\left\langle l_s, 2\right\rangle $ where $l_s$ denotes the length of sequence.

Then we permute the two dimensions for softmax calculation. The calculation process of loss function can be defined as followed.

where $O_s = softmax(permute(H_f)_0)$ denotes the probability score of each word to be the start word and similarly $O_e = softmax(permute(H_f)_1)$ denotes the end. $y_s$ and $y_e$ denotes the true answer of the output for start word and end word respectively.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>The Proposed Model for QA-CTS Task ::: Two-Stage Training Mechanism</section-title>
<section-number>9</section-number>
<paragraphs>
Two-stage training mechanism is previously applied on bilinear model in fine-grained visual recognition BIBREF31, BIBREF32, BIBREF33. Two CNNs are deployed in the model. One is trained at first for coarse-graind features while freezing the parameter of the other. Then unfreeze the other one and train the entire model in a low learning rate for fetching fine-grained features.

Inspired by this and due to the large amount of parameters in BERT model, to speed up the training process, we fine tune the BERT model with new prediction layer first to achieve a better contextualized representation performance. Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Experimental Studies</section-title>
<section-number>10</section-number>
<paragraphs>
In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Experimental Studies ::: Dataset and Evaluation Metrics</section-title>
<section-number>11</section-number>
<paragraphs>
Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.

In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Experimental Studies ::: Experimental Settings</section-title>
<section-number>12</section-number>
<paragraphs>
To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU. The models are trained by Adam optimization algorithm BIBREF38 whose parameters are the same as the default settings except for learning rate set to $5\times 10^{-5}$. Batch size is set to 3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in this paper. Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Experimental Studies ::: Comparison with State-of-the-art Methods</section-title>
<section-number>13</section-number>
<paragraphs>
Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23.

Table TABREF23 indicates that our proposed model achieved the best performance both in EM-score and F$_1$-score with EM-score of 91.84% and F$_1$-score of 93.75%. QANet outperformed BERT-Base with 3.56% score in F$_1$-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F$_1$-score. Although our model didn't outperform much with QANet in F$_1$-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Experimental Studies ::: Ablation Analysis</section-title>
<section-number>14</section-number>
<paragraphs>
To further investigate the effects of named entity information and two-stage training mechanism for our model, we apply ablation analysis to see the improvement brought by each of them, where $\times $ refers to removing that part from our model.

As demonstrated in Table TABREF25, with named entity information enabled, two-stage training mechanism improved the result by 4.36% in EM-score and 3.8% in F$_1$-score. Without two-stage training mechanism, named entity information led to an improvement by 1.28% in EM-score but it also led to a weak deterioration by 0.12% in F$_1$-score. With both of them enabled, our proposed model achieved a 5.64% score improvement in EM-score and a 3.69% score improvement in F$_1$-score. The experimental results show that both named entity information and two-stage training mechanism are helpful to our model.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Experimental Studies ::: Comparisons Between Two Integration Methods</section-title>
<section-number>15</section-number>
<paragraphs>
There are two methods to integrate named entity information into existing model, we experimentally compare these two integration methods. As named entity recognition has been applied on both pathology report text and query text, there will be two integration here. One is for two named entity information and the other is for contextualized representation and integrated named entity information. For multi-head attention BIBREF30, we set heads number $h = 16$ with 256-dimension hidden vector size for each head.

From Table TABREF27, we can observe that applying concatenation on both periods achieved the best performance on both EM-score and F$_1$-score. Unfortunately, applying multi-head attention on both period one and period two can not reach convergence in our experiments. This probably because it makes the model too complex to train. The difference on other two methods are the order of concatenation and multi-head attention. Applying multi-head attention on two named entity information $I_{nt}$ and $I_{nq}$ first achieved a better performance with 89.87% in EM-score and 92.88% in F$_1$-score. Applying Concatenation first can only achieve 80.74% in EM-score and 84.42% in F$_1$-score. This is probably due to the processing depth of hidden vectors and dataset size. BERT's output has been modified after many layers but named entity information representation is very close to input. With big amount of parameters in multi-head attention, it requires massive training to find out the optimal parameters. However, our dataset is significantly smaller than what pre-trained BERT uses. This probably can also explain why applying multi-head attention method on both periods can not converge.

Although Table TABREF27 shows the best integration method is concatenation, multi-head attention still has great potential. Due to the lack of computational resources, our experiment fixed the head number and hidden vector size. However, tuning these hyper parameters may have impact on the result. Tuning integration method and try to utilize larger datasets may give help to improving the performance.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Experimental Studies ::: Data Integration Analysis</section-title>
<section-number>16</section-number>
<paragraphs>
To investigate how shared task and shared model can benefit, we split our dataset by query types, train our proposed model with different datasets and demonstrate their performance on different datasets. Firstly, we investigate the performance on model without two-stage training and named entity information.

As indicated in Table TABREF30, The model trained by mixed data outperforms 2 of the 3 original tasks in EM-score with 81.55% for proximal resection margin and 86.85% for distal resection margin. The performance on tumor size declined by 1.57% score in EM-score and 3.14% score in F$_1$-score but they were still above 90%. 0.69% and 0.37% score improvement in EM-score was brought by shared model for proximal and distal resection margin prediction. Meanwhile F$_1$-score for those two tasks declined 3.11% and 0.77% score.

Then we investigate the performance on model with two-stage training and named entity information. In this experiment, pre-training process only use the specific dataset not the mixed data. From Table TABREF31 we can observe that the performance on proximal and distal resection margin achieved the best performance on both EM-score and F$_1$-score. Compared with Table TABREF30, the best performance on proximal resection margin improved by 6.9% in EM-score and 7.94% in F$_1$-score. Meanwhile, the best performance on distal resection margin improved by 5.56% in EM-score and 6.32% in F$_1$-score. Other performances also usually improved a lot. This proves the usefulness of two-stage training and named entity information as well.

Lastly, we fine tune the model for each task with a pre-trained parameter. Table TABREF32 summarizes the result. (Add some explanations for the Table TABREF32). Comparing Table TABREF32 with Table TABREF31, using mixed-data pre-trained parameters can significantly improve the model performance than task-specific data trained model. Except tumor size, the result was improved by 0.52% score in EM-score, 1.39% score in F$_1$-score for proximal resection margin and 2.6% score in EM-score, 2.96% score in F$_1$-score for distal resection margin. This proves mixed-data pre-trained parameters can lead to a great benefit for specific task. Meanwhile, the model performance on other tasks which are not trained in the final stage was also improved from around 0 to 60 or 70 percent. This proves that there is commonality between different tasks and our proposed QA-CTS task make this learnable. In conclusion, to achieve the best performance for a specific dataset, pre-training the model in multiple datasets and then fine tuning the model on the specific dataset is the best way.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Conclusion</section-title>
<section-number>17</section-number>
<paragraphs>
In this paper, we present a question answering based clinical text structuring (QA-CTS) task, which unifies different clinical text structuring tasks and utilize different datasets. A novel model is also proposed to integrate named entity information into a pre-trained language model and adapt it to QA-CTS task. Initially, sequential results of named entity recognition on both paragraph and query texts are integrated together. Contextualized representation on both paragraph and query texts are transformed by a pre-trained language model. Then, the integrated named entity information and contextualized representation are integrated together and fed into a feed forward network for final prediction. Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks. The shared task and shared model introduced by QA-CTS task has also been proved to be useful for improving the performance on most of the task-specific datasets. In conclusion, the best way to achieve the best performance for a specific dataset is to pre-train the model in multiple datasets and then fine tune it on the specific dataset.
</paragraphs>
</section>

---Paper Title: Question Answering based Clinical Text Structuring Using Pre-trained Language Model---
<section>
<section-title>Acknowledgment</section-title>
<section-number>18</section-number>
<paragraphs>
We would like to thank Ting Li and Xizhou Hong (Ruijin Hospital) who have helped us very much in data fetching and data cleansing. This work is supported by the National Key R&D Program of China for “Precision Medical Research" (No. 2018YFC0910500).
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b</title>
<abstract>Task B Phase B of the 2019 BioASQ challenge focuses on biomedical question answering. Macquarie University's participation applies query-based multi-document extractive summarisation techniques to generate a multi-sentence answer given the question and the set of relevant snippets. In past participation we explored the use of regression approaches using deep learning architectures and a simple policy gradient architecture. For the 2019 challenge we experiment with the use of classification approaches with and without reinforcement learning. In addition, we conduct a correlation analysis between various ROUGE metrics and the BioASQ human evaluation scores.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
The BioASQ Challenge includes a question answering task (Phase B, part B) where the aim is to find the “ideal answer” — that is, an answer that would normally be given by a person BIBREF0. This is in contrast with most other question answering challenges where the aim is normally to give an exact answer, usually a fact-based answer or a list. Given that the answer is based on an input that consists of a biomedical question and several relevant PubMed abstracts, the task can be seen as an instance of query-based multi-document summarisation.

As in past participation BIBREF1, BIBREF2, we wanted to test the use of deep learning and reinforcement learning approaches for extractive summarisation. In contrast with past years where the training procedure was based on a regression set up, this year we experiment with various classification set ups. The main contributions of this paper are:

We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels.

We conduct correlation analysis between various ROUGE evaluation metrics and the human evaluations conducted at BioASQ and show that Precision and F1 correlate better than Recall.

Section SECREF2 briefly introduces some related work for context. Section SECREF3 describes our classification and regression experiments. Section SECREF4 details our experiments using deep learning architectures. Section SECREF5 explains the reinforcement learning approaches. Section SECREF6 shows the results of our correlation analysis between ROUGE scores and human annotations. Section SECREF7 lists the specific runs submitted at BioASQ 7b. Finally, Section SECREF8 concludes the paper.
</paragraphs>
</section>

---Paper Title: Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b---
<section>
<section-title>Related Work</section-title>
<section-number>1</section-number>
<paragraphs>
The BioASQ challenge has organised annual challenges on biomedical semantic indexing and question answering since 2013 BIBREF0. Every year there has been a task about semantic indexing (task a) and another about question answering (task b), and occasionally there have been additional tasks. The tasks defined for 2019 are:

Large Scale Online Biomedical Semantic Indexing.

Biomedical Semantic QA involving Information Retrieval (IR), Question Answering (QA), and Summarisation.

Medical Semantic Indexing in Spanish.

BioASQ Task 7b consists of two phases. Phase A provides a biomedical question as an input, and participants are expected to find relevant concepts from designated terminologies and ontologies, relevant articles from PubMed, relevant snippets from the relevant articles, and relevant RDF triples from designated ontologies. Phase B provides a biomedical question and a list of relevant articles and snippets, and participant systems are expected to return the exact answers and the ideal answers. The training data is composed of the test data from all previous years, and amounts to 2,747 samples. There has been considerable research on the use of machine learning approaches for tasks related to text summarisation, especially on single-document summarisation. Abstractive approaches normally use an encoder-decoder architecture and variants of this architecture incorporate attention BIBREF3 and pointer-generator BIBREF4. Recent approaches leveraged the use of pre-trained models BIBREF5. Recent extractive approaches to summarisation incorporate recurrent neural networks that model sequences of sentence extractions BIBREF6 and may incorporate an abstractive component and reinforcement learning during the training stage BIBREF7. But relatively few approaches have been proposed for query-based multi-document summarisation. Table TABREF8 summarises the approaches presented in the proceedings of the 2018 BioASQ challenge.
</paragraphs>
</section>

---Paper Title: Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b---
<section>
<section-title>Classification vs. Regression Experiments</section-title>
<section-number>2</section-number>
<paragraphs>
Our past participation in BioASQ BIBREF1, BIBREF2 and this paper focus on extractive approaches to summarisation. Our decision to focus on extractive approaches is based on the observation that a relatively large number of sentences from the input snippets has very high ROUGE scores, thus suggesting that human annotators had a general tendency to copy text from the input to generate the target summaries BIBREF1. Our past participating systems used regression approaches using the following framework:

Train the regressor to predict the ROUGE-SU4 F1 score of the input sentence.

Produce a summary by selecting the top $n$ input sentences.

A novelty in the current participation is the introduction of classification approaches using the following framework.

Train the classifier to predict the target label (“summary” or “not summary”) of the input sentence.

Produce a summary by selecting all sentences predicted as “summary”.

If the total number of sentences selected is less than $n$, select $n$ sentences with higher probability of label “summary”.

Introducing a classifier makes labelling the training data not trivial, since the target summaries are human-generated and they do not have a perfect mapping to the input sentences. In addition, some samples have multiple reference summaries. BIBREF11 showed that different data labelling approaches influence the quality of the final summary, and some labelling approaches may lead to better results than using regression. In this paper we experiment with the following labelling approaches:

: Label as “summary” all sentences from the input text that have a ROUGE score above a threshold $t$.

: Label as “summary” the $m$ input text sentences with highest ROUGE score.

As in BIBREF11, The ROUGE score of an input sentence was the ROUGE-SU4 F1 score of the sentence against the set of reference summaries.

We conducted cross-validation experiments using various values of $t$ and $m$. Table TABREF26 shows the results for the best values of $t$ and $m$ obtained. The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. To enable a fair comparison we used the same input features in all systems. These input features combine information from the question and the input sentence and are shown in Fig. FIGREF16. The features are based on BIBREF12, and are the same as in BIBREF1, plus the addition of the position of the input snippet. The best SVC and SVR parameters were determined by grid search.

Preliminary experiments showed a relatively high number of cases where the classifier did not classify any of the input sentences as “summary”. To solve this problem, and as mentioned above, the summariser used in Table TABREF26 introduces a backoff step that extracts the $n$ sentences with highest predicted values when the summary has less than $n$ sentences. The value of $n$ is as reported in our prior work and shown in Table TABREF25.

The results confirm BIBREF11's finding that classification outperforms regression. However, the actual choice of optimal labelling scheme was different: whereas in BIBREF11 the optimal labelling was based on a labelling threshold of 0.1, our experiments show a better result when using the top 5 sentences as the target summary. The reason for this difference might be the fact that BIBREF11 used all sentences from the abstracts of the relevant PubMed articles, whereas we use only the snippets as the input to our summariser. Consequently, the number of input sentences is now much smaller. We therefore report the results of using the labelling schema of top 5 snippets in all subsequent classifier-based experiments of this paper.

barchart=[fill=black!20,draw=black] errorbar=[very thin,draw=black!75] sscale=[very thin,draw=black!75]
</paragraphs>
</section>

---Paper Title: Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b---
<section>
<section-title>Deep Learning Models</section-title>
<section-number>3</section-number>
<paragraphs>
Based on the findings of Section SECREF3, we apply minimal changes to the deep learning regression models of BIBREF2 to convert them to classification models. In particular, we add a sigmoid activation to the final layer, and use cross-entropy as the loss function. The complete architecture is shown in Fig. FIGREF28.

The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC). The neural classifier is trained in two set ups: “NNC top 5” uses classification labels as described in Section SECREF3, and “NNC SU4 F1” uses the regression labels, that is, the ROUGE-SU4 F1 scores of each sentence. Of interest is the fact that “NNC SU4 F1” outperforms the neural regressor. We have not explored this further and we presume that the relatively good results are due to the fact that ROUGE values range between 0 and 1, which matches the full range of probability values that can be returned by the sigmoid activation of the classifier final layer.

Table TABREF26 also shows the standard deviation across the cross-validation folds. Whereas this standard deviation is fairly large compared with the differences in results, in general the results are compatible with the top part of the table and prior work suggesting that classification-based approaches improve over regression-based approaches.
</paragraphs>
</section>

---Paper Title: Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b---
<section>
<section-title>Reinforcement Learning</section-title>
<section-number>4</section-number>
<paragraphs>
We also experiment with the use of reinforcement learning techniques. Again these experiments are based on BIBREF2, who uses REINFORCE to train a global policy. The policy predictor uses a simple feedforward network with a hidden layer.

The results reported by BIBREF2 used ROUGE Recall and indicated no improvement with respect to deep learning architectures. Human evaluation results are preferable over ROUGE but these were made available after the publication of the paper. When comparing the ROUGE and human evaluation results (Table TABREF29), we observe an inversion of the results. In particular, the reinforcement learning approaches (RL) of BIBREF2 receive good human evaluation results, and as a matter of fact they are the best of our runs in two of the batches. In contrast, the regression systems (NNR) fare relatively poorly. Section SECREF6 expands on the comparison between the ROUGE and human evaluation scores.

Encouraged by the results of Table TABREF29, we decided to continue with our experiments with reinforcement learning. We use the same features as in BIBREF2, namely the length (in number of sentences) of the summary generated so far, plus the $tf.idf$ vectors of the following:

Candidate sentence;

Entire input to summarise;

Summary generated so far;

Candidate sentences that are yet to be processed; and

Question.

The reward used by REINFORCE is the ROUGE value of the summary generated by the system. Since BIBREF2 observed a difference between the ROUGE values of the Python implementation of ROUGE and the original Perl version (partly because the Python implementation does not include ROUGE-SU4), we compare the performance of our system when trained with each of them. Table TABREF35 summarises some of our experiments. We ran the version trained on Python ROUGE once, and the version trained on Perl twice. The two Perl runs have different results, and one of them clearly outperforms the Python run. However, given the differences of results between the two Perl runs we advice to re-run the experiments multiple times and obtain the mean and standard deviation of the runs before concluding whether there is any statistical difference between the results. But it seems that there may be an improvement of the final evaluation results when training on the Perl ROUGE values, presumably because the final evaluation results are measured using the Perl implementation of ROUGE.

We have also tested the use of word embeddings instead of $tf.idf$ as input features to the policy model, while keeping the same neural architecture for the policy (one hidden layer using the same number of hidden nodes). In particular, we use the mean of word embeddings using 100 and 200 dimensions. These word embeddings were pre-trained using word2vec on PubMed documents provided by the organisers of BioASQ, as we did for the architectures described in previous sections. The results, not shown in the paper, indicated no major improvement, and re-runs of the experiments showed different results on different runs. Consequently, our submission to BioASQ included the original system using $tf.idf$ as input features in all batches but batch 2, as described in Section SECREF7.
</paragraphs>
</section>

---Paper Title: Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b---
<section>
<section-title>Evaluation Correlation Analysis</section-title>
<section-number>5</section-number>
<paragraphs>
As mentioned in Section SECREF5, there appears to be a large discrepancy between ROUGE Recall and the human evaluations. This section describes a correlation analysis between human and ROUGE evaluations using the runs of all participants to all previous BioASQ challenges that included human evaluations (Phase B, ideal answers). The human evaluation results were scraped from the BioASQ Results page, and the ROUGE results were kindly provided by the organisers. We compute the correlation of each of the ROUGE metrics (recall, precision, F1 for ROUGE-2 and ROUGE-SU4) against the average of the human scores. The correlation metrics are Pearson, Kendall, and a revised Kendall correlation explained below.

The Pearson correlation between two variables is computed as the covariance of the two variables divided by the product of their standard deviations. This correlation is a good indication of a linear relation between the two variables, but may not be very effective when there is non-linear correlation.

The Spearman rank correlation and the Kendall rank correlation are two of the most popular among metrics that aim to detect non-linear correlations. The Spearman rank correlation between two variables can be computed as the Pearson correlation between the rank values of the two variables, whereas the Kendall rank correlation measures the ordinal association between the two variables using Equation DISPLAY_FORM36.

It is useful to account for the fact that the results are from 28 independent sets (3 batches in BioASQ 1 and 5 batches each year between BioASQ 2 and BioASQ 6). We therefore also compute a revised Kendall rank correlation measure that only considers pairs of variable values within the same set. The revised metric is computed using Equation DISPLAY_FORM37, where $S$ is the list of different sets.

Table TABREF38 shows the results of all correlation metrics. Overall, ROUGE-2 and ROUGE-SU4 give similar correlation values but ROUGE-SU4 is marginally better. Among precision, recall and F1, both precision and F1 are similar, but precision gives a better correlation. Recall shows poor correlation, and virtually no correlation when using the revised Kendall measure. For reporting the evaluation of results, it will be therefore more useful to use precision or F1. However, given the small difference between precision and F1, and given that precision may favour short summaries when used as a function to optimise in a machine learning setting (e.g. using reinforcement learning), it may be best to use F1 as the metric to optimise.

Fig. FIGREF40 shows the scatterplots of ROUGE-SU4 recall, precision and F1 with respect to the average human evaluation. We observe that the relation between ROUGE and the human evaluations is not linear, and that Precision and F1 have a clear correlation.
</paragraphs>
</section>

---Paper Title: Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b---
<section>
<section-title>Submitted Runs</section-title>
<section-number>6</section-number>
<paragraphs>
Table TABREF41 shows the results and details of the runs submitted to BioASQ. The table uses ROUGE-SU4 Recall since this is the metric available at the time of writing this paper. However, note that, as explained in Section SECREF6, these results might differ from the final human evaluation results. Therefore we do not comment on the results, other than observing that the “first $n$” baseline produces the same results as the neural regressor. As mentioned in Section SECREF3, the labels used for the classification experiments are the 5 sentences with highest ROUGE-SU4 F1 score.
</paragraphs>
</section>

---Paper Title: Classification Betters Regression in Query-based Multi-document Summarisation Techniques for Question Answering: Macquarie University at BioASQ7b---
<section>
<section-title>Conclusions</section-title>
<section-number>7</section-number>
<paragraphs>
Macquarie University's participation in BioASQ 7 focused on the task of generating the ideal answers. The runs use query-based extractive techniques and we experiment with classification, regression, and reinforcement learning approaches. At the time of writing there were no human evaluation results, and based on ROUGE-F1 scores under cross-validation on the training data we observed that classification approaches outperform regression approaches. We experimented with several approaches to label the individual sentences for the classifier and observed that the optimal labelling policy for this task differed from prior work.

We also observed poor correlation between ROUGE-Recall and human evaluation metrics and suggest to use alternative automatic evaluation metrics with better correlation, such as ROUGE-Precision or ROUGE-F1. Given the nature of precision-based metrics which could bias the system towards returning short summaries, ROUGE-F1 is probably more appropriate when using at development time, for example for the reward function used by a reinforcement learning system.

Reinforcement learning gives promising results, especially in human evaluations made on the runs submitted to BioASQ 6b. This year we introduced very small changes to the runs using reinforcement learning, and will aim to explore more complex reinforcement learning strategies and more complex neural models in the policy and value estimators.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering</title>
<abstract>The last several years have seen intensive interest in exploring neural-network-based models for machine comprehension (MC) and question answering (QA). In this paper, we approach the problems by closely modelling questions in a neural network framework. We first introduce syntactic information to help encode questions. We then view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Enabling computers to understand given documents and answer questions about their content has recently attracted intensive interest, including but not limited to the efforts as in BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 . Many specific problems such as machine comprehension and question answering often involve modeling such question-document pairs.

The recent availability of relatively large training datasets (see Section "Related Work" for more details) has made it more feasible to train and estimate rather complex models in an end-to-end fashion for these problems, in which a whole model is fit directly with given question-answer tuples and the resulting model has shown to be rather effective.

In this paper, we take a closer look at modeling questions in such an end-to-end neural network framework, since we regard question understanding is of importance for such problems. We first introduced syntactic information to help encode questions. We then viewed and modelled different types of questions and the information shared among them as an adaptation problem and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results on our competitive baselines.
</paragraphs>
</section>

---Paper Title: Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering---
<section>
<section-title>Related Work</section-title>
<section-number>1</section-number>
<paragraphs>
Recent advance on reading comprehension and question answering has been closely associated with the availability of various datasets. BIBREF0 released the MCTest data consisting of 500 short, fictional open-domain stories and 2000 questions. The CNN/Daily Mail dataset BIBREF1 contains news articles for close style machine comprehension, in which only entities are removed and tested for comprehension. Children's Book Test (CBT) BIBREF2 leverages named entities, common nouns, verbs, and prepositions to test reading comprehension. The Stanford Question Answering Dataset (SQuAD) BIBREF3 is more recently released dataset, which consists of more than 100,000 questions for documents taken from Wikipedia across a wide range of topics. The question-answer pairs are annotated through crowdsourcing. Answers are spans of text marked in the original documents. In this paper, we use SQuAD to evaluate our models.

Many neural network models have been studied on the SQuAD task. BIBREF6 proposed match LSTM to associate documents and questions and adapted the so-called pointer Network BIBREF7 to determine the positions of the answer text spans. BIBREF8 proposed a dynamic chunk reader to extract and rank a set of answer candidates. BIBREF9 focused on word representation and presented a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on the properties of words. BIBREF10 proposed a multi-perspective context matching (MPCM) model, which matched an encoded document and question from multiple perspectives. BIBREF11 proposed a dynamic decoder and so-called highway maxout network to improve the effectiveness of the decoder. The bi-directional attention flow (BIDAF) BIBREF12 used the bi-directional attention to obtain a question-aware context representation.

In this paper, we introduce syntactic information to encode questions with a specific form of recursive neural networks BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 . More specifically, we explore a tree-structured LSTM BIBREF13 , BIBREF14 which extends the linear-chain long short-term memory (LSTM) BIBREF17 to a recursive structure, which has the potential to capture long-distance interactions over the structures.

Different types of questions are often used to seek for different types of information. For example, a "what" question could have very different property from that of a "why" question, while they may share information and need to be trained together instead of separately. We view this as a "adaptation" problem to let different types of questions share a basic model but still discriminate them when needed. Specifically, we are motivated by the ideas "i-vector" BIBREF18 in speech recognition, where neural network based adaptation is performed among different (groups) of speakers and we focused instead on different types of questions here.
</paragraphs>
</section>

---Paper Title: Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering---
<section>
<section-title>The Baseline Model</section-title>
<section-number>2</section-number>
<paragraphs>
Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction. Below we discuss these components in more details.

We concatenate embedding at two levels to represent a word: the character composition and word-level embedding. The character composition feeds all characters of a word into a convolutional neural network (CNN) BIBREF19 to obtain a representation for the word. And we use the pre-trained 300-D GloVe vectors BIBREF20 (see the experiment section for details) to initialize our word-level embedding. Each word is therefore represented as the concatenation of the character-composition vector and word-level embedding. This is performed on both questions and documents, resulting in two matrices: the $\mathbf {Q}^e \in \mathbb {R} ^{N\times d_w}$ for a question and the $\mathbf {D}^e \in \mathbb {R} ^{M\times d_w}$ for a document, where $N$ is the question length (number of word tokens), $M$ is the document length, and $d_w$ is the embedding dimensionality.

The above word representation focuses on representing individual words, and an input encoder here employs recurrent neural networks to obtain the representation of a word under its context. We use bi-directional GRU (BiGRU) BIBREF21 for both documents and questions.

$${\mathbf {Q}^c_i}&=\text{BiGRU}(\mathbf {Q}^e_i,i),\forall i \in [1, \dots , N] \\
{\mathbf {D}^c_j}&=\text{BiGRU}(\mathbf {D}^e_j,j),\forall j \in [1, \dots , M]$$   (Eq. 5) 

A BiGRU runs a forward and backward GRU on a sequence starting from the left and the right end, respectively. By concatenating the hidden states of these two GRUs for each word, we obtain the a representation for a question or document: $\mathbf {Q}^c \in \mathbb {R} ^{N\times d_c}$ for a question and $\mathbf {D}^c \in \mathbb {R} ^{M\times d_c}$ for a document.

Questions and documents interact closely. As in most previous work, our framework use both soft attention over questions and that over documents to capture the interaction between them. More specifically, in this soft-alignment layer, we first feed the contextual representation matrix $\mathbf {Q}^c$ and $\mathbf {D}^c$ to obtain alignment matrix $\mathbf {U} \in \mathbb {R} ^{N\times M}$ : 

$$\mathbf {U}_{ij} =\mathbf {Q}_i^c \cdot \mathbf {D}_j^{c\mathrm {T}}, \forall i \in [1, \dots , N], \forall j \in [1, \dots , M]$$   (Eq. 7) 

Each $\mathbf {U}_{ij}$ represents the similarity between a question word $\mathbf {Q}_i^c$ and a document word $\mathbf {D}_j^c$ .

Word-level Q-code Similar as in BIBREF12 , we obtain a word-level Q-code. Specifically, for each document word $w_j$ , we find which words in the question are relevant to it. To this end, $\mathbf {a}_j\in \mathbb {R} ^{N}$ is computed with the following equation and used as a soft attention weight: 

$$\mathbf {a}_j = softmax(\mathbf {U}_{:j}), \forall j \in [1, \dots , M]$$   (Eq. 8) 

With the attention weights computed, we obtain the encoding of the question for each document word $w_j$ as follows, which we call word-level Q-code in this paper: 

$$\mathbf {Q}^w=\mathbf {a}^{\mathrm {T}} \cdot \mathbf {Q}^{c} \in \mathbb {R} ^{M\times d_c}$$   (Eq. 9) 

Question-based filtering To better explore question understanding, we design this question-based filtering layer. As detailed later, different question representation can be easily incorporated to this layer in addition to being used as a filter to find key information in the document based on the question. This layer is expandable with more complicated question modeling.

In the basic form of question-based filtering, for each question word $w_i$ , we find which words in the document are associated. Similar to $\mathbf {a}_j$ discussed above, we can obtain the attention weights on document words for each question word $w_i$ : 

$$\mathbf {b}_i=softmax(\mathbf {U}_{i:})\in \mathbb {R} ^{M}, \forall i \in [1, \dots , N]$$   (Eq. 10) 

By pooling $\mathbf {b}\in \mathbb {R} ^{N\times M}$ , we can obtain a question-based filtering weight $\mathbf {b}^f$ : 

$$\mathbf {b}^f=norm(pooling(\mathbf {b})) \in \mathbb {R} ^{M}$$   (Eq. 11) 

$$norm(\mathbf {x})=\frac{\mathbf {x}}{\sum _i x_i}$$   (Eq. 12) 

where the specific pooling function we used include max-pooling and mean-pooling. Then the document softly filtered based on the corresponding question $\mathbf {D}^f$ can be calculated by: 

$$\mathbf {D}_j^{f_{max}}=b^{f_{max}}_j \mathbf {D}_j^{c}, \forall j \in [1, \dots , M]$$   (Eq. 13) 

$$\mathbf {D}_j^{f_{mean}}=b^{f_{mean}}_j \mathbf {D}_j^{c}, \forall j \in [1, \dots , M]$$   (Eq. 14) 

Through concatenating the document representation $\mathbf {D}^c$ , word-level Q-code $\mathbf {Q}^w$ and question-filtered document $\mathbf {D}^f$ , we can finally obtain the alignment layer representation: 

$$\mathbf {I}=[\mathbf {D}^c, \mathbf {Q}^w,\mathbf {D}^c \circ \mathbf {Q}^w,\mathbf {D}^c - \mathbf {Q}^w, \mathbf {D}^f, \mathbf {b}^{f_{max}}, \mathbf {b}^{f_{mean}}] \in \mathbb {R} ^{M \times (6d_c+2)}$$   (Eq. 16) 

where " $\circ $ " stands for element-wise multiplication and " $-$ " is simply the vector subtraction.

After acquiring the local alignment representation, key information in document and question has been collected, and the aggregation layer is then performed to find answers. We use three BiGRU layers to model the process that aggregates local information to make the global decision to find the answer spans. We found a residual architecture BIBREF22 as described in Figure 2 is very effective in this aggregation process: 

$$\mathbf {I}^1_i=\text{BiGRU}(\mathbf {I}_i)$$   (Eq. 18) 

$$\mathbf {I}^2_i=\mathbf {I}^1_i + \text{BiGRU}(\mathbf {I}^1_i)$$   (Eq. 19) 

The SQuAD QA task requires a span of text to answer a question. We use a pointer network BIBREF7 to predict the starting and end position of answers as in BIBREF6 . Different from their methods, we use a two-directional prediction to obtain the positions. For one direction, we first predict the starting position of the answer span followed by predicting the end position, which is implemented with the following equations: 

$$P(s+)=softmax(W_{s+}\cdot I^3)$$   (Eq. 23) 

$$P(e+)=softmax(W_{e+} \cdot I^3 + W_{h+} \cdot h_{s+})$$   (Eq. 24) 

where $\mathbf {I}^3$ is inference layer output, $\mathbf {h}_{s+}$ is the hidden state of the first step, and all $\mathbf {W}$ are trainable matrices. We also perform this by predicting the end position first and then the starting position: 

$$P(e-)=softmax(W_{e-}\cdot I^3)$$   (Eq. 25) 

$$P(s-)=softmax(W_{s-} \cdot I^3 + W_{h-} \cdot h_{e-})$$   (Eq. 26) 

We finally identify the span of an answer with the following equation: 

$$P(s)=pooling([P(s+), P(s-)])$$   (Eq. 27) 

$$P(e)=pooling([P(e+), P(e-)])$$   (Eq. 28) 

We use the mean-pooling here as it is more effective on the development set than the alternatives such as the max-pooling.
</paragraphs>
</section>

---Paper Title: Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering---
<section>
<section-title>Question Understanding and Adaptation</section-title>
<section-number>3</section-number>
<paragraphs>
The interplay of syntax and semantics of natural language questions is of interest for question representation. We attempt to incorporate syntactic information in questions representation with TreeLSTM BIBREF13 , BIBREF14 . In general a TreeLSTM could perform semantic composition over given syntactic structures.

Unlike the chain-structured LSTM BIBREF17 , the TreeLSTM captures long-distance interaction on a tree. The update of a TreeLSTM node is described at a high level with Equation ( 31 ), and the detailed computation is described in (–). Specifically, the input of a TreeLSTM node is used to configure four gates: the input gate $\mathbf {i}_t$ , output gate $\mathbf {o}_t$ , and the two forget gates $\mathbf {f}_t^L$ for the left child input and $\mathbf {f}_t^R$ for the right. The memory cell $\mathbf {c}_t$ considers each child's cell vector, $\mathbf {c}_{t-1}^L$ and $\mathbf {c}_{t-1}^R$ , which are gated by the left forget gate $\mathbf {f}_t^L$ and right forget gate $\mathbf {f}_t^R$ , respectively.

$$\mathbf {h}_t &= \text{TreeLSTM}(\mathbf {x}_t, \mathbf {h}_{t-1}^L, \mathbf {h}_{t-1}^R), \\

\mathbf {h}_t &= \mathbf {o}_t \circ \tanh (\mathbf {c}_{t}),\\
\mathbf {o}_t &= \sigma (\mathbf {W}_o \mathbf {x}_t + \mathbf {U}_o^L \mathbf {h}_{t-1}^L + \mathbf {U}_o^R \mathbf {h}_{t-1}^R), \\\mathbf {c}_t &= \mathbf {f}_t^L \circ \mathbf {c}_{t-1}^L + \mathbf {f}_t^R \circ \mathbf {c}_{t-1}^R + \mathbf {i}_t \circ \mathbf {u}_t, \\\mathbf {f}_t^L &= \sigma (\mathbf {W}_f \mathbf {x}_t + \mathbf {U}_f^{LL} \mathbf {h}_{t-1}^L + \mathbf {U}_f^{LR} \mathbf {h}_{t-1}^R),\\
\mathbf {f}_t^R &= \sigma (\mathbf {W}_f \mathbf {x}_t + \mathbf {U}_f^{RL} \mathbf {h}_{t-1}^L + \mathbf {U}_f^{RR} \mathbf {h}_{t-1}^R), \\\mathbf {i}_t &= \sigma (\mathbf {W}_i \mathbf {x}_t + \mathbf {U}_i^L \mathbf {h}_{t-1}^L + \mathbf {U}_i^R \mathbf {h}_{t-1}^R), \\\mathbf {u}_t &= \tanh (\mathbf {W}_c \mathbf {x}_t + \mathbf {U}_c^L \mathbf {h}_{t-1}^L + \mathbf {U}_c^R \mathbf {h}_{t-1}^R),$$   (Eq. 31) 

where $\sigma $ is the sigmoid function, $\circ $ is the element-wise multiplication of two vectors, and all $\mathbf {W}$ , $\mathbf {U}$ are trainable matrices.

To obtain the parse tree information, we use Stanford CoreNLP (PCFG Parser) BIBREF23 , BIBREF24 to produce a binarized constituency parse for each question and build the TreeLSTM based on the parse tree. The root node of TreeLSTM is used as the representation for the whole question. More specifically, we use it as TreeLSTM Q-code $\mathbf {Q}^{TL}\in \mathbb {R} ^{d_c}$ , by not only simply concatenating it to the alignment layer output but also using it as a question filter, just as we discussed in the question-based filtering section: 

$$\mathbf {Q}^{TL}=\text{TreeLSTM}(\mathbf {Q}^e) \in \mathbb {R} ^{d_c}$$   (Eq. 32) 

$$\mathbf {b}^{TL}=norm(\mathbf {Q}^{TL} \cdot \mathbf {D}^{c\mathrm {T}}) \in \mathbb {R} ^{M}$$   (Eq. 33) 

where $\mathbf {I}_{new}$ is the new output of alignment layer, and function $repmat$ copies $\mathbf {Q}^{TL}$ for M times to fit with $\mathbf {I}$ .

Questions by nature are often composed to fulfill different types of information needs. For example, a "when" question seeks for different types of information (i.e., temporal information) than those for a "why" question. Different types of questions and the corresponding answers could potentially have different distributional regularity.

The previous models are often trained for all questions without explicitly discriminating different question types; however, for a target question, both the common features shared by all questions and the specific features for a specific type of question are further considered in this paper, as they could potentially obey different distributions. In this paper we further explicitly model different types of questions in the end-to-end training. We start from a simple way to first analyze the word frequency of all questions, and obtain top-10 most frequent question types: what, how, who, when, which, where, why, be, whose, and whom, in which be stands for the questions beginning with different forms of the word be such as is, am, and are. We explicitly encode question-type information to be an 11-dimensional one-hot vector (the top-10 question types and "other" question type). Each question type is with a trainable embedding vector. We call this explicit question type code, $\mathbf {ET}\in \mathbb {R} ^{d_{ET}}$ . Then the vector for each question type is tuned during training, and is added to the system with the following equation: 

$$\mathbf {I}_{new}=[\mathbf {I}, repmat(\mathbf {ET})]$$   (Eq. 38) 

As discussed, different types of questions and their answers may share common regularity and have separate property at the same time. We also view this as an adaptation problem in order to let different types of questions share a basic model but still discriminate them when needed. Specifically, we borrow ideas from speaker adaptation BIBREF18 in speech recognition, where neural-network-based adaptation is performed among different groups of speakers.

Conceptually we regard a type of questions as a group of acoustically similar speakers. Specifically we propose a question discriminative block or simply called a discriminative block (Figure 3 ) below to perform question adaptation. The main idea is described below: 

$$\mathbf {x^\prime } = f([\mathbf {x}, \mathbf {\bar{x}}^c, \mathbf {\delta _x}])$$   (Eq. 40) 

For each input question $\mathbf {x}$ , we can decompose it to two parts: the cluster it belong(i.e., question type) and the diverse in the cluster. The information of the cluster is encoded in a vector $\mathbf {\bar{x}}^c$ . In order to keep calculation differentiable, we compute the weight of all the clusters based on the distances of $\mathbf {x}$ and each cluster center vector, in stead of just choosing the closest cluster. Then the discriminative vector $\mathbf {\delta _x}$ with regard to these most relevant clusters are computed. All this information is combined to obtain the discriminative information. In order to keep the full information of input, we also copy the input question $\mathbf {x}$ , together with the acquired discriminative information, to a feed-forward layer to obtain a new representation $\mathbf {x^\prime }$ for the question.

More specifically, the adaptation algorithm contains two steps: adapting and updating, which is detailed as follows:

Adapting In the adapting step, we first compute the similarity score between an input question vector $\mathbf {x}\in \mathbb {R} ^{h}$ and each centroid vector of $K$ clusters $~\mathbf {\bar{x}}\in \mathbb {R} ^{K \times h}$ . Each cluster here models a question type. Unlike the explicit question type modeling discussed above, here we do not specify what question types we are modeling but let the system to learn. Specifically, we only need to pre-specific how many clusters, $K$ , we are modeling. The similarity between an input question and cluster centroid can be used to compute similarity weight $\mathbf {w}^a$ : 

$$w_k^a = softmax(cos\_sim(\mathbf {x}, \mathbf {\bar{x}}_k), \alpha ), \forall k \in [1, \dots , K]$$   (Eq. 43) 

$$cos\_sim(\mathbf {u}, \mathbf {v}) = \frac{<\mathbf {u},\mathbf {v}>}{||\mathbf {u}|| \cdot ||\mathbf {v}||}$$   (Eq. 44) 

We set $\alpha $ equals 50 to make sure only closest class will have a high weight while maintain differentiable. Then we acquire a soft class-center vector $\mathbf {\bar{x}}^c$ : 

$$\mathbf {\bar{x}}^c = \sum _k w^a_k \mathbf {\bar{x}}_k \in \mathbb {R} ^{h}$$   (Eq. 46) 

We then compute a discriminative vector $\mathbf {\delta _x}$ between the input question with regard to the soft class-center vector: 

$$\mathbf {\delta _x} = \mathbf {x} - \mathbf {\bar{x}}^c$$   (Eq. 47) 

Note that $\bar{\mathbf {x}}^c$ here models the cluster information and $\mathbf {\delta _x}$ represents the discriminative information in the cluster. By feeding $\mathbf {x}$ , $\bar{\mathbf {x}}^c$ and $\mathbf {\delta _x}$ into feedforward layer with Relu, we obtain $\mathbf {x^{\prime }}\in \mathbb {R} ^{K}$ : 

$$\mathbf {x^{\prime }} = Relu(\mathbf {W} \cdot [\mathbf {x},\bar{\mathbf {x}}^c,\mathbf {\delta _x}])$$   (Eq. 48) 

With $\mathbf {x^{\prime }}$ ready, we can apply Discriminative Block to any question code and obtain its adaptation Q-code. In this paper, we use TreeLSTM Q-code as the input vector $\mathbf {x}$ , and obtain TreeLSTM adaptation Q-code $\mathbf {Q}^{TLa}\in \mathbb {R} ^{d_c}$ . Similar to TreeLSTM Q-code $\mathbf {Q}^{TL}$ , we concatenate $\mathbf {Q}^{TLa}$ to alignment output $\mathbf {I}$ and also use it as a question filter: 

$$\mathbf {Q}^{TLa} = Relu(\mathbf {W} \cdot [\mathbf {Q}^{TL},\overline{\mathbf {Q}^{TL}}^c,\mathbf {\delta _{\mathbf {Q}^{TL}}}])$$   (Eq. 49) 

$$\mathbf {b}^{TLa}=norm(\mathbf {Q}^{TLa} \cdot \mathbf {D}^{c\mathrm {T}}) \in \mathbb {R} ^{M}$$   (Eq. 50) 

Updating The updating stage attempts to modify the center vectors of the $K$ clusters in order to fit each cluster to model different types of questions. The updating is performed according to the following formula: 

$$\mathbf {\bar{x}^{\prime }}_k = (1-\beta \text{w}_k^a)\mathbf {\bar{x}}_k+\beta \text{w}_k^a\mathbf {x}, \forall k \in [1, \dots , K]$$   (Eq. 54) 

In the equation, $\beta $ is an updating rate used to control the amount of each updating, and we set it to 0.01. When $\mathbf {x}$ is far away from $K$ -th cluster center $\mathbf {\bar{x}}_k$ , $\text{w}_k^a$ is close to be value 0 and the $k$ -th cluster center $\mathbf {\bar{x}}_k$ tends not to be updated. If $\mathbf {x}$ is instead close to the $j$ -th cluster center $\mathbf {\bar{x}}_j$ , $\mathbf {x}$0 is close to the value 1 and the centroid of the $\mathbf {x}$1 -th cluster $\mathbf {x}$2 will be updated more aggressively using $\mathbf {x}$3 .
</paragraphs>
</section>

---Paper Title: Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering---
<section>
<section-title>Set-Up</section-title>
<section-number>4</section-number>
<paragraphs>
We test our models on Stanford Question Answering Dataset (SQuAD) BIBREF3 . The SQuAD dataset consists of more than 100,000 questions annotated by crowdsourcing workers on a selected set of Wikipedia articles, and the answer to each question is a span of text in the Wikipedia articles. Training data includes 87,599 instances and validation set has 10,570 instances. The test data is hidden and kept by the organizer. The evaluation of SQuAD is Exact Match (EM) and F1 score.

We use pre-trained 300-D Glove 840B vectors BIBREF20 to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. CharCNN filter length is 1,3,5, each is 50 dimensions. All vectors including word embedding are updated during training. The cluster number K in discriminative block is 100. The Adam method BIBREF25 is used for optimization. And the first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. We will half learning rate when meet a bad iteration, and the patience is 7. Our early stop evaluation is the EM and F1 score of validation set. All hidden states of GRUs, and TreeLSTMs are 500 dimensions, while word-level embedding $d_w$ is 300 dimensions. We set max length of document to 500, and drop the question-document pairs beyond this on training set. Explicit question-type dimension $d_{ET}$ is 50. We apply dropout to the Encoder layer and aggregation layer with a dropout rate of 0.5.
</paragraphs>
</section>

---Paper Title: Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering---
<section>
<section-title>Results</section-title>
<section-number>5</section-number>
<paragraphs>
Table 1 shows the official leaderboard on SQuAD test set when we submitted our system. Our model achieves a 68.73% EM score and 77.39% F1 score, which is ranked among the state of the art single models (without model ensembling).

Table 2 shows the ablation performances of various Q-code on the development set. Note that since the testset is hidden from us, we can only perform such an analysis on the development set. Our baseline model using no Q-code achieved a 68.00% and 77.36% EM and F1 scores, respectively. When we added the explicit question type T-code into the baseline model, the performance was improved slightly to 68.16%(EM) and 77.58%(F1). We then used TreeLSTM introduce syntactic parses for question representation and understanding (replacing simple question type as question understanding Q-code), which consistently shows further improvement. We further incorporated the soft adaptation. When letting the number of hidden question types ( $K$ ) to be 20, the performance improves to 68.73%/77.74% on EM and F1, respectively, which corresponds to the results of our model reported in Table 1 . Furthermore, after submitted our result, we have experimented with a large value of $K$ and found that when $K=100$ , we can achieve a better performance of 69.10%/78.38% on the development set.

Figure UID61 shows the EM/F1 scores of different question types while Figure UID62 is the question type amount distribution on the development set. In Figure UID61 we can see that the average EM/F1 of the "when" question is highest and those of the "why" question is the lowest. From Figure UID62 we can see the "what" question is the major class.

Figure 5 shows the composition of F1 score. Take our best model as an example, we observed a 78.38% F1 score on the whole development set, which can be separated into two parts: one is where F1 score equals to 100%, which means an exact match. This part accounts for 69.10% of the entire development set. And the other part accounts for 30.90%, of which the average F1 score is 30.03%. For the latter, we can further divide it into two sub-parts: one is where the F1 score equals to 0%, which means that predict answer is totally wrong. This part occupies 14.89% of the total development set. The other part accounts for 16.01% of the development set, of which average F1 score is 57.96%. From this analysis we can see that reducing the zero F1 score (14.89%) is potentially an important direction to further improve the system.
</paragraphs>
</section>

---Paper Title: Exploring Question Understanding and Adaptation in Neural-Network-Based Question Answering---
<section>
<section-title>Conclusions</section-title>
<section-number>6</section-number>
<paragraphs>
Closely modelling questions could be of importance for question answering and machine reading. In this paper, we introduce syntactic information to help encode questions in neural networks. We view and model different types of questions and the information shared among them as an adaptation task and proposed adaptation models for them. On the Stanford Question Answering Dataset (SQuAD), we show that these approaches can help attain better results over a competitive baseline.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Answering Complex Questions Using Open Information Extraction</title>
<abstract>While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Effective question answering (QA) systems have been a long-standing quest of AI research. Structured curated KBs have been used successfully for this task BIBREF0 , BIBREF1 . However, these KBs are expensive to build and typically domain-specific. Automatically constructed open vocabulary (subject; predicate; object) style tuples have broader coverage, but have only been used for simple questions where a single tuple suffices BIBREF2 , BIBREF3 .

Our goal in this work is to develop a QA system that can perform reasoning with Open IE BIBREF4 tuples for complex multiple-choice questions that require tuples from multiple sentences. Such a system can answer complex questions in resource-poor domains where curated knowledge is unavailable. Elementary-level science exams is one such domain, requiring complex reasoning BIBREF5 . Due to the lack of a large-scale structured KB, state-of-the-art systems for this task either rely on shallow reasoning with large text corpora BIBREF6 , BIBREF7 or deeper, structured reasoning with a small amount of automatically acquired BIBREF8 or manually curated BIBREF9 knowledge.

Consider the following question from an Alaska state 4th grade science test:

Which object in our solar system reflects light and is a satellite that orbits around one planet? (A) Earth (B) Mercury (C) the Sun (D) the Moon

This question is challenging for QA systems because of its complex structure and the need for multi-fact reasoning. A natural way to answer it is by combining facts such as (Moon; is; in the solar system), (Moon; reflects; light), (Moon; is; satellite), and (Moon; orbits; around one planet).

A candidate system for such reasoning, and which we draw inspiration from, is the TableILP system of BIBREF9 . TableILP treats QA as a search for an optimal subgraph that connects terms in the question and answer via rows in a set of curated tables, and solves the optimization problem using Integer Linear Programming (ILP). We similarly want to search for an optimal subgraph. However, a large, automatically extracted tuple KB makes the reasoning context different on three fronts: (a) unlike reasoning with tables, chaining tuples is less important and reliable as join rules aren't available; (b) conjunctive evidence becomes paramount, as, unlike a long table row, a single tuple is less likely to cover the entire question; and (c) again, unlike table rows, tuples are noisy, making combining redundant evidence essential. Consequently, a table-knowledge centered inference model isn't the best fit for noisy tuples.

To address this challenge, we present a new ILP-based model of inference with tuples, implemented in a reasoner called TupleInf. We demonstrate that TupleInf significantly outperforms TableILP by 11.8% on a broad set of over 1,300 science questions, without requiring manually curated tables, using a substantially simpler ILP formulation, and generalizing well to higher grade levels. The gains persist even when both solvers are provided identical knowledge. This demonstrates for the first time how Open IE based QA can be extended from simple lookup questions to an effective system for complex questions.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Related Work</section-title>
<section-number>1</section-number>
<paragraphs>
We discuss two classes of related work: retrieval-based web question-answering (simple reasoning with large scale KB) and science question-answering (complex reasoning with small KB).
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Tuple Inference Solver</section-title>
<section-number>2</section-number>
<paragraphs>
We first describe the tuples used by our solver. We define a tuple as (subject; predicate; objects) with zero or more objects. We refer to the subject, predicate, and objects as the fields of the tuple.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Tuple KB</section-title>
<section-number>3</section-number>
<paragraphs>
We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB. For each test set, we use the corresponding training questions $Q_\mathit {tr}$ to retrieve domain-relevant sentences from S. Specifically, for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$ to create the tuple KB (T).
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Tuple Selection</section-title>
<section-number>4</section-number>
<paragraphs>
Given a multiple-choice question $qa$ with question text $q$ and answer choices A= $\lbrace a_i\rbrace $ , we select the most relevant tuples from $T$ and $S$ as follows.

Selecting from Tuple KB: We use an inverted index to find the 1,000 tuples that have the most overlapping tokens with question tokens $tok(qa).$ . We also filter out any tuples that overlap only with $tok(q)$ as they do not support any answer. We compute the normalized TF-IDF score treating the question, $q$ as a query and each tuple, $t$ as a document: $
&\textit {tf}(x, q)=1\; \textmd {if x} \in q ; \textit {idf}(x) = log(1 + N/n_x) \\
&\textit {tf-idf}(t, q)=\sum _{x \in t\cap q} idf(x)
$ 

 where $N$ is the number of tuples in the KB and $n_x$ are the number of tuples containing $x$ . We normalize the tf-idf score by the number of tokens in $t$ and $q$ . We finally take the 50 top-scoring tuples $T_{qa}$ .

On-the-fly tuples from text: To handle questions from new domains not covered by the training set, we extract additional tuples on the fly from S (similar to BIBREF17 knowlhunting). We perform the same ElasticSearch query described earlier for building T. We ignore sentences that cover none or all answer choices as they are not discriminative. We also ignore long sentences ( $>$ 300 characters) and sentences with negation as they tend to lead to noisy inference. We then run Open IE on these sentences and re-score the resulting tuples using the Jaccard score due to the lossy nature of Open IE, and finally take the 50 top-scoring tuples $T^{\prime }_{qa}$ .
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Support Graph Search</section-title>
<section-number>5</section-number>
<paragraphs>
Similar to TableILP, we view the QA task as searching for a graph that best connects the terms in the question (qterms) with an answer choice via the knowledge; see Figure 1 for a simple illustrative example. Unlike standard alignment models used for tasks such as Recognizing Textual Entailment (RTE) BIBREF18 , however, we must score alignments between a set $T_{qa} \cup T^{\prime }_{qa}$ of structured tuples and a (potentially multi-sentence) multiple-choice question $qa$ .

The qterms, answer choices, and tuples fields form the set of possible vertices, $\mathcal {V}$ , of the support graph. Edges connecting qterms to tuple fields and tuple fields to answer choices form the set of possible edges, $\mathcal {E}$ . The support graph, $G(V, E)$ , is a subgraph of $\mathcal {G}(\mathcal {V}, \mathcal {E})$ where $V$ and $E$ denote “active” nodes and edges, resp. We define the desired behavior of an optimal support graph via an ILP model as follows.

Similar to TableILP, we score the support graph based on the weight of the active nodes and edges. Each edge $e(t, h)$ is weighted based on a word-overlap score. While TableILP used WordNet BIBREF19 paths to compute the weight, this measure results in unreliable scores when faced with longer phrases found in Open IE tuples.

Compared to a curated KB, it is easy to find Open IE tuples that match irrelevant parts of the questions. To mitigate this issue, we improve the scoring of qterms in our ILP objective to focus on important terms. Since the later terms in a question tend to provide the most critical information, we scale qterm coefficients based on their position. Also, qterms that appear in almost all of the selected tuples tend not to be discriminative as any tuple would support such a qterm. Hence we scale the coefficients by the inverse frequency of the tokens in the selected tuples.

Since Open IE tuples do not come with schema and join rules, we can define a substantially simpler model compared to TableILP. This reduces the reasoning capability but also eliminates the reliance on hand-authored join rules and regular expressions used in TableILP. We discovered (see empirical evaluation) that this simple model can achieve the same score as TableILP on the Regents test (target test set used by TableILP) and generalizes better to different grade levels.

We define active vertices and edges using ILP constraints: an active edge must connect two active vertices and an active vertex must have at least one active edge. To avoid positive edge coefficients in the objective function resulting in spurious edges in the support graph, we limit the number of active edges from an active tuple, question choice, tuple fields, and qterms (first group of constraints in Table 1 ). Our model is also capable of using multiple tuples to support different parts of the question as illustrated in Figure 1 . To avoid spurious tuples that only connect with the question (or choice) or ignore the relation being expressed in the tuple, we add constraints that require each tuple to connect a qterm with an answer choice (second group of constraints in Table 1 ).

We also define new constraints based on the Open IE tuple structure. Since an Open IE tuple expresses a fact about the tuple's subject, we require the subject to be active in the support graph. To avoid issues such as (Planet; orbit; Sun) matching the sample question in the introduction (“Which object $\ldots $ orbits around a planet”), we also add an ordering constraint (third group in Table 1 ).

Its worth mentioning that TupleInf only combines parallel evidence i.e. each tuple must connect words in the question to the answer choice. For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Experiments</section-title>
<section-number>6</section-number>
<paragraphs>
Comparing our method with two state-of-the-art systems for 4th and 8th grade science exams, we demonstrate that (a) TupleInf with only automatically extracted tuples significantly outperforms TableILP with its original curated knowledge as well as with additional tuples, and (b) TupleInf's complementary approach to IR leads to an improved ensemble. Numbers in bold indicate statistical significance based on the Binomial exact test BIBREF20 at $p=0.05$ .

We consider two question sets. (1) 4th Grade set (1220 train, 1304 test) is a 10x larger superset of the NY Regents questions BIBREF6 , and includes professionally written licensed questions. (2) 8th Grade set (293 train, 282 test) contains 8th grade questions from various states.

We consider two knowledge sources. The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. This corpus is used by the IR solver and also used to create the tuple KB T and on-the-fly tuples $T^{\prime }_{qa}$ . Additionally, TableILP uses $\sim $ 70 Curated tables (C) designed for 4th grade NY Regents exams.

We compare TupleInf with two state-of-the-art baselines. IR is a simple yet powerful information-retrieval baseline BIBREF6 that selects the answer option with the best matching sentence in a corpus. TableILP is the state-of-the-art structured inference baseline BIBREF9 developed for science questions.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Results</section-title>
<section-number>7</section-number>
<paragraphs>
Table 2 shows that TupleInf, with no curated knowledge, outperforms TableILP on both question sets by more than 11%. The lower half of the table shows that even when both solvers are given the same knowledge (C+T), the improved selection and simplified model of TupleInf results in a statistically significant improvement. Our simple model, TupleInf(C + T), also achieves scores comparable to TableILP on the latter's target Regents questions (61.4% vs TableILP's reported 61.5%) without any specialized rules.

Table 3 shows that while TupleInf achieves similar scores as the IR solver, the approaches are complementary (structured lossy knowledge reasoning vs. lossless sentence retrieval). The two solvers, in fact, differ on 47.3% of the training questions. To exploit this complementarity, we train an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the individual solvers. Further, IR + TupleInf is consistently better than IR + TableILP. Finally, in combination with IR and the statistical association based PMI solver (that scores 54.1% by itself) of BIBREF6 aristo2016:combining, TupleInf achieves a score of 58.2% as compared to TableILP's ensemble score of 56.7% on the 4th grade set, again attesting to TupleInf's strength.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Error Analysis</section-title>
<section-number>8</section-number>
<paragraphs>
We describe four classes of failures that we observed, and the future work they suggest.

Missing Important Words: Which material will spread out to completely fill a larger container? (A)air (B)ice (C)sand (D)water

In this question, we have tuples that support water will spread out and fill a larger container but miss the critical word “completely”. An approach capable of detecting salient question words could help avoid that.

Lossy IE: Which action is the best method to separate a mixture of salt and water? ...

The IR solver correctly answers this question by using the sentence: Separate the salt and water mixture by evaporating the water. However, TupleInf is not able to answer this question as Open IE is unable to extract tuples from this imperative sentence. While the additional structure from Open IE is useful for more robust matching, converting sentences to Open IE tuples may lose important bits of information.

Bad Alignment: Which of the following gases is necessary for humans to breathe in order to live?(A) Oxygen(B) Carbon dioxide(C) Helium(D) Water vapor

TupleInf returns “Carbon dioxide” as the answer because of the tuple (humans; breathe out; carbon dioxide). The chunk “to breathe” in the question has a high alignment score to the “breathe out” relation in the tuple even though they have completely different meanings. Improving the phrase alignment can mitigate this issue.

Out of scope: Deer live in forest for shelter. If the forest was cut down, which situation would most likely happen?...

Such questions that require modeling a state presented in the question and reasoning over the state are out of scope of our solver.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Conclusion</section-title>
<section-number>9</section-number>
<paragraphs>
We presented a new QA system, TupleInf, that can reason over a large, potentially noisy tuple KB to answer complex questions. Our results show that TupleInf is a new state-of-the-art structured solver for elementary-level science that does not rely on curated knowledge and generalizes to higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context and distributional measures.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Appendix: ILP Model Details</section-title>
<section-number>10</section-number>
<paragraphs>
To build the ILP model, we first need to get the questions terms (qterm) from the question by chunking the question using an in-house chunker based on the postagger from FACTORIE. 
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Experiment Details</section-title>
<section-number>11</section-number>
<paragraphs>
We use the SCIP ILP optimization engine BIBREF21 to optimize our ILP model. To get the score for each answer choice $a_i$ , we force the active variable for that choice $x_{a_i}$ to be one and use the objective function value of the ILP model as the score. For evaluations, we use a 2-core 2.5 GHz Amazon EC2 linux machine with 16 GB RAM. To evaluate TableILP and TupleInf on curated tables and tuples, we converted them into the expected format of each solver as follows.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Using curated tables with TupleInf</section-title>
<section-number>12</section-number>
<paragraphs>
For each question, we select the 7 best matching tables using the tf-idf score of the table w.r.t. the question tokens and top 20 rows from each table using the Jaccard similarity of the row with the question. (same as BIBREF9 tableilp2016). We then convert the table rows into the tuple structure using the relations defined by TableILP. For every pair of cells connected by a relation, we create a tuple with the two cells as the subject and primary object with the relation as the predicate. The other cells of the table are used as additional objects to provide context to the solver. We pick top-scoring 50 tuples using the Jaccard score.
</paragraphs>
</section>

---Paper Title: Answering Complex Questions Using Open Information Extraction---
<section>
<section-title>Using Open IE tuples with TableILP</section-title>
<section-number>13</section-number>
<paragraphs>
We create an additional table in TableILP with all the tuples in $T$ . Since TableILP uses fixed-length $(subject; predicate; object)$ triples, we need to map tuples with multiple objects to this format. For each object, $O_i$ in the input Open IE tuple $(S; P; O_1; O_2 \ldots )$ , we add a triple $(S; P; O_i)$ to this table.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Recurrent Neural Network Encoder with Attention for Community Question Answering</title>
<abstract>We apply a general recurrent neural network (RNN) encoder framework to community question answering (cQA) tasks. Our approach does not rely on any linguistic processing, and can be applied to different languages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that encourages reasoning over entire sequences. To deal with practical issues such as data sparsity and imbalanced labels, we apply various techniques such as transfer learning and multitask learning. Our experiments on the SemEval-2016 cQA task show 10% improvement on a MAP score compared to an information retrieval-based approach, and achieve comparable performance to a strong handcrafted feature-based method.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Community question answering (cQA) is a paradigm that provides forums for users to ask or answer questions on any topic with barely any restrictions. In the past decade, these websites have attracted a great number of users, and have accumulated a large collection of question-comment threads generated by these users. However, the low restriction results in a high variation in answer quality, which makes it time-consuming to search for useful information from the existing content. It would therefore be valuable to automate the procedure of ranking related questions and comments for users with a new question, or when looking for solutions from comments of an existing question.

Automation of cQA forums can be divided into three tasks: question-comment relevance (Task A), question-question relevance (Task B), and question-external comment relevance (Task C). One might think that classic retrieval models like language models for information retrieval BIBREF0 could solve these tasks. However, a big challenge for cQA tasks is that users are used to expressing similar meanings with different words, which creates gaps when matching questions based on common words. Other challenges include informal usage of language, highly diverse content of comments, and variation in the length of both questions and comments.

To overcome these issues, most previous work (e.g. SemEval 2015 BIBREF1 ) relied heavily on additional features and reasoning capabilities. In BIBREF2 , a neural attention-based model was proposed for automatically recognizing entailment relations between pairs of natural language sentences. In this study, we first modify this model for all three cQA tasks. We also extend this framework into a jointly trained model when the external resources are available, i.e. selecting an external comment when we know the question that the external comment answers (Task C).

Our ultimate objective is to classify relevant questions and comments without complicated handcrafted features. By applying RNN-based encoders, we avoid heavily engineered features and learn the representation automatically. In addition, an attention mechanism augments encoders with the ability to attend to past outputs directly. This becomes helpful when encoding longer sequences, since we no longer need to compress all information into a fixed-length vector representation.

In our view, existing annotated cQA corpora are generally too small to properly train an end-to-end neural network. To address this, we investigate transfer learning by pretraining the recurrent systems on other corpora, and also generating additional instances from existing cQA corpus.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Related Work</section-title>
<section-number>1</section-number>
<paragraphs>
Earlier work of community question answering relied heavily on feature engineering, linguistic tools, and external resource. BIBREF3 and BIBREF4 utilized rich non-textual features such as answer's profile. BIBREF5 syntactically analyzed the question and extracted name entity features. BIBREF6 demonstrated a textual entailment system can enhance cQA task by casting question answering to logical entailment.

More recent work incorporated word vector into their feature extraction system and based on it designed different distance metric for question and answer BIBREF7 BIBREF8 . While these approaches showed effectiveness, it is difficult to generalize them to common cQA tasks since linguistic tools and external resource may be restrictive in other languages and features are highly customized for each cQA task.

Very recent work on answer selection also involved the use of neural networks. BIBREF9 used LSTM to construct a joint vector based on both the question and the answer and then converted it into a learning to rank problem. BIBREF10 proposed several convolutional neural network (CNN) architectures for cQA. Our method differs in that RNN encoder is applied here and by adding attention mechanism we jointly learn which words in question to focus and hence available to conduct qualitative analysis. During classification, we feed the extracted vector into a feed-forward neural network directly instead of using mean/max pooling on top of each time steps.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Method</section-title>
<section-number>2</section-number>
<paragraphs>
In this section, we first discuss long short-term memory (LSTM) units and an associated attention mechanism. Next, we explain how we can encode a pair of sentences into a dense vector for predicting relationships using an LSTM with an attention mechanism. Finally, we apply these models to predict question-question similarity, question-comment similarity, and question-external comment similarity.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>LSTM Models</section-title>
<section-number>3</section-number>
<paragraphs>
LSTMs have shown great success in many different fields. An LSTM unit contains a memory cell with self-connections, as well as three multiplicative gates to control information flow. Given input vector $x_t$ , previous hidden outputs $h_{t-1}$ , and previous cell state $c_{t-1}$ , LSTM units operate as follows: 

$$X &= \begin{bmatrix}
x_t\\[0.3em]
h_{t-1}\\[0.3em]
\end{bmatrix}\\
i_t &= \sigma (\mathbf {W_{iX}}X + \mathbf {W_{ic}}c_{t-1} + \mathbf {b_i})\\
f_t &= \sigma (\mathbf {W_{fX}}X + \mathbf {W_{fc}}c_{t-1} + \mathbf {b_f})\\
o_t &= \sigma (\mathbf {W_{oX}}X + \mathbf {W_{oc}}c_{t-1} + \mathbf {b_o})\\
c_t &= f_t \odot c_{t-1} + i_t \odot tanh(\mathbf {W_{cX}}X + \mathbf {b_c})\\
h_t &= o_t \odot tanh(c_t)$$   (Eq. 3) 

where $i_t$ , $f_t$ , $o_t$ are input, forget, and output gates, respectively. The sigmoid function $\sigma ()$ is a soft gate function controlling the amount of information flow. $W$ s and $b$ s are model parameters to learn.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Neural Attention</section-title>
<section-number>4</section-number>
<paragraphs>
A traditional RNN encoder-decoder approach BIBREF11 first encodes an arbitrary length input sequence into a fixed-length dense vector that can be used as input to subsequent classification models, or to initialize the hidden state of a secondary decoder. However, the requirement to compress all necessary information into a single fixed length vector can be problematic. A neural attention model BIBREF12 BIBREF13 has been recently proposed to alleviate this issue by enabling the network to attend to past outputs when decoding. Thus, the encoder no longer needs to represent an entire sequence with one vector; instead, it encodes information into a sequence of vectors, and adaptively chooses a subset of the vectors when decoding.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Predicting Relationships of Object Pairs with an Attention Model</section-title>
<section-number>5</section-number>
<paragraphs>
In our cQA tasks, the pair of objects are (question, question) or (question, comment), and the relationship is relevant/irrelevant. The left side of Figure 1 shows one intuitive way to predict relationships using RNNs. Parallel LSTMs encode two objects independently, and then concatenate their outputs as an input to a feed-forward neural network (FNN) with a softmax output layer for classification.

The representations of the two objects are generated independently in this manner. However, we are more interested in the relationship instead of the object representations themselves. Therefore, we consider a serialized LSTM-encoder model in the right side of Figure 1 that is similar to that in BIBREF2 , but also allows an augmented feature input to the FNN classifier.

Figure 2 illustrates our attention framework in more detail. The first LSTM reads one object, and passes information through hidden units to the second LSTM. The second LSTM then reads the other object and generates the representation of this pair after the entire sequence is processed. We build another FNN that takes this representation as input to classify the relationship of this pair.

By adding an attention mechanism to the encoder, we allow the second LSTM to attend to the sequence of output vectors from the first LSTM, and hence generate a weighted representation of first object according to both objects. Let $h_N$ be the last output of second LSTM and $M = [h_1, h_2, \cdots , h_L]$ be the sequence of output vectors of the first object. The weighted representation of the first object is 

$$h^{\prime } = \sum _{i=1}^{L} \alpha _i h_i$$   (Eq. 7) 

The weight is computed by 

$$\alpha _i = \dfrac{exp(a(h_i,h_N))}{\sum _{j=1}^{L}exp(a(h_j,h_N))}$$   (Eq. 8) 

where $a()$ is the importance model that produces a higher score for $(h_i, h_N)$ if $h_i$ is useful to determine the object pair's relationship. We parametrize this model using another FNN. Note that in our framework, we also allow other augmented features (e.g., the ranking score from the IR system) to enhance the classifier. So the final input to the classifier will be $h_N$ , $h^{\prime }$ , as well as augmented features.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Modeling Question-External Comments</section-title>
<section-number>6</section-number>
<paragraphs>
For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly learns to predict the relationships of the three pairs (oriQ/relQ, oriQ/relC, relQ/relC).

Figure 3 shows our framework: the three lower models are separate serialized LSTM-encoders for the three respective object pairs, whereas the upper model is an FNN that takes as input the concatenation of the outputs of three encoders, and predicts the relationships for all three pairs. More specifically, the output layer consists of three softmax layers where each one is intended to predict the relationship of one particular pair.

For the overall loss function, we combine three separate loss functions using a heuristic weight vector $\beta $ that allocates a higher weight to the main task (oriQ-relC relationship prediction) as follows: 

$$\mathcal {L} = \beta _1 \mathcal {L}_1 + \beta _2 \mathcal {L}_2 + \beta _3 \mathcal {L}_3$$   (Eq. 11) 

By doing so, we hypothesize that the related tasks can improve the main task by leveraging commonality among all tasks.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Experiments</section-title>
<section-number>7</section-number>
<paragraphs>
We evaluate our approach on all three cQA tasks. We use the cQA datasets provided by the Semeval 2016 task . The cQA data is organized as follows: there are 267 original questions, each question has 10 related question, and each related question has 10 comments. Therefore, for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with the training set. To evaluate the performance, we use mean average precision (MAP) and F1 score.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Preliminary Results</section-title>
<section-number>8</section-number>
<paragraphs>
Table 2 shows the initial results using the RNN encoder for different tasks. We observe that the attention model always gets better results than the RNN without attention, especially for task C. However, the RNN model achieves a very low F1 score. For task B, it is even worse than the random baseline. We believe the reason is because for task B, there are only 2,670 pairs for training which is very limited training for a reasonable neural network. For task C, we believe the problem is highly imbalanced data. Since the related comments did not directly comment on the original question, more than $90\%$ of the comments are labeled as irrelevant to the original question. The low F1 (with high precision and low recall) means our system tends to label most comments as irrelevant. In the following section, we investigate methods to address these issues.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Robust Parameter Initialization</section-title>
<section-number>9</section-number>
<paragraphs>
One way to improve models trained on limited data is to use external data to pretrain the neural network. We therefore considered two different datasets for this task.

Cross-domain: The Stanford natural language inference (SNLI) corpus BIBREF17 has a huge amount of cleaned premise and hypothesis pairs. Unfortunately the pairs are for a different task. The relationship between the premise and hypothesis may be similar to the relation between questions and comments, but may also be different.

In-domain: since task A seems has reasonable performance, and the network is also well-trained, we could use it directly to initialize task B.

To utilize the data, we first trained the model on each auxiliary data (SNLI or Task A) and then removed the softmax layer. After that, we retrain the network using the target data with a softmax layer that was randomly initialized.

For task A, the SNLI cannot improve MAP or F1 scores. Actually it slightly hurts the performance. We surmise that it is probably because the domain is different. Further investigation is needed: for example, we could only use the parameter for embedding layers etc. For task B, the SNLI yields a slight improvement on MAP ( $0.2\%$ ), and Task A could give ( $1.2\%$ ) on top of that. No improvement was observed on F1. For task C, pretraining by task A is also better than using SNLI (task A is $1\%$ better than the baseline, while SNLI is almost the same).

In summary, the in-domain pretraining seems better, but overall, the improvement is less than we expected, especially for task B, which only has very limited target data. We will not make a conclusion here since more investigation is needed.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Multitask Learning</section-title>
<section-number>10</section-number>
<paragraphs>
As mentioned in Section "Modeling Question-External Comments" , we also explored a multitask learning framework that jointly learns to predict the relationships of all three tasks. We set $0.8$ for the main task (task C) and $0.1$ for the other auxiliary tasks. The MAP score did not improve, but F1 increases to $0.1617$ . We believe this is because other tasks have more balanced labels, which improves the shared parameters for task C.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Augmented data</section-title>
<section-number>11</section-number>
<paragraphs>
There are many sources of external question-answer pairs that could be used in our tasks. For example: WebQuestion (was introduced by the authors of SEMPRE system BIBREF18 ) and The SimpleQuestions dataset . All of them are positive examples for our task and we can easily create negative examples from it. Initial experiments indicate that it is very easy to overfit these obvious negative examples. We believe this is because our negative examples are non-informative for our task and just introduce noise.

Since the external data seems to hurt the performance, we try to use the in-domain pairs to enhance task B and task C. For task B, if relative question 1 (rel1) and relative question 2 (rel2) are both relevant to the original question, then we add a positive sample (rel1, rel2, 1). If either rel1 and rel2 is irrelevant and the other is relevant, we add a negative sample (rel1, rel2, 0). After doing this, the samples of task B increase from $2,670$ to $11,810$ . By applying this method, the MAP score increased slightly from $0.5723$ to $0.5789$ but the F1 score improved from $0.4334$ to $0.5860$ .

For task C, we used task A's data directly. The results are very similar with a slight improvement on MAP, but large improvement on F1 score from $0.1449$ to $0.2064$ .
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Augmented features</section-title>
<section-number>12</section-number>
<paragraphs>
To further enhance the system, we incorporate a one hot vector of the original IR ranking as an additional feature into the FNN classifier. Table 3 shows the results. In comparing the models with and without augmented features, we can see large improvement for task B and C. The F1 score for task A degrades slightly but MAP improves. This might be because task A already had a substantial amount of training data.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Comparison with Other Systems</section-title>
<section-number>13</section-number>
<paragraphs>
Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will give large gains on tasks B and C. This implies that our system is complimentary with the IR system.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Analysis of Attention Mechanism</section-title>
<section-number>14</section-number>
<paragraphs>
In addition to quantitative analysis, it is natural to qualitatively evaluate the performance of the attention mechanism by visualizing the weight distribution of each instance. We randomly picked several instances from the test set in task A, for which the sentence lengths are more moderate for demonstration. These examples are shown in Figure 5 , and categorized into short, long, and noisy sentences for discussion. A darker blue patch refers to a larger weight relative to other words in the same sentence.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Short Sentences</section-title>
<section-number>15</section-number>
<paragraphs>
Figure 5 illustrates two cQA examples whose questions are relatively short. The comments corresponding to these questions are “...snorkeling two days ago off the coast of dukhan...” and “the doha international airport...”. We can observe that our model successfully learns to focus on the most representative part of the question pertaining to classifying the relationship, which is "place for snorkeling" for the first example and “place can ... visited in qatar” for the second example.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Long Sentences</section-title>
<section-number>16</section-number>
<paragraphs>
In Figure 5 , we investigate two examples with longer questions, which both contain 63 words. Interestingly, the distribution of weights does not become more uniform; the model still focuses attention on a small number of hot words, for example, “puppy dog for ... mall” and “hectic driving in doha ... car insurance ... quite costly”. Additionally, some words that appear frequently but carry little information for classification are assigned very small weights, such as I/we/my, is/am, like, and to.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Noisy Sentence</section-title>
<section-number>17</section-number>
<paragraphs>
Due to the open nature of cQA forums, some content is noisy. Figure 5 is an example with excessive usage of question marks. Again, our model exhibits its robustness by allocating very low weights to the noise symbols and therefore excludes the noninformative content.
</paragraphs>
</section>

---Paper Title: Recurrent Neural Network Encoder with Attention for Community Question Answering---
<section>
<section-title>Conclusion</section-title>
<section-number>18</section-number>
<paragraphs>
In this paper, we demonstrate that a general RNN encoder framework can be applied to community question answering tasks. By adding a neural attention mechanism, we showed quantitatively and qualitatively that attention can improve the RNN encoder framework. To deal with a more realistic scenario, we expanded the framework to incorporate metadata as augmented inputs to a FNN classifier, and pretrained models on larger datasets, increasing both stability and performance. Our model is consistently better than or comparable to a strong feature-rich baseline system, and is superior to an IR-based system when there is a reasonable amount of training data.

Our model is complimentary with an IR-based system that uses vast amounts of external resources but trained for general purposes. By combining the two systems, it exceeds the feature-rich and IR-based system in all three tasks.

Moreover, our approach is also language independent. We have also performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task. The results are competitive with a hand-tuned strong baseline from SemEval-2015.

Future work could proceed in two directions: first, we can enrich the existing system by incorporating available metadata and preprocessing data with morphological normalization and out-of-vocabulary mappings; second, we can reinforce our model by carrying out word-by-word and history-aware attention mechanisms instead of attending only when reading the last word.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Recent Advances in Neural Question Generation</title>
<abstract>Emerging research in Neural Question Generation (NQG) has started to integrate a larger variety of inputs, and generating questions requiring higher levels of cognition. These trends point to NQG as a bellwether for NLP, about how human intelligence embodies the skills of curiosity and integration. We present a comprehensive survey of neural question generation, examining the corpora, methodologies, and evaluation methods. From this, we elaborate on what we see as emerging on NQG's trend: in terms of the learning paradigms, input modalities, and cognitive levels considered by NQG. We end by pointing out the potential directions ahead.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Question Generation (QG) concerns the task of “automatically generating questions from various inputs such as raw text, database, or semantic representation" BIBREF0 . People have the ability to ask rich, creative, and revealing questions BIBREF1 ; e.g., asking Why did Gollum betray his master Frodo Baggins? after reading the fantasy novel The Lord of the Rings. How can machines be endowed with the ability to ask relevant and to-the-point questions, given various inputs? This is a challenging, complementary task to Question Answering (QA). Both QA and QG require an in-depth understanding of the input source and the ability to reason over relevant contexts. But beyond understanding, QG additionally integrates the challenges of Natural Language Generation (NLG), i.e., generating grammatically and semantically correct questions.

QG is of practical importance: in education, forming good questions are crucial for evaluating students’ knowledge and stimulating self-learning. QG can generate assessments for course materials BIBREF2 or be used as a component in adaptive, intelligent tutoring systems BIBREF3 . In dialog systems, fluent QG is an important skill for chatbots, e.g., in initiating conversations or obtaining specific information from human users. QA and reading comprehension also benefit from QG, by reducing the needed human labor for creating large-scale datasets. We can say that traditional QG mainly focused on generating factoid questions from a single sentence or a paragraph, spurred by a series of workshops during 2008–2012 BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 .

Recently, driven by advances in deep learning, QG research has also begun to utilize “neural” techniques, to develop end-to-end neural models to generate deeper questions BIBREF8 and to pursue broader applications BIBREF9 , BIBREF10 .

While there have been considerable advances made in NQG, the area lacks a comprehensive survey. This paper fills this gap by presenting a systematic survey on recent development of NQG, focusing on three emergent trends that deep learning has brought in QG: (1) the change of learning paradigm, (2) the broadening of the input spectrum, and (3) the generation of deep questions.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Fundamental Aspects of NQG</section-title>
<section-number>1</section-number>
<paragraphs>
For the sake of clean exposition, we first provide a broad overview of QG by conceptualizing the problem from the perspective of the three introduced aspects: (1) its learning paradigm, (2) its input modalities, and (3) the cognitive level it involves. This combines past research with recent trends, providing insights on how NQG connects to traditional QG research.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Learning Paradigm</section-title>
<section-number>2</section-number>
<paragraphs>
QG research traditionally considers two fundamental aspects in question asking: “What to ask” and “How to ask”. A typical QG task considers the identification of the important aspects to ask about (“what to ask”), and learning to realize such identified aspects as natural language (“how to ask”). Deciding what to ask is a form of machine understanding: a machine needs to capture important information dependent on the target application, akin to automatic summarization. Learning how to ask, however, focuses on aspects of the language quality such as grammatical correctness, semantically preciseness and language flexibility.

Past research took a reductionist approach, separately considering these two problems of “what” and “how” via content selection and question construction. Given a sentence or a paragraph as input, content selection selects a particular salient topic worthwhile to ask about and determines the question type (What, When, Who, etc.). Approaches either take a syntactic BIBREF11 , BIBREF12 , BIBREF13 or semantic BIBREF14 , BIBREF3 , BIBREF15 , BIBREF16 tack, both starting by applying syntactic or semantic parsing, respectively, to obtain intermediate symbolic representations. Question construction then converts intermediate representations to a natural language question, taking either a tranformation- or template-based approach. The former BIBREF17 , BIBREF18 , BIBREF13 rearranges the surface form of the input sentence to produce the question; the latter BIBREF19 , BIBREF20 , BIBREF21 generates questions from pre-defined question templates. Unfortunately, such QG architectures are limiting, as their representation is confined to the variety of intermediate representations, transformation rules or templates.

In contrast, neural models motivate an end-to-end architectures. Deep learned frameworks contrast with the reductionist approach, admitting approaches that jointly optimize for both the “what” and “how” in an unified framework. The majority of current NQG models follow the sequence-to-sequence (Seq2Seq) framework that use a unified representation and joint learning of content selection (via the encoder) and question construction (via the decoder). In this framework, traditional parsing-based content selection has been replaced by more flexible approaches such as attention BIBREF22 and copying mechanism BIBREF23 . Question construction has become completely data-driven, requiring far less labor compared to transformation rules, enabling better language flexibility compared to question templates.

However, unlike other Seq2Seq learning NLG tasks, such as Machine Translation, Image Captioning, and Abstractive Summarization, which can be loosely regarded as learning a one-to-one mapping, generated questions can differ significantly when the intent of asking differs (e.g., the target answer, the target aspect to ask about, and the question's depth). In Section "Methodology" , we summarize different NQG methodologies based on Seq2Seq framework, investigating how some of these QG-specific factors are integrated with neural models, and discussing what could be further explored. The change of learning paradigm in NQG era is also represented by multi-task learning with other NLP tasks, for which we discuss in Section "Multi-task Learning" .
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Input Modality</section-title>
<section-number>3</section-number>
<paragraphs>
Question generation is an NLG task for which the input has a wealth of possibilities depending on applications. While a host of input modalities have been considered in other NLG tasks, such as text summarization BIBREF24 , image captioning BIBREF25 and table-to-text generation BIBREF26 , traditional QG mainly focused on textual inputs, especially declarative sentences, explained by the original application domains of question answering and education, which also typically featured textual inputs.

Recently, with the growth of various QA applications such as Knowledge Base Question Answering (KBQA) BIBREF27 and Visual Question Answering (VQA) BIBREF28 , NQG research has also widened the spectrum of sources to include knowledge bases BIBREF29 and images BIBREF10 . This trend is also spurred by the remarkable success of neural models in feature representation, especially on image features BIBREF30 and knowledge representations BIBREF31 . We discuss adapting NQG models to other input modalities in Section "Wider Input Modalities" .
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Cognitive Levels</section-title>
<section-number>4</section-number>
<paragraphs>
Finally, we consider the required cognitive process behind question asking, a distinguishing factor for questions BIBREF32 . A typical framework that attempts to categorize the cognitive levels involved in question asking comes from Bloom's taxonomy BIBREF33 , which has undergone several revisions and currently has six cognitive levels: Remembering, Understanding, Applying, Analyzing, Evaluating and Creating BIBREF32 .

Traditional QG focuses on shallow levels of Bloom's taxonomy: typical QG research is on generating sentence-based factoid questions (e.g., Who, What, Where questions), whose answers are simple constituents in the input sentence BIBREF2 , BIBREF13 . However, a QG system achieving human cognitive level should be able to generate meaningful questions that cater to higher levels of Bloom's taxonomy BIBREF34 , such as Why, What-if, and How questions. Traditionally, those “deep” questions are generated through shallow methods such as handcrafted templates BIBREF20 , BIBREF21 ; however, these methods lack a real understanding and reasoning over the input.

Although asking deep questions is complex, NQG's ability to generalize over voluminous data has enabled recent research to explore the comprehension and reasoning aspects of QG BIBREF35 , BIBREF1 , BIBREF8 , BIBREF34 . We investigate this trend in Section "Generation of Deep Questions" , examining the limitations of current Seq2Seq model in generating deep questions, and the efforts made by existing works, indicating further directions ahead.

The rest of this paper provides a systematic survey of NQG, covering corpus and evaluation metrics before examining specific neural models.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Corpora</section-title>
<section-number>5</section-number>
<paragraphs>
As QG can be regarded as a dual task of QA, in principle any QA dataset can be used for QG as well. However, there are at least two corpus-related factors that affect the difficulty of question generation. The first is the required cognitive level to answer the question, as we discussed in the previous section. Current NQG has achieved promising results on datasets consisting mainly of shallow factoid questions, such as SQuAD BIBREF36 and MS MARCO BIBREF38 . However, the performance drops significantly on deep question datasets, such as LearningQ BIBREF8 , shown in Section "Generation of Deep Questions" . The second factor is the answer type, i.e., the expected form of the answer, typically having four settings: (1) the answer is a text span in the passage, which is usually the case for factoid questions, (2) human-generated, abstractive answer that may not appear in the passage, usually the case for deep questions, (3) multiple choice question where question and its distractors should be jointly generated, and (4) no given answer, which requires the model to automatically learn what is worthy to ask. The design of NQG system differs accordingly.

Table 1 presents a listing of the NQG corpora grouped by their cognitive level and answer type, along with their statistics. Among them, SQuAD was used by most groups as the benchmark to evaluate their NQG models. This provides a fair comparison between different techniques. However, it raises the issue that most NQG models work on factoid questions with answer as text span, leaving other types of QG problems less investigated, such as generating deep multi-choice questions. To overcome this, a wider variety of corpora should be benchmarked against in future NQG research.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Evaluation Metrics</section-title>
<section-number>6</section-number>
<paragraphs>
Although the datasets are commonly shared between QG and QA, it is not the case for evaluation: it is challenging to define a gold standard of proper questions to ask. Meaningful, syntactically correct, semantically sound and natural are all useful criteria, yet they are hard to quantify. Most QG systems involve human evaluation, commonly by randomly sampling a few hundred generated questions, and asking human annotators to rate them on a 5-point Likert scale. The average rank or the percentage of best-ranked questions are reported and used for quality marks.

As human evaluation is time-consuming, common automatic evaluation metrics for NLG, such as BLEU BIBREF41 , METEOR BIBREF42 , and ROUGE BIBREF43 , are also widely used. However, some studies BIBREF44 , BIBREF45 have shown that these metrics do not correlate well with fluency, adequacy, coherence, as they essentially compute the $n$ -gram similarity between the source sentence and the generated question. To overcome this, BIBREF46 proposed a new metric to evaluate the “answerability” of a question by calculating the scores for several question-specific factors, including question type, content words, function words, and named entities. However, as it is newly proposed, it has not been applied to evaluate any NQG system yet.

To accurately measure what makes a good question, especially deep questions, improved evaluation schemes are required to specifically investigate the mechanism of question asking.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Methodology</section-title>
<section-number>7</section-number>
<paragraphs>
Many current NQG models follow the Seq2Seq architecture. Under this framework, given a passage (usually a sentence) $X = (x_1, \cdots , x_n)$ and (possibly) a target answer $A$ (a text span in the passage) as input, an NQG model aims to generate a question $Y = (y_1, \cdots , y_m)$ asking about the target answer $A$ in the passage $X$ , which is defined as finding the best question $\bar{Y}$ that maximizes the conditional likelihood given the passage $X$ and the answer $A$ :

$$\bar{Y} & = \arg \max _Y P(Y \vert X, A) \\
\vspace{-14.22636pt}
& = \arg \max _Y \sum _{t=1}^m P(y_t \vert X, A, y_{< t})$$   (Eq. 5) 

 BIBREF47 pioneered the first NQG model using an attention Seq2Seq model BIBREF22 , which feeds a sentence into an RNN-based encoder, and generate a question about the sentence through a decoder. The attention mechanism is applied to help decoder pay attention to the most relevant parts of the input sentence while generating a question. Note that this base model does not take the target answer as input. Subsequently, neural models have adopted attention mechanism as a default BIBREF48 , BIBREF49 , BIBREF50 .

Although these NQG models all share the Seq2Seq framework, they differ in the consideration of — (1) QG-specific factors (e.g., answer encoding, question word generation, and paragraph-level contexts), and (2) common NLG techniques (e.g., copying mechanism, linguistic features, and reinforcement learning) — discussed next.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Encoding Answers</section-title>
<section-number>8</section-number>
<paragraphs>
The most commonly considered factor by current NQG systems is the target answer, which is typically taken as an additional input to guide the model in deciding which information to focus on when generating; otherwise, the NQG model tend to generate questions without specific target (e.g., “What is mentioned?"). Models have solved this by either treating the answer's position as an extra input feature BIBREF48 , BIBREF51 , or by encoding the answer with a separate RNN BIBREF49 , BIBREF52 .

The first type of method augments each input word vector with an extra answer indicator feature, indicating whether this word is within the answer span. BIBREF48 implement this feature using the BIO tagging scheme, while BIBREF50 directly use a binary indicator. In addition to the target answer, BIBREF53 argued that the context words closer to the answer also deserve more attention from the model, since they are usually more relevant. To this end, they incorporate trainable position embeddings $(d_{p_1}, d_{p_2}, \cdots , d_{p_n})$ into the computation of attention distribution, where $p_i$ is the relative distance between the $i$ -th word and the answer, and $d_{p_i}$ is the embedding of $p_i$ . This achieved an extra BLEU-4 gain of $0.89$ on SQuAD.

To generate answer-related questions, extra answer indicators explicitly emphasize the importance of answer; however, it also increases the tendency that generated questions include words from the answer, resulting in useless questions, as observed by BIBREF52 . For example, given the input “John Francis O’Hara was elected president of Notre Dame in 1934.", an improperly generated question would be “Who was elected John Francis?", which exposes some words in the answer. To address this, they propose to replace the answer into a special token for passage encoding, and a separate RNN is used to encode the answer. The outputs from two encoders are concatenated as inputs to the decoder. BIBREF54 adopted a similar idea that separately encodes passage and answer, but they instead use the multi-perspective matching between two encodings as an extra input to the decoder.

We forecast treating the passage and the target answer separately as a future trend, as it results in a more flexible model, which generalizes to the abstractive case when the answer is not a text span in the input passage. However, this inevitably increases the model complexity and difficulty in training.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Question Word Generation</section-title>
<section-number>9</section-number>
<paragraphs>
Question words (e.g., “when”, “how”, and “why”) also play a vital role in QG; BIBREF53 observed that the mismatch between generated question words and answer type is common for current NQG systems. For example, a when-question should be triggered for answer “the end of the Mexican War" while a why-question is generated by the model. A few works BIBREF49 , BIBREF53 considered question word generation separately in model design.

 BIBREF49 proposed to first generate a question template that contains question word (e.g., “how to #", where # is the placeholder), before generating the rest of the question. To this end, they train two Seq2Seq models; the former learns to generate question templates for a given text , while the latter learns to fill the blank of template to form a complete question. Instead of a two-stage framework, BIBREF53 proposed a more flexible model by introducing an additional decoding mode that generates the question word. When entering this mode, the decoder produces a question word distribution based on a restricted set of vocabulary using the answer embedding, the decoder state, and the context vector. The switch between different modes is controlled by a discrete variable produced by a learnable module of the model in each decoding step.

Determining the appropriate question word harks back to question type identification, which is correlated with the question intention, as different intents may yield different questions, even when presented with the same (passage, answer) input pair. This points to the direction of exploring question pragmatics, where external contextual information (such as intent) can inform and influence how questions should optimally be generated.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Paragraph-level Contexts</section-title>
<section-number>10</section-number>
<paragraphs>
Leveraging rich paragraph-level contexts around the input text is another natural consideration to produce better questions. According to BIBREF47 , around 20% of questions in SQuAD require paragraph-level information to be answered. However, as input texts get longer, Seq2Seq models have a tougher time effectively utilizing relevant contexts, while avoiding irrelevant information.

To address this challenge, BIBREF51 proposed a gated self-attention encoder to refine the encoded context by fusing important information with the context's self-representation properly, which has achieved state-of-the-art results on SQuAD. The long passage consisting of input texts and its context is first embedded via LSTM with answer position as an extra feature. The encoded representation is then fed through a gated self-matching network BIBREF55 to aggregate information from the entire passage and embed intra-passage dependencies. Finally, a feature fusion gate BIBREF56 chooses relevant information between the original and self-matching enhanced representations.

Instead of leveraging the whole context, BIBREF57 performed a pre-filtering by running a coreference resolution system on the context passage to obtain coreference clusters for both the input sentence and the answer. The co-referred sentences are then fed into a gating network, from which the outputs serve as extra features to be concatenated with the original input vectors.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Answer-unaware QG</section-title>
<section-number>11</section-number>
<paragraphs>
The aforementioned models require the target answer as an input, in which the answer essentially serves as the focus of asking. However, in the case that only the input passage is given, a QG system should automatically identify question-worthy parts within the passage. This task is synonymous with content selection in traditional QG. To date, only two works BIBREF58 , BIBREF59 have worked in this setting. They both follow the traditional decomposition of QG into content selection and question construction but implement each task using neural networks. For content selection, BIBREF58 learn a sentence selection task to identify question-worthy sentences from the input paragraph using a neural sequence tagging model. BIBREF59 train a neural keyphrase extractor to predict keyphrases of the passage. For question construction, they both employed the Seq2Seq model, for which the input is either the selected sentence or the input passage with keyphrases as target answer.

However, learning what aspect to ask about is quite challenging when the question requires reasoning over multiple pieces of information within the passage; cf the Gollum question from the introduction. Beyond retrieving question-worthy information, we believe that studying how different reasoning patterns (e.g., inductive, deductive, causal and analogical) affects the generation process will be an aspect for future study.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Technical Considerations</section-title>
<section-number>12</section-number>
<paragraphs>
Common techniques of NLG have also been considered in NQG model, summarized as 3 tactics:

1. Copying Mechanism. Most NQG models BIBREF48 , BIBREF60 , BIBREF61 , BIBREF50 , BIBREF62 employ the copying mechanism of BIBREF23 , which directly copies relevant words from the source sentence to the question during decoding. This idea is widely accepted as it is common to refer back to phrases and entities appearing in the text when formulating factoid questions, and difficult for a RNN decoder to generate such rare words on its own.

2. Linguistic Features. Approaches also seek to leverage additional linguistic features that complements word embeddings, including word case, POS and NER tags BIBREF48 , BIBREF61 as well as coreference BIBREF50 and dependency information BIBREF62 . These categorical features are vectorized and concatenated with word embeddings. The feature vectors can be either one-hot or trainable and serve as input to the encoder.

3. Policy Gradient. Optimizing for just ground-truth log likelihood ignores the many equivalent ways of asking a question. Relevant QG work BIBREF60 , BIBREF63 have adopted policy gradient methods to add task-specific rewards (such as BLEU or ROUGE) to the original objective. This helps to diversify the questions generated, as the model learns to distribute probability mass among equivalent expressions rather than the single ground truth question.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>The State of the Art</section-title>
<section-number>13</section-number>
<paragraphs>
In Table 2 , we summarize existing NQG models with their employed techniques and their best-reported performance on SQuAD. These methods achieve comparable results; as of this writing, BIBREF51 is the state-of-the-art.

Two points deserve mention. First, while the copying mechanism has shown marked improvements, there exist shortcomings. BIBREF52 observed many invalid answer-revealing questions attributed to the use of the copying mechanism; cf the John Francis example in Section "Emerging Trends" . They abandoned copying but still achieved a performance rivaling other systems. In parallel application areas such as machine translation, the copy mechanism has been to a large extent replaced with self-attention BIBREF64 or transformer BIBREF65 . The future prospect of the copying mechanism requires further investigation. Second, recent approaches that employ paragraph-level contexts have shown promising results: not only boosting performance, but also constituting a step towards deep question generation, which requires reasoning over rich contexts.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Emerging Trends</section-title>
<section-number>14</section-number>
<paragraphs>
We discuss three trends that we wish to call practitioners' attention to as NQG evolves to take the center stage in QG: Multi-task Learning, Wider Input Modalities and Deep Question Generation.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Multi-task Learning</section-title>
<section-number>15</section-number>
<paragraphs>
As QG has become more mature, work has started to investigate how QG can assist in other NLP tasks, and vice versa. Some NLP tasks benefit from enriching training samples by QG to alleviate the data shortage problem. This idea has been successfully applied to semantic parsing BIBREF66 and QA BIBREF67 . In the semantic parsing task that maps a natural language question to a SQL query, BIBREF66 achieved a 3 $\%$ performance gain with an enlarged training set that contains pseudo-labeled $(SQL, question)$ pairs generated by a Seq2Seq QG model. In QA, BIBREF67 employed the idea of self-training BIBREF68 to jointly learn QA and QG. The QA and QG models are first trained on a labeled corpus. Then, the QG model is used to create more questions from an unlabeled text corpus and the QA model is used to answer these newly-created questions. The newly-generated question–answer pairs form an enlarged dataset to iteratively retrain the two models. The process is repeated while performance of both models improve.

Investigating the core aspect of QG, we say that a well-trained QG system should have the ability to: (1) find the most salient information in the passage to ask questions about, and (2) given this salient information as target answer, to generate an answer related question. BIBREF69 leveraged the first characteristic to improve text summarization by performing multi-task learning of summarization with QG, as both these two tasks require the ability to search for salient information in the passage. BIBREF49 applied the second characteristic to improve QA. For an input question $q$ and a candidate answer $\hat{a}$ , they generate a question $\hat{q}$ for $\hat{a}$ by way of QG system. Since the generated question $\hat{q}$ is closely related to $\hat{a}$ , the similarity between $q$ and $\hat{q}$ helps to evaluate whether $\hat{a}$ is the correct answer.

Other works focus on jointly training to combine QG and QA. BIBREF70 simultaneously train the QG and QA models in the same Seq2Seq model by alternating input data between QA and QG examples. BIBREF71 proposed a training algorithm that generalizes Generative Adversarial Network (GANs) BIBREF72 under the question answering scenario. The model improves QG by incorporating an additional QA-specific loss, and improving QA performance by adding artificially generated training instances from QG. However, while joint training has shown some effectiveness, due to the mixed objectives, its performance on QG are lower than the state-of-the-art results, which leaves room for future exploration.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Wider Input Modalities</section-title>
<section-number>16</section-number>
<paragraphs>
QG work now has incorporated input from knowledge bases (KBQG) and images (VQG).

Inspired by the use of SQuAD as a question benchmark, BIBREF9 created a 30M large-scale dataset of (KB triple, question) pairs to spur KBQG work. They baselined an attention seq2seq model to generate the target factoid question. Due to KB sparsity, many entities and predicates are unseen or rarely seen at training time. BIBREF73 address these few-/zero-shot issues by applying the copying mechanism and incorporating textual contexts to enrich the information for rare entities and relations. Since a single KB triple provides only limited information, KB-generated questions also overgeneralize — a model asks “Who was born in New York?" when given the triple (Donald_Trump, Place_of_birth, New_York). To solve this, BIBREF29 enrich the input with a sequence of keywords collected from its related triples.

Visual Question Generation (VQG) is another emerging topic which aims to ask questions given an image. We categorize VQG into grounded- and open-ended VQG by the level of cognition. Grounded VQG generates visually grounded questions, i.e., all relevant information for the answer can be found in the input image BIBREF74 . A key purpose of grounded VQG is to support the dataset construction for VQA. To ensure the questions are grounded, existing systems rely on image captions to varying degrees. BIBREF75 and BIBREF76 simply convert image captions into questions using rule-based methods with textual patterns. BIBREF74 proposed a neural model that can generate questions with diverse types for a single image, using separate networks to construct dense image captions and to select question types.

In contrast to grounded QG, humans ask higher cognitive level questions about what can be inferred rather than what can be seen from an image. Motivated by this, BIBREF10 proposed open-ended VQG that aims to generate natural and engaging questions about an image. These are deep questions that require high cognition such as analyzing and creation. With significant progress in deep generative models, marked by variational auto-encoders (VAEs) and GANs, such models are also used in open-ended VQG to bring “creativity” into generated questions BIBREF77 , BIBREF78 , showing promising results. This also brings hope to address deep QG from text, as applied in NLG: e.g., SeqGAN BIBREF79 and LeakGAN BIBREF80 .
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Generation of Deep Questions</section-title>
<section-number>17</section-number>
<paragraphs>
Endowing a QG system with the ability to ask deep questions will help us build curious machines that can interact with humans in a better manner. However, BIBREF81 pointed out that asking high-quality deep questions is difficult, even for humans. Citing the study from BIBREF82 to show that students in college asked only about 6 deep-reasoning questions per hour in a question–encouraging tutoring session. These deep questions are often about events, evaluation, opinions, syntheses or reasons, corresponding to higher-order cognitive levels.

To verify the effectiveness of existing NQG models in generating deep questions, BIBREF8 conducted an empirical study that applies the attention Seq2Seq model on LearningQ, a deep-question centric dataset containing over 60 $\%$ questions that require reasoning over multiple sentences or external knowledge to answer. However, the results were poor; the model achieved miniscule BLEU-4 scores of $< 4$ and METEOR scores of $< 9$ , compared with $> 12$ (BLEU-4) and $> 16$ (METEOR) on SQuAD. Despite further in-depth analysis are needed to explore the reasons behind, we believe there are two plausible explanations: (1) Seq2Seq models handle long inputs ineffectively, and (2) Seq2Seq models lack the ability to reason over multiple pieces of information.

Despite still having a long way to go, some works have set out a path forward. A few early QG works attempted to solve this through building deep semantic representations of the entire text, using concept maps over keywords BIBREF83 or minimal recursion semantics BIBREF84 to reason over concepts in the text. BIBREF35 proposed a crowdsourcing-based workflow that involves building an intermediate ontology for the input text, soliciting question templates through crowdsourcing, and generating deep questions based on template retrieval and ranking. Although this process is semi-automatic, it provides a practical and efficient way towards deep QG. In a separate line of work, BIBREF1 proposed a framework that simulates how people ask deep questions by treating questions as formal programs that execute on the state of the world, outputting an answer.

Based on our survey, we believe the roadmap towards deep NGQ points towards research that will (1) enhance the NGQ model with the ability to consider relationships among multiple source sentences, (2) explicitly model typical reasoning patterns, and (3) understand and simulate the mechanism behind human question asking.
</paragraphs>
</section>

---Paper Title: Recent Advances in Neural Question Generation---
<section>
<section-title>Conclusion – What's the Outlook?</section-title>
<section-number>18</section-number>
<paragraphs>
We have presented a comprehensive survey of NQG, categorizing current NQG models based on different QG-specific and common technical variations, and summarizing three emerging trends in NQG: multi-task learning, wider input modalities, and deep question generation.

What's next for NGQ? We end with future potential directions by applying past insights to current NQG models; the “unknown unknown", promising directions yet explored.

When to Ask: Besides learning what and how to ask, in many real-world applications that question plays an important role, such as automated tutoring and conversational systems, learning when to ask become an important issue. In contrast to general dialog management BIBREF85 , no research has explored when machine should ask an engaging question in dialog. Modeling question asking as an interactive and dynamic process may become an interesting topic ahead.

Personalized QG: Question asking is quite personalized: people with different characters and knowledge background ask different questions. However, integrating QG with user modeling in dialog management or recommendation system has not yet been explored. Explicitly modeling user state and awareness leads us towards personalized QG, which dovetails deep, end-to-end QG with deep user modeling and pairs the dual of generation–comprehension much in the same vein as in the vision–image generation area.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Dynamic Memory Networks for Visual and Textual Question Answering</title>
<abstract>Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \babi-10k text question-answering dataset without supporting fact supervision.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Neural network based methods have made tremendous progress in image and text classification BIBREF0 , BIBREF1 . However, only recently has progress been made on more complex tasks that require logical reasoning. This success is based in part on the addition of memory and attention components to complex neural networks. For instance, memory networks BIBREF2 are able to reason over several facts written in natural language or (subject, relation, object) triplets. Attention mechanisms have been successful components in both machine translation BIBREF3 , BIBREF4 and image captioning models BIBREF5 .

The dynamic memory network BIBREF6 (DMN) is one example of a neural network model that has both a memory component and an attention mechanism. The DMN yields state of the art results on question answering with supporting facts marked during training, sentiment analysis, and part-of-speech tagging.

We analyze the DMN components, specifically the input module and memory module, to improve question answering. We propose a new input module which uses a two level encoder with a sentence reader and input fusion layer to allow for information flow between sentences. For the memory, we propose a modification to gated recurrent units (GRU) BIBREF7 . The new GRU formulation incorporates attention gates that are computed using global knowledge over the facts. Unlike before, the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training. The model learns to select the important facts from a larger set.

In addition, we introduce a new input module to represent images. This module is compatible with the rest of the DMN architecture and its output is fed into the memory module. We show that the changes in the memory module that improved textual question answering also improve visual question answering. Both tasks are illustrated in Fig. 1 .
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Dynamic Memory Networks</section-title>
<section-number>1</section-number>
<paragraphs>
We begin by outlining the DMN for question answering and the modules as presented in BIBREF6 .

The DMN is a general architecture for question answering (QA). It is composed of modules that allow different aspects such as input representations or memory components to be analyzed and improved independently. The modules, depicted in Fig. 1 , are as follows:

Input Module: This module processes the input data about which a question is being asked into a set of vectors termed facts, represented as $F=[f_1,\hdots ,f_N]$ , where $N$ is the total number of facts. These vectors are ordered, resulting in additional information that can be used by later components. For text QA in BIBREF6 , the module consists of a GRU over the input words.

As the GRU is used in many components of the DMN, it is useful to provide the full definition. For each time step $i$ with input $x_i$ and previous hidden state $h_{i-1}$ , we compute the updated hidden state $h_i = GRU(x_i,h_{i-1})$ by 

$$u_i &=& \sigma \left(W^{(u)}x_{i} + U^{(u)} h_{i-1} + b^{(u)} \right)\\
r_i &=& \sigma \left(W^{(r)}x_{i} + U^{(r)} h_{i-1} + b^{(r)} \right)\\
\tilde{h}_i &=& \tanh \left(Wx_{i} + r_i \circ U h_{i-1} + b^{(h)}\right)\\
h_i &=& u_i\circ \tilde{h}_i + (1-u_i) \circ h_{i-1}$$   (Eq. 2) 

where $\sigma $ is the sigmoid activation function, $\circ $ is an element-wise product, $W^{(z)}, W^{(r)}, W \in \mathbb {R}^{n_H \times n_I}$ , $U^{(z)}, U^{(r)}, U \in \mathbb {R}^{n_H \times n_H}$ , $n_H$ is the hidden size, and $n_I$ is the input size.

Question Module: This module computes a vector representation $q$ of the question, where $q \in \mathbb {R}^{n_H}$ is the final hidden state of a GRU over the words in the question.

Episodic Memory Module: Episode memory aims to retrieve the information required to answer the question $q$ from the input facts. To improve our understanding of both the question and input, especially if questions require transitive reasoning, the episode memory module may pass over the input multiple times, updating episode memory after each pass. We refer to the episode memory on the $t^{th}$ pass over the inputs as $m^t$ , where $m^t \in \mathbb {R}^{n_H}$ , the initial memory vector is set to the question vector: $m^0 = q$ .

The episodic memory module consists of two separate components: the attention mechanism and the memory update mechanism. The attention mechanism is responsible for producing a contextual vector $c^t$ , where $c^t \in \mathbb {R}^{n_H}$ is a summary of relevant input for pass $t$ , with relevance inferred by the question $q$ and previous episode memory $m^{t-1}$ . The memory update mechanism is responsible for generating the episode memory $m^t$ based upon the contextual vector $c^t$ and previous episode memory $m^{t-1}$ . By the final pass $T$ , the episodic memory $m^T$ should contain all the information required to answer the question $c^t \in \mathbb {R}^{n_H}$0 .

Answer Module: The answer module receives both $q$ and $m^T$ to generate the model's predicted answer. For simple answers, such as a single word, a linear layer with softmax activation may be used. For tasks requiring a sequence output, an RNN may be used to decode $a = [q ; m^T]$ , the concatenation of vectors $q$ and $m^T$ , to an ordered set of tokens. The cross entropy error on the answers is used for training and backpropagated through the entire network.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Improved Dynamic Memory Networks: DMN+</section-title>
<section-number>2</section-number>
<paragraphs>
We propose and compare several modeling choices for two crucial components: input representation, attention mechanism and memory update. The final DMN+ model obtains the highest accuracy on the bAbI-10k dataset without supporting facts and the VQA dataset BIBREF8 . Several design choices are motivated by intuition and accuracy improvements on that dataset.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Input Module for Text QA</section-title>
<section-number>3</section-number>
<paragraphs>
In the DMN specified in BIBREF6 , a single GRU is used to process all the words in the story, extracting sentence representations by storing the hidden states produced at the end of sentence markers. The GRU also provides a temporal component by allowing a sentence to know the content of the sentences that came before them. Whilst this input module worked well for bAbI-1k with supporting facts, as reported in BIBREF6 , it did not perform well on bAbI-10k without supporting facts (Sec. "Model Analysis" ).

We speculate that there are two main reasons for this performance disparity, all exacerbated by the removal of supporting facts. First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU.

Input Fusion Layer

For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, responsible only for encoding the words into a sentence embedding. The second component is the input fusion layer, allowing for interactions between sentences. This resembles the hierarchical neural auto-encoder architecture of BIBREF9 and allows content interaction between sentences. We adopt the bi-directional GRU for this input fusion layer because it allows information from both past and future sentences to be used. As gradients do not need to propagate through the words between sentences, the fusion layer also allows for distant supporting sentences to have a more direct interaction.

Fig. 2 shows an illustration of an input module, where a positional encoder is used for the sentence reader and a bi-directional GRU is adopted for the input fusion layer. Each sentence encoding $f_i$ is the output of an encoding scheme taking the word tokens $[w^i_1, \hdots , w^i_{M_i}]$ , where $M_i$ is the length of the sentence.

The sentence reader could be based on any variety of encoding schemes. We selected positional encoding described in BIBREF10 to allow for a comparison to their work. GRUs and LSTMs were also considered but required more computational resources and were prone to overfitting if auxiliary tasks, such as reconstructing the original sentence, were not used.

For the positional encoding scheme, the sentence representation is produced by $f_i = \sum ^{j=1}_M l_j \circ w^i_j$ , where $\circ $ is element-wise multiplication and $l_j$ is a column vector with structure $l_{jd} = (1 - j / M) - (d / D) (1 - 2j / M)$ , where $d$ is the embedding index and $D$ is the dimension of the embedding.

The input fusion layer takes these input facts and enables an information exchange between them by applying a bi-directional GRU. 

$$\overrightarrow{f_i} = GRU_{fwd}(f_i, \overrightarrow{f_{i-1}}) \\
\overleftarrow{f_{i}} = GRU_{bwd}(f_{i}, \overleftarrow{f_{i+1}}) \\
\overleftrightarrow{f_i} = \overleftarrow{f_i} + \overrightarrow{f_i}$$   (Eq. 5) 

where $f_i$ is the input fact at timestep $i$ , $ \overrightarrow{f_i}$ is the hidden state of the forward GRU at timestep $i$ , and $\overleftarrow{f_i}$ is the hidden state of the backward GRU at timestep $i$ . This allows contextual information from both future and past facts to impact $\overleftrightarrow{f_i}$ .

We explored a variety of encoding schemes for the sentence reader, including GRUs, LSTMs, and the positional encoding scheme described in BIBREF10 . For simplicity and speed, we selected the positional encoding scheme. GRUs and LSTMs were also considered but required more computational resources and were prone to overfitting if auxiliary tasks, such as reconstructing the original sentence, were not used.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Input Module for VQA</section-title>
<section-number>4</section-number>
<paragraphs>
To apply the DMN to visual question answering, we introduce a new input module for images. The module splits an image into small local regions and considers each region equivalent to a sentence in the input module for text. The input module for VQA is composed of three parts, illustrated in Fig. 3 : local region feature extraction, visual feature embedding, and the input fusion layer introduced in Sec. "Input Module for Text QA" .

Local region feature extraction: To extract features from the image, we use a convolutional neural network BIBREF0 based upon the VGG-19 model BIBREF11 . We first rescale the input image to $448 \times 448$ and take the output from the last pooling layer which has dimensionality $d = 512 \times 14 \times 14$ . The pooling layer divides the image into a grid of $14 \times 14$ , resulting in 196 local regional vectors of $d = 512$ .

Visual feature embedding: As the VQA task involves both image features and text features, we add a linear layer with tanh activation to project the local regional vectors to the textual feature space used by the question vector $q$ .

Input fusion layer: The local regional vectors extracted from above do not yet have global information available to them. Without global information, their representational power is quite limited, with simple issues like object scaling or locational variance causing accuracy problems.

To solve this, we add an input fusion layer similar to that of the textual input module described in Sec. "Input Module for Text QA" . First, to produce the input facts $F$ , we traverse the image in a snake like fashion, as seen in Figure 3 . We then apply a bi-directional GRU over these input facts $F$ to produce the globally aware input facts $\overleftrightarrow{F}$ . The bi-directional GRU allows for information propagation from neighboring image patches, capturing spatial information.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>The Episodic Memory Module</section-title>
<section-number>5</section-number>
<paragraphs>
The episodic memory module, as depicted in Fig. 4 , retrieves information from the input facts $\overleftrightarrow{F} = [\overleftrightarrow{f_1}, \hdots , \overleftrightarrow{f_N}]$ provided to it by focusing attention on a subset of these facts. We implement this attention by associating a single scalar value, the attention gate $g^t_i$ , with each fact $\overleftrightarrow{f}_i$ during pass $t$ . This is computed by allowing interactions between the fact and both the question representation and the episode memory state. 

$$z^t_i &=& [\overleftrightarrow{f_i} \circ q; \overleftrightarrow{f_i} \circ m^{t-1}; \vert \overleftrightarrow{f_i} - q \vert ; \vert \overleftrightarrow{f_i} - m^{t-1} \vert ] \\
Z^t_i &=& W^{(2)} \tanh \left(W^{(1)}z^t_i + b^{(1)} \right)+ b^{(2)} \\
g^t_i &=& \frac{\exp (Z^t_i)}{\sum _{k=1}^{M_i} \exp (Z^t_k)} $$   (Eq. 10) 

where $\overleftrightarrow{f_i}$ is the $i^{th}$ fact, $m^{t-1}$ is the previous episode memory, $q$ is the original question, $\circ $ is the element-wise product, $|\cdot |$ is the element-wise absolute value, and $;$ represents concatenation of the vectors.

The DMN implemented in BIBREF6 involved a more complex set of interactions within $z$ , containing the additional terms $[f; m^{t-1}; q; f^T W^{(b)} q; f^T W^{(b)} m^{t-1}]$ . After an initial analysis, we found these additional terms were not required.

Attention Mechanism

Once we have the attention gate $g^t_i$ we use an attention mechanism to extract a contextual vector $c^t$ based upon the current focus. We focus on two types of attention: soft attention and a new attention based GRU. The latter improves performance and is hence the final modeling choice for the DMN+.

Soft attention: Soft attention produces a contextual vector $c^t$ through a weighted summation of the sorted list of vectors $\overleftrightarrow{F}$ and corresponding attention gates $g_i^t$ : $c^t = \sum _{i=1}^N g^t_i \overleftrightarrow{f}_i$ This method has two advantages. First, it is easy to compute. Second, if the softmax activation is spiky it can approximate a hard attention function by selecting only a single fact for the contextual vector whilst still being differentiable. However the main disadvantage to soft attention is that the summation process loses both positional and ordering information. Whilst multiple attention passes can retrieve some of this information, this is inefficient.

Attention based GRU: For more complex queries, we would like for the attention mechanism to be sensitive to both the position and ordering of the input facts $\overleftrightarrow{F}$ . An RNN would be advantageous in this situation except they cannot make use of the attention gate from Equation .

We propose a modification to the GRU architecture by embedding information from the attention mechanism. The update gate $u_i$ in Equation 2 decides how much of each dimension of the hidden state to retain and how much should be updated with the transformed input $x_i$ from the current timestep. As $u_i$ is computed using only the current input and the hidden state from previous timesteps, it lacks any knowledge from the question or previous episode memory.

By replacing the update gate $u_i$ in the GRU (Equation 2 ) with the output of the attention gate $g^t_i$ (Equation ) in Equation , the GRU can now use the attention gate for updating its internal state. This change is depicted in Fig 5 . 

$$h_i &=& g^t_i \circ \tilde{h}_i + (1-g^t_i) \circ h_{i-1}$$   (Eq. 12) 

An important consideration is that $g^t_i$ is a scalar, generated using a softmax activation, as opposed to the vector $u_i \in \mathbb {R}^{n_H}$ , generated using a sigmoid activation. This allows us to easily visualize how the attention gates activate over the input, later shown for visual QA in Fig. 6 . Though not explored, replacing the softmax activation in Equation with a sigmoid activation would result in $g^t_i \in \mathbb {R}^{n_H}$ . To produce the contextual vector $c^t$ used for updating the episodic memory state $m^t$ , we use the final hidden state of the attention based GRU.

Episode Memory Updates

After each pass through the attention mechanism, we wish to update the episode memory $m^{t-1}$ with the newly constructed contextual vector $c^t$ , producing $m^t$ . In the DMN, a GRU with the initial hidden state set to the question vector $q$ is used for this purpose. The episodic memory for pass $t$ is computed by 

$$m^t = GRU(c^t, m^{t-1})$$   (Eq. 13) 

The work of BIBREF10 suggests that using different weights for each pass through the episodic memory may be advantageous. When the model contains only one set of weights for all episodic passes over the input, it is referred to as a tied model, as in the “Mem Weights” row in Table 1 .

Following the memory update component used in BIBREF10 and BIBREF12 we experiment with using a ReLU layer for the memory update, calculating the new episode memory state by 

$$m^t = ReLU\left(W^t [m^{t-1} ; c^t ; q] + b\right)$$   (Eq. 14) 

where $;$ is the concatenation operator, $W^t \in \mathbb {R}^{n_H \times n_H}$ , $b \in \mathbb {R}^{n_H}$ , and $n_H$ is the hidden size. The untying of weights and using this ReLU formulation for the memory update improves accuracy by another 0.5% as shown in Table 1 in the last column. The final output of the memory network is passed to the answer module as in the original DMN.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Related Work</section-title>
<section-number>6</section-number>
<paragraphs>
The DMN is related to two major lines of recent work: memory and attention mechanisms. We work on both visual and textual question answering which have, until now, been developed in separate communities.

Neural Memory Models The earliest recent work with a memory component that is applied to language processing is that of memory networks BIBREF2 which adds a memory component for question answering over simple facts. They are similar to DMNs in that they also have input, scoring, attention and response mechanisms. However, unlike the DMN their input module computes sentence representations independently and hence cannot easily be used for other tasks such as sequence labeling. Like the original DMN, this memory network requires that supporting facts are labeled during QA training. End-to-end memory networks BIBREF10 do not have this limitation. In contrast to previous memory models with a variety of different functions for memory attention retrieval and representations, DMNs BIBREF6 have shown that neural sequence models can be used for input representation, attention and response mechanisms. Sequence models naturally capture position and temporality of both the inputs and transitive reasoning steps.

Neural Attention Mechanisms Attention mechanisms allow neural network models to use a question to selectively pay attention to specific inputs. They can benefit image classification BIBREF13 , generating captions for images BIBREF5 , among others mentioned below, and machine translation BIBREF14 , BIBREF3 , BIBREF4 . Other recent neural architectures with memory or attention which have proposed include neural Turing machines BIBREF15 , neural GPUs BIBREF16 and stack-augmented RNNs BIBREF17 .

Question Answering in NLP Question answering involving natural language can be solved in a variety of ways to which we cannot all do justice. If the potential input is a large text corpus, QA becomes a combination of information retrieval and extraction BIBREF18 . Neural approaches can include reasoning over knowledge bases, BIBREF19 , BIBREF20 or directly via sentences for trivia competitions BIBREF21 .

Visual Question Answering (VQA) In comparison to QA in NLP, VQA is still a relatively young task that is feasible only now that objects can be identified with high accuracy. The first large scale database with unconstrained questions about images was introduced by BIBREF8 . While VQA datasets existed before they did not include open-ended, free-form questions about general images BIBREF22 . Others are were too small to be viable for a deep learning approach BIBREF23 . The only VQA model which also has an attention component is the stacked attention network BIBREF24 . Their work also uses CNN based features. However, unlike our input fusion layer, they use a single layer neural network to map the features of each patch to the dimensionality of the question vector. Hence, the model cannot easily incorporate adjacency of local information in its hidden state. A model that also uses neural modules, albeit logically inspired ones, is that by BIBREF25 who evaluate on knowledgebase reasoning and visual question answering. We compare directly to their method on the latter task and dataset.

Related to visual question answering is the task of describing images with sentences BIBREF26 . BIBREF27 used deep learning methods to map images and sentences into the same space in order to describe images with sentences and to find images that best visualize a sentence. This was the first work to map both modalities into a joint space with deep learning methods, but it could only select an existing sentence to describe an image. Shortly thereafter, recurrent neural networks were used to generate often novel sentences based on images BIBREF28 , BIBREF29 , BIBREF30 , BIBREF5 .
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Datasets</section-title>
<section-number>7</section-number>
<paragraphs>
To analyze our proposed model changes and compare our performance with other architectures, we use three datasets.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>bAbI-10k</section-title>
<section-number>8</section-number>
<paragraphs>
For evaluating the DMN on textual question answering, we use bAbI-10k English BIBREF31 , a synthetic dataset which features 20 different tasks. Each example is composed of a set of facts, a question, the answer, and the supporting facts that lead to the answer. The dataset comes in two sizes, referring to the number of training examples each task has: bAbI-1k and bAbI-10k. The experiments in BIBREF10 found that their lowest error rates on the smaller bAbI-1k dataset were on average three times higher than on bAbI-10k.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>DAQUAR-ALL visual dataset</section-title>
<section-number>9</section-number>
<paragraphs>
The DAtaset for QUestion Answering on Real-world images (DAQUAR) BIBREF23 consists of 795 training images and 654 test images. Based upon these images, 6,795 training questions and 5,673 test questions were generated. Following the previously defined experimental method, we exclude multiple word answers BIBREF32 , BIBREF33 . The resulting dataset covers 90% of the original data. The evaluation method uses classification accuracy over the single words. We use this as a development dataset for model analysis (Sec. "Model Analysis" ).
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Visual Question Answering</section-title>
<section-number>10</section-number>
<paragraphs>
The Visual Question Answering (VQA) dataset was constructed using the Microsoft COCO dataset BIBREF34 which contained 123,287 training/validation images and 81,434 test images. Each image has several related questions with each question answered by multiple people. This dataset contains 248,349 training questions, 121,512 validation questions, and 244,302 for testing. The testing data was split into test-development, test-standard and test-challenge in BIBREF8 .

Evaluation on both test-standard and test-challenge are implemented via a submission system. test-standard may only be evaluated 5 times and test-challenge is only evaluated at the end of the competition. To the best of our knowledge, VQA is the largest and most complex image dataset for the visual question answering task.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Model Analysis</section-title>
<section-number>11</section-number>
<paragraphs>
To understand the impact of the proposed module changes, we analyze the performance of a variety of DMN models on textual and visual question answering datasets.

The original DMN (ODMN) is the architecture presented in BIBREF6 without any modifications. DMN2 only replaces the input module with the input fusion layer (Sec. "Input Module for Text QA" ). DMN3, based upon DMN2, replaces the soft attention mechanism with the attention based GRU proposed in Sec. "The Episodic Memory Module" . Finally, DMN+, based upon DMN3, is an untied model, using a unique set of weights for each pass and a linear layer with a ReLU activation to compute the memory update. We report the performance of the model variations in Table 1 .

A large improvement to accuracy on both the bAbI-10k textual and DAQUAR visual datasets results from updating the input module, seen when comparing ODMN to DMN2. On both datasets, the input fusion layer improves interaction between distant facts. In the visual dataset, this improvement is purely from providing contextual information from neighboring image patches, allowing it to handle objects of varying scale or questions with a locality aspect. For the textual dataset, the improved interaction between sentences likely helps the path finding required for logical reasoning when multiple transitive steps are required.

The addition of the attention GRU in DMN3 helps answer questions where complex positional or ordering information may be required. This change impacts the textual dataset the most as few questions in the visual dataset are likely to require this form of logical reasoning. Finally, the untied model in the DMN+ overfits on some tasks compared to DMN3, but on average the error rate decreases.

From these experimental results, we find that the combination of all the proposed model changes results, culminating in DMN+, achieves the highest performance across both the visual and textual datasets.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Comparison to state of the art using bAbI-10k</section-title>
<section-number>12</section-number>
<paragraphs>
We trained our models using the Adam optimizer BIBREF35 with a learning rate of 0.001 and batch size of 128. Training runs for up to 256 epochs with early stopping if the validation loss had not improved within the last 20 epochs. The model from the epoch with the lowest validation loss was then selected. Xavier initialization was used for all weights except for the word embeddings, which used random uniform initialization with range $[-\sqrt{3}, \sqrt{3}]$ . Both the embedding and hidden dimensions were of size $d = 80$ . We used $\ell _2$ regularization on all weights except bias and used dropout on the initial sentence encodings and the answer module, keeping the input with probability $p=0.9$ . The last 10% of the training data on each task was chosen as the validation set. For all tasks, three passes were used for the episodic memory module, allowing direct comparison to other state of the art methods. Finally, we limited the input to the last 70 sentences for all tasks except QA3 for which we limited input to the last 130 sentences, similar to BIBREF10 .

On some tasks, the accuracy was not stable across multiple runs. This was particularly problematic on QA3, QA17, and QA18. To solve this, we repeated training 10 times using random initializations and evaluated the model that achieved the lowest validation set loss.

Text QA Results

We compare our best performing approach, DMN+, to two state of the art question answering architectures: the end to end memory network (E2E) BIBREF10 and the neural reasoner framework (NR) BIBREF12 . Neither approach use supporting facts for training.

The end-to-end memory network is a form of memory network BIBREF2 tested on both textual question answering and language modeling. The model features both explicit memory and a recurrent attention mechanism. We select the model from the paper that achieves the lowest mean error over the bAbI-10k dataset. This model utilizes positional encoding for input, RNN-style tied weights for the episode module, and a ReLU non-linearity for the memory update component.

The neural reasoner framework is an end-to-end trainable model which features a deep architecture for logical reasoning and an interaction-pooling mechanism for allowing interaction over multiple facts. While the neural reasoner framework was only tested on QA17 and QA19, these were two of the most challenging question types at the time.

In Table 2 we compare the accuracy of these question answering architectures, both as mean error and error on individual tasks. The DMN+ model reduces mean error by 1.4% compared to the the end-to-end memory network, achieving a new state of the art for the bAbI-10k dataset.

One notable deficiency in our model is that of QA16: Basic Induction. In BIBREF10 , an untied model using only summation for memory updates was able to achieve a near perfect error rate of $0.4$ . When the memory update was replaced with a linear layer with ReLU activation, the end-to-end memory network's overall mean error decreased but the error for QA16 rose sharply. Our model experiences the same difficulties, suggesting that the more complex memory update component may prevent convergence on certain simpler tasks.

The neural reasoner model outperforms both the DMN and end-to-end memory network on QA17: Positional Reasoning. This is likely as the positional reasoning task only involves minimal supervision - two sentences for input, yes/no answers for supervision, and only 5,812 unique examples after removing duplicates from the initial 10,000 training examples. BIBREF12 add an auxiliary task of reconstructing both the original sentences and question from their representations. This auxiliary task likely improves performance by preventing overfitting.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Comparison to state of the art using VQA</section-title>
<section-number>13</section-number>
<paragraphs>
For the VQA dataset, each question is answered by multiple people and the answers may not be the same, the generated answers are evaluated using human consensus. For each predicted answer $a_i$ for the $i_{th}$ question with target answer set $T^{i}$ , the accuracy of VQA: $Acc_{VQA} = \frac{1}{N}\sum _{i=1}^Nmin(\frac{\sum _{t\in T^i}{1}_{(a_i==t)}}{3},1)$ where ${1}_{(\cdot )}$ is the indicator function. Simply put, the answer $a_i$ is only 100 $\%$ accurate if at least 3 people provide that exact answer.

Training Details We use the Adam optimizer BIBREF35 with a learning rate of 0.003 and batch size of 100. Training runs for up to 256 epochs with early stopping if the validation loss has not improved in the last 10 epochs. For weight initialization, we sampled from a random uniform distribution with range $[-0.08, 0.08]$ . Both the word embedding and hidden layers were vectors of size $d=512$ . We apply dropout on the initial image output from the VGG convolutional neural network BIBREF11 as well as the input to the answer module, keeping input with probability $p=0.5$ .

Results and Analysis

The VQA dataset is composed of three question domains: Yes/No, Number, and Other. This enables us to analyze the performance of the models on various tasks that require different reasoning abilities.

The comparison models are separated into two broad classes: those that utilize a full connected image feature for classification and those that perform reasoning over multiple small image patches. Only the SAN and DMN approach use small image patches, while the rest use the fully-connected whole image feature approach.

Here, we show the quantitative and qualitative results in Table 3 and Fig. 6 , respectively. The images in Fig. 6 illustrate how the attention gate $g^t_i$ selectively activates over relevant portions of the image according to the query. In Table 3 , our method outperforms baseline and other state-of-the-art methods across all question domains (All) in both test-dev and test-std, and especially for Other questions, achieves a wide margin compared to the other architectures, which is likely as the small image patches allow for finely detailed reasoning over the image.

However, the granularity offered by small image patches does not always offer an advantage. The Number questions may be not solvable for both the SAN and DMN architectures, potentially as counting objects is not a simple task when an object crosses image patch boundaries.
</paragraphs>
</section>

---Paper Title: Dynamic Memory Networks for Visual and Textual Question Answering---
<section>
<section-title>Conclusion</section-title>
<section-number>14</section-number>
<paragraphs>
We have proposed new modules for the DMN framework to achieve strong results without supervision of supporting facts. These improvements include the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering</title>
<abstract>While question answering (QA) with neural network, i.e. neural QA, has achieved promising results in recent years, lacking of large scale real-word QA dataset is still a challenge for developing and evaluating neural QA system. To alleviate this problem, we propose a large scale human annotated real-world QA dataset WebQA with more than 42k questions and 556k evidences. As existing neural QA methods resolve QA either as sequence generation or classification/ranking problem, they face challenges of expensive softmax computation, unseen answers handling or separate candidate answer generation component. In this work, we cast neural QA as a sequence labeling problem and propose an end-to-end sequence labeling model, which overcomes all the above challenges. Experimental results on WebQA show that our model outperforms the baselines significantly with an F1 score of 74.69% with word-based input, and the performance drops only 3.72 F1 points with more challenging character-based input.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Question answering (QA) with neural network, i.e. neural QA, is an active research direction along the road towards the long-term AI goal of building general dialogue agents BIBREF0 . Unlike conventional methods, neural QA does not rely on feature engineering and is (at least nearly) end-to-end trainable. It reduces the requirement for domain specific knowledge significantly and makes domain adaption easier. Therefore, it has attracted intensive attention in recent years.

Resolving QA problem requires several fundamental abilities including reasoning, memorization, etc. Various neural methods have been proposed to improve such abilities, including neural tensor networks BIBREF1 , recursive networks BIBREF2 , convolution neural networks BIBREF3 , BIBREF4 , BIBREF5 , attention models BIBREF6 , BIBREF5 , BIBREF7 , and memories BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , etc. These methods achieve promising results on various datasets, which demonstrates the high potential of neural QA. However, we believe there are still two major challenges for neural QA:

System development and/or evaluation on real-world data: Although several high quality and well-designed QA datasets have been proposed in recent years, there are still problems about using them to develop and/or evaluate QA system under real-world settings due to data size and the way they are created. For example, bAbI BIBREF0 and the 30M Factoid Question-Answer Corpus BIBREF13 are artificially synthesized; the TREC datasets BIBREF14 , Free917 BIBREF15 and WebQuestions BIBREF16 are human generated but only have few thousands of questions; SimpleQuestions BIBREF11 and the CNN and Daily Mail news datasets BIBREF6 are large but generated under controlled conditions. Thus, a new large-scale real-world QA dataset is needed.

A new design choice for answer production besides sequence generation and classification/ranking: Without loss of generality, the methods used for producing answers in existing neural QA works can be roughly categorized into the sequence generation type and the classification/ranking type. The former generates answers word by word, e.g. BIBREF0 , BIBREF10 , BIBREF6 . As it generally involves INLINEFORM0 computation over a large vocabulary, the computational cost is remarkably high and it is hard to produce answers with out-of-vocabulary word. The latter produces answers by classification over a predefined set of answers, e.g. BIBREF12 , or ranking given candidates by model score, e.g. BIBREF5 . Although it generally has lower computational cost than the former, it either also has difficulties in handling unseen answers or requires an extra candidate generating component which is hard for end-to-end training. Above all, we need a new design choice for answer production that is both computationally effective and capable of handling unseen words/answers.

In this work, we address the above two challenges by a new dataset and a new neural QA model. Our contributions are two-fold:

Experimental results show that our model outperforms baselines with a large margin on the WebQA dataset, indicating that it is effective. Furthermore, our model even achieves an F1 score of 70.97% on character-based input, which is comparable with the 74.69% F1 score on word-based input, demonstrating that our model is robust.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Factoid QA as Sequence Labeling</section-title>
<section-number>1</section-number>
<paragraphs>
In this work, we focus on open-domain factoid QA. Taking Figure FIGREF3 as an example, we formalize the problem as follows: given each question Q, we have one or more evidences E, and the task is to produce the answer A, where an evidence is a piece of text of any length that contains relevant information to answer the question. The advantage of this formalization is that evidences can be retrieved from web or unstructured knowledge base, which can improve system coverage significantly.

Inspired by BIBREF18 , we introduce end-to-end sequence labeling as a new design choice for answer production in neural QA. Given a question and an evidence, we use CRF BIBREF17 to assign a label to each word in the evidence to indicate whether the word is at the beginning (B), inside (I) or outside (O) of the answer (see Figure FIGREF3 for example). The key difference between our work and BIBREF18 is that BIBREF18 needs a lot work on feature engineering which further relies on POS/NER tagging, dependency parsing, question type analysis, etc. While we avoid feature engineering, and only use one single model to solve the problem. Furthermore, compared with sequence generation and classification/ranking methods for answer production, our method avoids expensive INLINEFORM0 computation and can handle unseen answers/words naturally in a principled way.

Formally, we formalize QA as a sequence labeling problem as follows: suppose we have a vocabulary INLINEFORM0 of size INLINEFORM1 , given question INLINEFORM2 and evidence INLINEFORM3 , where INLINEFORM4 and INLINEFORM5 are one-hot vectors of dimension INLINEFORM6 , and INLINEFORM7 and INLINEFORM8 are the number of words in the question and evidence respectively. The problem is to find the label sequence INLINEFORM9 which maximizes the conditional probability under parameter INLINEFORM10 DISPLAYFORM0 

In this work, we model INLINEFORM0 by a neural network composed of LSTMs and CRF.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Overview</section-title>
<section-number>2</section-number>
<paragraphs>
Figure FIGREF4 shows the structure of our model. The model consists of three components: (1) question LSTM for computing question representation; (2) evidence LSTMs for evidence analysis; and (3) a CRF layer for sequence labeling. The question LSTM in a form of a single layer LSTM equipped with a single time attention takes the question as input and generates the question representation INLINEFORM0 . The three-layer evidence LSTMs takes the evidence, question representation INLINEFORM1 and optional features as input and produces “features” for the CRF layer. The CRF layer takes the “features” as input and produces the label sequence. The details will be given in the following sections.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Long Short-Term Memory (LSTM)</section-title>
<section-number>3</section-number>
<paragraphs>
Following BIBREF19 , we define INLINEFORM0 as a function mapping its input INLINEFORM1 , previous state INLINEFORM2 and output INLINEFORM3 to current state INLINEFORM4 and output INLINEFORM5 : DISPLAYFORM0 

where INLINEFORM0 are parameter matrices, INLINEFORM1 are biases, INLINEFORM2 is LSTM layer width, INLINEFORM3 is the INLINEFORM4 function, INLINEFORM5 , INLINEFORM6 and INLINEFORM7 are the input gate, forget gate and output gate respectively.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Question LSTM</section-title>
<section-number>4</section-number>
<paragraphs>
The question LSTM consists of a single-layer LSTM and a single-time attention model. The question INLINEFORM0 is fed into the LSTM to produce a sequence of vector representations INLINEFORM1 DISPLAYFORM0 

where INLINEFORM0 is the embedding matrix and INLINEFORM1 is word embedding dimension. Then a weight INLINEFORM2 is computed by the single-time attention model for each INLINEFORM3 DISPLAYFORM0 

where INLINEFORM0 and INLINEFORM1 . And finally the weighted average INLINEFORM2 of INLINEFORM3 is used as the representation of the question DISPLAYFORM0 
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Evidence LSTMs</section-title>
<section-number>5</section-number>
<paragraphs>
The three-layer evidence LSTMs processes evidence INLINEFORM0 INLINEFORM1 to produce “features” for the CRF layer.

The first LSTM layer takes evidence INLINEFORM0 , question representation INLINEFORM1 and optional features as input. We find the following two simple common word indicator features are effective:

Question-Evidence common word feature (q-e.comm): for each word in the evidence, the feature has value 1 when the word also occurs in the question, otherwise 0. The intuition is that words occurring in questions tend not to be part of the answers for factoid questions.



Evidence-Evidence common word feature (e-e.comm): for each word in the evidence, the feature has value 1 when the word occurs in another evidence, otherwise 0. The intuition is that words shared by two or more evidences are more likely to be part of the answers.

Although counterintuitive, we found non-binary e-e.comm feature values does not work well. Because the more evidences we considered, the more words tend to get non-zero feature values, and the less discriminative the feature is.

The second LSTM layer stacks on top of the first LSTM layer, but processes its output in a reverse order. The third LSTM layer stacks upon the first and second LSTM layers with cross layer links, and its output serves as features for CRF layer.

Formally, the computations are defined as follows DISPLAYFORM0 

where INLINEFORM0 and INLINEFORM1 are one-hot feature vectors, INLINEFORM2 and INLINEFORM3 are embeddings for the features, and INLINEFORM4 and INLINEFORM5 are the feature embedding dimensions. Note that we use the same word embedding matrix INLINEFORM6 as in question LSTM.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Sequence Labeling</section-title>
<section-number>6</section-number>
<paragraphs>
Following BIBREF20 , BIBREF21 , we use CRF on top of evidence LSTMs for sequence labeling. The probability of a label sequence INLINEFORM0 given question INLINEFORM1 and evidence INLINEFORM2 is computed as DISPLAYFORM0 

where INLINEFORM0 , INLINEFORM1 , INLINEFORM2 is the number of label types, INLINEFORM3 is the transition weight from label INLINEFORM4 to INLINEFORM5 , and INLINEFORM6 is the INLINEFORM7 -th value of vector INLINEFORM8 .
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Training</section-title>
<section-number>7</section-number>
<paragraphs>
The objective function of our model is INLINEFORM0 

where INLINEFORM0 is the golden label sequence, and INLINEFORM1 is training set.

We use a minibatch stochastic gradient descent (SGD) BIBREF22 algorithm with rmsprop BIBREF23 to minimize the objective function. The initial learning rate is 0.001, batch size is 120, and INLINEFORM0 . We also apply dropout BIBREF24 to the output of all the LSTM layers. The dropout rate is 0.05. All these hyper-parameters are determined empirically via grid search on validation set.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>WebQA Dataset</section-title>
<section-number>8</section-number>
<paragraphs>
In order to train and evaluate open-domain factoid QA system for real-world questions, we build a new Chinese QA dataset named as WebQA. The dataset consists of tuples of (question, evidences, answer), which is similar to example in Figure FIGREF3 . All the questions, evidences and answers are collected from web. Table TABREF20 shows some statistics of the dataset.

The questions and answers are mainly collected from a large community QA website Baidu Zhidao and a small portion are from hand collected web documents. Therefore, all these questions are indeed asked by real-world users in daily life instead of under controlled conditions. All the questions are of single-entity factoid type, which means (1) each question is a factoid question and (2) its answer involves only one entity (but may have multiple words). The question in Figure FIGREF3 is a positive example, while the question “Who are the children of Albert Enistein?” is a counter example because the answer involves three persons. The type and correctness of all the question answer pairs are verified by at least two annotators.

All the evidences are retrieved from Internet by using a search engine with questions as queries. We download web pages returned in the first 3 result pages and take all the text pieces which have no more than 5 sentences and include at least one question word as candidate evidences. As evidence retrieval is beyond the scope of this work, we simply use TF-IDF values to re-rank these candidates.

For each question in the training set, we provide the top 10 ranked evidences to annotate (“Annotated Evidence” in Table TABREF20 ). An evidence is annotated as positive if the question can be answered by just reading the evidence without any other prior knowledge, otherwise negative. Only evidences whose annotations are agreed by at least two annotators are retained. We also provide trivial negative evidences (“Retrieved Evidence” in Table TABREF20 ), i.e. evidences that do not contain golden standard answers.

For each question in the validation and test sets, we provide one major positive evidence, and maybe an additional positive one to compute features. Both of them are annotated. Raw retrieved evidences are also provided for evaluation purpose (“Retrieved Evidence” in Table TABREF20 ).

The dataset will be released on the project page http://idl.baidu.com/WebQA.html.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Baselines</section-title>
<section-number>9</section-number>
<paragraphs>
We compare our model with two sets of baselines:

MemN2N BIBREF12 is an end-to-end trainable version of memory networks BIBREF9 . It encodes question and evidence with a bag-of-word method and stores the representations of evidences in an external memory. A recurrent attention model is used to retrieve relevant information from the memory to answer the question.

Attentive and Impatient Readers BIBREF6 use bidirectional LSTMs to encode question and evidence, and do classification over a large vocabulary based on these two encodings. The simpler Attentive Reader uses a similar way as our work to compute attention for the evidence. And the more complex Impatient Reader computes attention after processing each question word.

The key difference between our model and the two readers is that they produce answer by doing classification over a large vocabulary, which is computationally expensive and has difficulties in handling unseen words. However, as our model uses an end-to-end trainable sequence labeling technique, it avoids both of the two problems by its nature.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Evaluation Method</section-title>
<section-number>10</section-number>
<paragraphs>
The performance is measured with precision (P), recall (R) and F1-measure (F1) DISPLAYFORM0 

where INLINEFORM0 is the list of correctly answered questions, INLINEFORM1 is the list of produced answers, and INLINEFORM2 is the list of all questions .

As WebQA is collected from web, the same answer may be expressed in different surface forms in the golden standard answer and the evidence, e.g. “北京 (Beijing)” v.s. “北京市 (Beijing province)”. Therefore, we use two ways to count correctly answered questions, which are referred to as “strict” and “fuzzy” in the tables:

Strict matching: A question is counted if and only if the produced answer is identical to the golden standard answer;

Fuzzy matching: A question is counted if and only if the produced answer is a synonym of the golden standard answer;

And we also consider two evaluation settings:

Annotated evidence: Each question has one major annotated evidence and maybe another annotated evidence for computing q-e.comm and e-e.comm features (Section SECREF14 );

Retrieved evidence: Each question is provided with at most 20 automatically retrieved evidences (see Section SECREF5 for details). All the evidences will be processed by our model independently and answers are voted by frequency to decide the final result. Note that a large amount of the evidences are negative and our model should not produce any answer for them.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Model Settings</section-title>
<section-number>11</section-number>
<paragraphs>
If not specified, the following hyper-parameters will be used in the reset of this section: LSTM layer width INLINEFORM0 (Section SECREF7 ), word embedding dimension INLINEFORM1 (Section SECREF9 ), feature embedding dimension INLINEFORM2 (Section SECREF9 ). The word embeddings are initialized with pre-trained embeddings using a 5-gram neural language model BIBREF25 and is fixed during training.

We will show that injecting noise data is important for improving performance on retrieved evidence setting in Section SECREF37 . In the following experiments, 20% of the training evidences will be negative ones randomly selected on the fly, of which 25% are annotated negative evidences and 75% are retrieved trivial negative evidences (Section SECREF5 ). The percentages are determined empirically. Intuitively, we provide the noise data to teach the model learning to recognize unreliable evidence.

For each evidence, we will randomly sample another evidence from the rest evidences of the question and compare them to compute the e-e.comm feature (Section SECREF14 ). We will develop more powerful models to process multiple evidences in a more principle way in the future.

As the answer for each question in our WebQA dataset only involves one entity (Section SECREF5 ), we distinguish label Os before and after the first B in the label sequence explicitly to discourage our model to produce multiple answers for a question. For example, the golden labels for the example evidence in Figure FIGREF3 will became “Einstein/O1 married/O1 his/O1 first/O1 wife/O1 Mileva/B Marić/I in/O2 1903/O2”, where we use “O1” and “O2” to denote label Os before and after the first B . “Fuzzy matching” is also used for computing golden standard labels for training set.

For each setting, we will run three trials with different random seeds and report the average performance in the following sections.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Comparison with Baselines</section-title>
<section-number>12</section-number>
<paragraphs>
As the baselines can only predict one-word answers, we only do experiments on the one-word answer subset of WebQA, i.e. only questions with one-word answers are retained for training, validation and test. As shown in Table TABREF23 , our model achieves significant higher F1 scores than all the baselines.

The main reason for the relative low performance of MemN2N is that it uses a bag-of-word method to encode question and evidence such that higher order information like word order is absent to the model. We think its performance can be improved by designing more complex encoding methods BIBREF26 and leave it as a future work.

The Attentive and Impatient Readers only have access to the fixed length representations when doing classification. However, our model has access to the outputs of all the time steps of the evidence LSTMs, and scores the label sequence as a whole. Therefore, our model achieves better performance.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Evaluation on the Entire WebQA Dataset</section-title>
<section-number>13</section-number>
<paragraphs>
In this section, we evaluate our model on the entire WebQA dataset. The evaluation results are shown in Table TABREF24 . Although producing multi-word answers is harder, our model achieves comparable results with the one-word answer subset (Table TABREF23 ), demonstrating that our model is effective for both single-word and multi-word word settings.

“Softmax” in Table TABREF24 means we replace CRF with INLINEFORM0 , i.e. replace Eq. ( EQREF19 ) with DISPLAYFORM0 

CRF outperforms INLINEFORM0 significantly in all cases. The reason is that INLINEFORM1 predicts each label independently, suggesting that modeling label transition explicitly is essential for improving performance. A natural choice for modeling label transition in INLINEFORM2 is to take the last prediction into account as in BIBREF27 . The result is shown in Table TABREF24 as “Softmax( INLINEFORM3 -1)”. However, its performance is only comparable with “Softmax” and significantly lower than CRF. The reason is that we can enumerate all possible label sequences implicitly by dynamic programming for CRF during predicting but this is not possible for “Softmax( INLINEFORM4 -1)” , which indicates CRF is a better choice.

“Noise” in Table TABREF24 means whether we inject noise data or not (Section SECREF34 ). As all evidences are positive under the annotated evidence setting, the ability for recognizing unreliable evidence will be useless. Therefore, the performance of our model with and without noise is comparable under the annotated evidence setting. However, the ability is important to improve the performance under the retrieved evidence setting because a large amount of the retrieved evidences are negative ones. As a result, we observe significant improvement by injecting noise data for this setting.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Effect of Word Embedding</section-title>
<section-number>14</section-number>
<paragraphs>
As stated in Section SECREF34 , the word embedding INLINEFORM0 is initialized with LM embedding and kept fixed in training. We evaluate different initialization and optimization methods in this section. The evaluation results are shown in Table TABREF40 . The second row shows the results when the embedding is optimized jointly during training. The performance drops significantly. Detailed analysis reveals that the trainable embedding enlarge trainable parameter number and the model gets over fitting easily. The model acts like a context independent entity tagger to some extend, which is not desired. For example, the model will try to find any location name in the evidence when the word “在哪 (where)” occurs in the question. In contrary, pre-trained fixed embedding forces the model to pay more attention to the latent syntactic regularities. And it also carries basic priors such as “梨 (pear)” is fruit and “李世石 (Lee Sedol)” is a person, thus the model will generalize better to test data with fixed embedding. The third row shows the result when the embedding is randomly initialized and jointly optimized. The performance drops significantly further, suggesting that pre-trained embedding indeed carries meaningful priors.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Effect of q-e.comm and e-e.comm Features</section-title>
<section-number>15</section-number>
<paragraphs>
As shown in Table TABREF41 , both the q-e.comm and e-e.comm features are effective, and the q-e.comm feature contributes more to the overall performance. The reason is that the interaction between question and evidence is limited and q-e.comm feature with value 1, i.e. the corresponding word also occurs in the question, is a strong indication that the word may not be part of the answer.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Effect of Question Representations</section-title>
<section-number>16</section-number>
<paragraphs>
In this section, we compare the single-time attention method for computing INLINEFORM0 ( INLINEFORM1 , Eq. ( EQREF12 , EQREF13 )) with two widely used options: element-wise max operation INLINEFORM2 : INLINEFORM3 and element-wise average operation INLINEFORM4 : INLINEFORM5 . Intuitively, INLINEFORM6 can distill information in a more flexible way from { INLINEFORM7 }, while INLINEFORM8 tends to hide the differences between them, and INLINEFORM9 lies between INLINEFORM10 and INLINEFORM11 . The results in Table TABREF41 suggest that the more flexible and selective the operation is, the better the performance is.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Effect of Evidence LSTMs Structures</section-title>
<section-number>17</section-number>
<paragraphs>
We investigate the effect of evidence LSTMs layer number, layer width and cross layer links in this section. The results are shown in Figure TABREF46 . For fair comparison, we do not use cross layer links in Figure TABREF46 (a) (dotted lines in Figure FIGREF4 ), and highlight the results with cross layer links (layer width 64) with circle and square for retrieved and annotated evidence settings respectively. We can conclude that: (1) generally the deeper and wider the model is, the better the performance is; (2) cross layer links are effective as they make the third evidence LSTM layer see information in both directions.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Word-based v.s. Character-based Input</section-title>
<section-number>18</section-number>
<paragraphs>
Our model achieves fuzzy matching F1 scores of 69.78% and 70.97% on character-based input in annotated and retrieved evidence settings respectively (Table TABREF46 ), which are only 3.72 and 3.72 points lower than the corresponding scores on word-based input respectively. The performance is promising, demonstrating that our model is robust and effective.
</paragraphs>
</section>

---Paper Title: Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering---
<section>
<section-title>Conclusion and Future Work</section-title>
<section-number>19</section-number>
<paragraphs>
In this work, we build a new human annotated real-world QA dataset WebQA for developing and evaluating QA system on real-world QA data. We also propose a new end-to-end recurrent sequence labeling model for QA. Experimental results show that our model outperforms baselines significantly.

There are several future directions we plan to pursue. First, multi-entity factoid and non-factoid QA are also interesting topics. Second, we plan to extend our model to multi-evidence cases. Finally, inspired by Residual Network BIBREF28 , we will investigate deeper and wider models in the future.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data</title>
<abstract>With the growing number and size of Linked Data datasets, it is crucial to make the data accessible and useful for users without knowledge of formal query languages. Two approaches towards this goal are knowledge graph visualization and natural language interfaces. Here, we investigate specifically question answering (QA) over Linked Data by comparing a diagrammatic visual approach with existing natural language-based systems. Given a QA benchmark (QALD7), we evaluate a visual method which is based on iteratively creating diagrams until the answer is found, against four QA systems that have natural language queries as input. Besides other benefits, the visual approach provides higher performance, but also requires more manual input. The results indicate that the methods can be used complementary, and that such a combination has a large positive impact on QA performance, and also facilitates additional features such as data exploration.</abstract>
<sections>
<section>
<section-title>INTRODUCTION</section-title>
<section-number>0</section-number>
<paragraphs>
The Semantic Web provides a large number of structured datasets in form of Linked Data. One central obstacle is to make this data available and consumable to lay users without knowledge of formal query languages such as SPARQL. In order to satisfy specific information needs of users, a typical approach are natural language interfaces to allow question answering over the Linked Data (QALD) by translating user queries into SPARQL BIBREF0 , BIBREF1 . As an alternative method, BIBREF2 propose a visual method of QA using an iterative diagrammatic approach. The diagrammatic approach relies on the visual means only, it requires more user interaction than natural language QA, but also provides additional benefits like intuitive insights into dataset characteristics, or a broader understanding of the answer and the potential to further explore the answer context, and finally allows for knowledge sharing by storing and sharing resulting diagrams.

In contrast to BIBREF2 , who present the basic method and tool for diagrammatic question answering (DQA), here we evaluate DQA in comparison to natural language QALD systems. Both approaches have different characteristics, therefore we see them as complementary rather than in competition.

The basic research goals are: i) Given a dataset extracted from the QALD7 benchmark, we evaluate DQA versus state-of-the-art QALD systems. ii) More specifically, we investigate if and to what extent DQA can be complementary to QALD systems, especially in cases where those systems do not find a correct answer. iii) Finally, we want to present the basic outline for the integration of the two methods.

In a nutshell, users that applied DQA found the correct answer with an F1-score of 79.5%, compared to a maximum of 59.2% for the best performing QALD system. Furthermore, for the subset of questions where the QALD system could not provide a correct answer, users found the answer with 70% F1-score with DQA. We further analyze the characteristics of questions where the QALD or DQA, respectively, approach is better suited.

The results indicate, that aside from the other benefits of DQA, it can be a valuable component for integration into larger QALD systems, in cases where those systems cannot find an answer, or when the user wants to explore the answer context in detail by visualizing the relevant nodes and relations. Moreover, users can verify answers given by a QALD system using DQA in case of doubt.

This publication is organized as follows: After the presentation of related work in Section SECREF2 , and a brief system description of the DQA tool in Section SECREF3 , the main focus of the paper is on evaluation setup and results of the comparison of DQA and QALD, including a discussion, in Section SECREF4 . The paper concludes with Section SECREF5 .
</paragraphs>
</section>

---Paper Title: A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data---
<section>
<section-title>RELATED WORK</section-title>
<section-number>1</section-number>
<paragraphs>
As introduced in BIBREF2 we understand diagrammatic question answering (DQA) as the process of QA relying solely on visual exploration using diagrams as a representation of the underlying knowledge source. The process includes (i) a model for diagrammatic representation of semantic data which supports data interaction using embedded queries, (ii) a simple method for step-by-step construction of diagrams with respect to cognitive boundaries and a layout that boosts understandability of diagrams, (iii) a library for visual data exploration and sharing based on its internal data model, and (iv) an evaluation of DQA as knowledge understanding and knowledge sharing tool. BIBREF3 propose a framework of five perspectives of knowledge visualization, which can be used to describe certain aspects of the DQA use cases, such as its goal to provide an iterative exploration method, which is accessible to any user, the possibility of knowledge sharing (via saved diagrams), or the general purpose of knowledge understanding and abstraction from technical details.

Many tools exist for visual consumption and interaction with RDF knowledge bases, however, they are not designed specifically towards the question answering use case. BIBREF4 give an overview of ontology and Linked Data visualization tools, and categorize them based on the used visualization methods, interaction techniques and supported ontology constructs.

Regarding language-based QA over Linked Data, BIBREF5 discuss and study the usefulness of natural language interfaces to ontology-based knowledge bases in a general way. They focus on usability of such systems for the end user, and conclude that users prefer full sentences for query formulation and that natural language interfaces are indeed useful.

 BIBREF0 describe the challenges of QA over knowledge bases using natural languages, and elaborate the various techniques used by existing QALD systems to overcome those challenges. In the present work, we compare DQA with four of those systems using a subset of questions of the QALD7 benchmark. Those systems are: gAnswer BIBREF6 is an approach for RDF QA that has a “graph-driven” perspective. In contrast to traditional approaches, which first try to understand the question, and then evaluate the query, in gAnswer the intention of the query is modeled in a structured way, which leads to a subgraph matching problem. Secondly, QAKiS BIBREF7 is QA system over structured knowledge bases such as DBpedia that makes use of relational patterns which capture different ways to express a certain relation in a natural language in order to construct a target-language (SPARQL) query. Further, Platypus BIBREF8 is a QA system on Wikidata. It represents questions in an internal format related to dependency-based compositional semantics which allows for question decomposition and language independence. The platform can answer complex questions in several languages by using hybrid grammatical and template-based techniques. And finally, also the WDAqua BIBREF0 system aims for language-independence and for being agnostic of the underlying knowledge base. WDAqua puts more importance on word semantics than on the syntax of the user query, and follows a processes of query expansion, SPARQL construction, query ranking and then making an answer decision.

For the evaluation of QA systems, several benchmarks have been proposed such as WebQuestions BIBREF9 or SimpleQuestions BIBREF10 . However, the most popular benchmarks in the Semantic Web field arise from the QALD evaluation campaign BIBREF1 . The recent QALD7 evaluation campaign includes task 4: “English question answering over Wikidata” which serves as basis to compile our evaluation dataset.
</paragraphs>
</section>

---Paper Title: A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data---
<section>
<section-title>SYSTEM DESCRIPTION</section-title>
<section-number>2</section-number>
<paragraphs>
The DQA functionality is part of the Ontodia tool. The initial idea of Ontodia was to enable the exploration of semantic graphs for ordinary users. Data exploration is about efficiently extracting knowledge from data even in situations where it is unclear what is being looked for exactly BIBREF11 .

The DQA tool uses an incremental approach to exploration typically starting from a very small number of nodes. With the context menu of a particular node, relations and related nodes can be added until the diagram fulfills the information need of the user. Figure FIGREF1 gives an example of a start node, where a user wants to learn more about the painting style of Van Gogh.

To illustrate the process, we give a brief example here. More details about the DQA tool, the motivation for DQA and diagram-based visualizations are found in previous work BIBREF2 , BIBREF12 .

As for the example, when attempting to answer a question such as “Who is the mayor of Paris?” the first step for a DQA user is finding a suitable starting point, in our case the entity Paris. The user enters “Paris” into the search box, and can then investigate the entity on the tool canvas. The information about the entity stems from the underlying dataset, for example Wikidata. The user can – in an incremental process – search in the properties of the given entity (or entities) and add relevant entities onto the canvas. In the given example, the property “head of government” connects the mayor to the city of Paris, Anne Hidalgo. The final diagram which answers the given question is presented in Figure FIGREF3 .
</paragraphs>
</section>

---Paper Title: A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data---
<section>
<section-title>EVALUATION</section-title>
<section-number>3</section-number>
<paragraphs>
Here we present the evaluation of DQA in comparison to four QALD systems.
</paragraphs>
</section>

---Paper Title: A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data---
<section>
<section-title>Evaluation Setup</section-title>
<section-number>4</section-number>
<paragraphs>
As evaluation dataset, we reuse questions from the QALD7 benchmark task 4 “QA over Wikidata”. Question selection from QALD7 is based on the principles of question classification in QA BIBREF13 . Firstly, it is necessary to define question types which correspond to different scenarios of data exploration in DQA, as well as the type of expected answers and the question focus. The question focus refers to the main information in the question which help a user find the answer. We follow the model of BIBREF14 who categorize questions by their question word into WHO, WHICH, WHAT, NAME, and HOW questions. Given the question and answer type categories, we created four questionnaires with nine questions each resulting in 36 questions from the QALD dataset. The questions were picked in equal number for five basic question categories.

20 persons participated in the DQA evaluation – 14 male and six female from eight different countries. The majority of respondents work within academia, however seven users were employed in industry. 131 diagrams (of 140 expected) were returned by the users.

The same 36 questions were answered using four QALD tools: WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8 .

For the QALD tools, a human evaluator pasted the questions as is into the natural language Web interfaces, and submitted them to the systems. Typically QALD tools provide a distinct answer, which may be a simple literal, or a set of entities which represent the answer, and which can be compared to the gold standard result. However, the WDAqua system, sometimes, additionally to the direct answer to the question, provides links to documents related to the question. We always chose the answer available via direct answer.

To assess the correctness of the answers given both by participants in the DQA experiments, and by the QALD system, we use the classic information retrieval metrics of precision (P), recall (R), and F1. INLINEFORM0 measures the fraction of relevant (correct) answer (items) given versus all answers (answer items) given. INLINEFORM1 is the faction of correct answer (parts) given divided by all correct ones in the gold answer, and INLINEFORM2 is the harmonic mean of INLINEFORM3 and INLINEFORM4 . As an example, if the question is “Where was Albert Einstein born?” (gold answer: “Ulm”), and the system gives two answers “Ulm” and “Bern”, then INLINEFORM5 , INLINEFORM6 and INLINEFORM7 .

For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. The detailed answers by the participants and available online.
</paragraphs>
</section>

---Paper Title: A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data---
<section>
<section-title>Evaluation Results and Discussion</section-title>
<section-number>5</section-number>
<paragraphs>
Table TABREF8 presents the overall evaluation metrics of DQA, and the four QALD tools studied. With the given dataset, WDAqua (56.1% F1) and gAnswer (59.2% F1) clearly outperform askplatyp.us (8.6% F1) and QAKiS (27.5% F1). Detailed results per question including the calculation of INLINEFORM0 , INLINEFORM1 and INLINEFORM2 scores are available online. DQA led to 79.5% F1 (80.1% precision and 78.5% recall).

In further evaluations, we compare DQA results to WDAqua in order to study the differences and potential complementary aspects of the approaches. We selected WDAqua as representative of QALD tools, as it provides state-of-the-art results, and is well grounded in the Semantic Web community. 

Comparing DQA and WDAqua, the first interesting question is: To what extend is DQA helpful on questions that could not be answered by the QALD system? For WDAqua the overall F1 score on our test dataset is INLINEFORM0 . For the subset of questions where WDAqua had no, or only a partial, answer, DQA users found the correct answer in INLINEFORM1 of cases. On the other hand, the subset of questions that DQA users (partially) failed to answer, were answered correctly by WDAqua with an F1 of INLINEFORM2 . If DQA is used as a backup method for questions not correctly answered with WDAqua, then overall F1 can be raised to INLINEFORM3 . The increase from INLINEFORM4 to INLINEFORM5 demonstrates the potential of DQA as complementary component in QALD systems.

As expected, questions that are difficult to answer with one approach are also harder for the other approach – as some questions in the dataset or just more complex to process and understand than others. However, almost 70% of questions not answered by WDAqua could still be answered by DQA. As examples of cases which are easier to answer for one approach than the other, a question that DQA users could answer, but where WDAqua failed is: “What is the name of the school where Obama's wife studied?”. This complex question formulation is hard to interpret correctly for a machine. In contrast to DQA, QALD systems also struggled with “Who is the son of Sonny and Cher?”. This question needs a lot of real-world knowledge to map the names Sonny and Cher to their corresponding entities. The QALD system needs to select the correct Cher entity from multiple options in Wikidata, and also to understand that “Sonny” refers to the entity Sonny Bono. The resulting answer diagram is given in Figure FIGREF17 . More simple questions, like “Who is the mayor of Paris?” were correctly answered by WDAqua, but not by all DQA users. DQA participants in this case struggled to make the leap from the noun “mayor” to the head-of-government property in Wikidata.

Regarding the limits of DQA, this method has difficulties when the answer can be obtained only with joins of queries, or when it is hard to find the initial starting entities related to question focus. For example, a question like “Show me the list of African birds that are extinct.” typically requires an intersection of two (large) sets of candidates entities, ie. all African birds and extinct birds. Such a task can easily be represented in a SPARQL query, but is hard to address with diagrams, because it would require placing, and interacting with, a huge amount of nodes on the exploration canvas.

Overall, the experiments indicate, that additionally to the use cases where QALD and DQA are useful on their own, there is a lot of potential in combining the two approaches, especially by providing a user the opportunity to explore the dataset with DQA if QALD did not find a correct answer, or when a user wants to confirm the QALD answer by checking in the underlying knowledge base. Furthermore, visually exploring the dataset provides added benefits, like understanding the dataset characteristics, sharing of resulting diagrams (if supported by the tool), and finding more information related to the original information need.

For the integration of QALD and DQA, we envision two scenarios. The first scenario addresses plain question answering, and here DQA can be added to a QALD system for cases where a user is not satisfied with a given answer. The QALD Web interface can for example have a Explore visually with diagrams button, which brings the user to a canvas on which the entities detected by the QALD system within the question and results (if any) are displayed on the canvas as starting nodes. The user will then explore the knowledge graph and find the answers in the same way as the participants in our experiments. The first scenario can lead to a large improvement in answer F1 (see above).

The second scenario of integration of QALD and DQA focuses on the exploration aspect. Even if the QALD system provides the correct answer, a user might be interested to explore the knowledge graph to validate the result and to discover more interesting information about the target entities. From an implementation and UI point of view, the same Explore visually with diagrams button and pre-population of the canvas can be used. Both scenarios also provide the additional benefits of potentially saving and sharing the created diagrams, which elaborate the relation between question and answer.
</paragraphs>
</section>

---Paper Title: A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data---
<section>
<section-title>CONCLUSIONS</section-title>
<section-number>6</section-number>
<paragraphs>
In this work, we compare two approaches to answer questions over Linked Data datasets: a visual diagrammatic approach (DQA) which involves iterative exploration of the graph, and a natural language-based (QALD). The evaluations show, that DQA can be a helpful addition to pure QALD systems, both regarding evaluation metrics (precision, recall, and F1), and also for dataset understanding and further exploration. The contributions include: i) a comparative evaluation of four QALD tools and DQA with a dataset extracted from the QALD7 benchmark, ii) an investigation into the differences and potential complementary aspects of the two approaches, and iii) the proposition of integration scenarios for QALD and DQA.

In future work we plan to study the integration of DQA and QALD, especially the aspect of automatically creating an initial diagram from a user query, in order to leverage the discussed potentials. We envision an integrated tool, that uses QALD as basic method to find an answer to a question quickly, but also allows to explore the knowledge graph visually to raise answer quality and support exploration with all its discussed benefits.
</paragraphs>
</section>

---Paper Title: A Comparative Evaluation of Visual and Natural Language Question Answering Over Linked Data---
<section>
<section-title>ACKNOWLEDGEMENTS</section-title>
<section-number>7</section-number>
<paragraphs>
This work was supported by the Government of the Russian Federation (Grant 074-U01) through the ITMO Fellowship and Professorship Program.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games</title>
<abstract>We propose the new problem of learning to recover reasoning chains from weakly supervised signals, i.e., the question-answer pairs. We propose a cooperative game approach to deal with this problem, in which how the evidence passages are selected and how the selected passages are connected are handled by two models that cooperate to select the most confident chains from a large set of candidates (from distant supervision). For evaluation, we created benchmarks based on two multi-hop QA datasets, HotpotQA and MedHop; and hand-labeled reasoning chains for the latter. The experimental results demonstrate the effectiveness of our proposed approach.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
NLP tasks that require multi-hop reasoning have recently enjoyed rapid progress, especially on multi-hop question answering BIBREF0, BIBREF1, BIBREF2. Advances have benefited from rich annotations of supporting evidence, as in the popular multi-hop QA and relation extraction benchmarks, e.g., HotpotQA BIBREF3 and DocRED BIBREF4, where the evidence sentences for the reasoning process were labeled by human annotators.

Such evidence annotations are crucial for modern model training, since they provide finer-grained supervision for better guiding the model learning. Furthermore, they allow a pipeline fashion of model training, with each step, such as passage ranking and answer extraction, trained as a supervised learning sub-task. This is crucial from a practical perspective, in order to reduce the memory usage when handling a large amount of inputs with advanced, large pre-trained models BIBREF5, BIBREF6, BIBREF7.

Manual evidence annotation is expensive, so there are only a few benchmarks with supporting evidence annotated. Even for these datasets, the structures of the annotations are still limited, as new model designs keep emerging and they may require different forms of evidence annotations. As a result, the supervision from these datasets can still be insufficient for training accurate models.

Taking question answering with multi-hop reasoning as an example, annotating only supporting passages is not sufficient to show the reasoning processes due to the lack of necessary structural information (Figure FIGREF1). One example is the order of annotated evidence, which is crucial in logic reasoning and the importance of which has also been demonstrated in text-based QA BIBREF8. The other example is how the annotated evidence pieces are connected, which requires at least the definition of arguments, such as a linking entity, concept, or event. Such information has proved useful by the recently popular entity-centric methods BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF0, BIBREF2 and intuitively will be a benefit to these methods if available.

We propose a cooperative game approach to recovering the reasoning chains with the aforementioned necessary structural information for multi-hop QA. Each recovered chain corresponds to a list of ordered passages and each pair of adjacent passages is connected with a linking entity. Specifically, we start with a model, the Ranker, which selects a sequence of passages arriving at the answers, with the restriction that each adjacent passage pair shares at least an entity. This is essentially an unsupervised task and the selection suffers from noise and ambiguity. Therefore we introduce another model, the Reasoner, which predicts the exact linking entity that points to the next passage. The two models play a cooperative game and are rewarded when they find a consistent chain. In this way, we restrict the selection to satisfy not only the format constraints (i.e., ordered passages with connected adjacencies) but also the semantic constraints (i.e., finding the next passage given that the partial selection can be effectively modeled by a Reasoner). Therefore, the selection can be less noisy.

We evaluate the proposed method on datasets with different properties, i.e., HotpotQA and MedHop BIBREF13, to cover cases with both 2-hop and 3-hop reasoning. We created labeled reasoning chains for both datasets. Experimental results demonstrate the significant advantage of our proposed approach.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Task Definition</section-title>
<section-number>1</section-number>
<paragraphs>
Reasoning Chains Examples of reasoning chains in HotpotQA and MedHop are shown in Figure FIGREF1. Formally, we aim at recovering the reasoning chain in the form of $(p_1 \rightarrow e_{1,2} \rightarrow p_2 \rightarrow e_{2,3} \rightarrow \cdots \rightarrow e_{n-1,n} \rightarrow p_n)$, where each $p_i$ is a passage and each $e_{i,i+1}$ is an entity that connects $p_i$ and $p_{i+1}$, i.e., appearing in both passages. The last passage $p_n$ in the chain contains the correct answer. We say $p_i$ connects $e_{i-1,i}$ and $e_{i,i+1}$ in the sense that it describes a relationship between the two entities.

Our Task Given a QA pair $(q,a)$ and all its candidate passages $\mathcal {P}$, we can extract all possible candidate chains that satisfy the conditions mentioned above, denoted as $\mathcal {C}$. The goal of reasoning chain recovery is to extract the correct chains from all the candidates, given $q,a$ and $\mathcal {P}$ as inputs.

Related Work Although there are recent interests on predicting reasoning chains for multi-hop QA BIBREF0, BIBREF14, BIBREF2, they all consider a fully supervised setting; i.e., annotated reasoning chains are available. Our work is the first to recover reasoning chains in a more general unsupervised setting, thus falling into the direction of denoising over distant supervised signals. From this perspective, the most relevant studies in the NLP field includes BIBREF15, BIBREF16 for evidence identification in open-domain QA and BIBREF17, BIBREF18, BIBREF19 for rationale recovery.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Method</section-title>
<section-number>2</section-number>
<paragraphs>
The task of recovering reasoning chains is essentially an unsupervised problem, as we have no access to annotated reasoning chains. Therefore, we resort to the noisy training signal from chains obtained by distant supervision. We first propose a conditional selection model that optimizes the passage selection by considering their orders (Section SECREF4). We then propose a cooperative Reasoner-Ranker game (Section SECREF12) in which the Reasoner recovers the linking entities that point to the next passage. This enhancement encourages the Ranker to select the chains such that their distribution is easier for a linking entity prediction model (Reasoner) to capture. Therefore, it enables our model to denoise the supervision signals while recovering chains with entity information. Figure FIGREF3 gives our overall framework, with a flow describing how the Reasoner passes additional rewards to the Ranker.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Method ::: Passage Ranking Model</section-title>
<section-number>3</section-number>
<paragraphs>
The key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\mathcal {P} = \lbrace p_1, p_2 ... p_K\rbrace $ from a pool of candidates, and outputs a chain of selected passages.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Method ::: Passage Ranking Model ::: Passage Scoring</section-title>
<section-number>4</section-number>
<paragraphs>
For each step of the chain, the Ranker estimates a distribution of the selection of each passage. To this end we first encode the question and passage with a 2-layer bi-directional GRU network, resulting in an encoded question $\mathbf {Q} = \lbrace \vec{\mathbf {q}_0}, \vec{\mathbf {q}_1}, ..., \vec{\mathbf {q}_N}\rbrace $ and $\mathbf {H}_i = \lbrace \vec{\mathbf {h}_{i,0}}, \vec{\mathbf {h}_{i,1}}, ..., \vec{\mathbf {h}_{i,M_i}}\rbrace $ for each passage $p_i \in P$ of length $M_i$. Then we use the MatchLSTM model BIBREF20 to get the matching score between $\mathbf {Q}$ and each $\mathbf {H}_i$ and derive the distribution of passage selection $P(p_i|q)$ (see Appendix SECREF6 for details). We denote $P(p_i|q)=\textrm {MatchLSTM}(\mathbf {H}_i, \mathbf {Q})$ for simplicity.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Method ::: Passage Ranking Model ::: Conditional Selection</section-title>
<section-number>5</section-number>
<paragraphs>
To model passage dependency along the chain of reasoning, we use a hard selection model that builds a chain incrementally. Provided with the $K$ passages, at each step $t$ the Ranker computes $P^t(p_i|\mathbf {Q}^{t-1}), i = 0, ..., K$, which is the probability of selecting passage $p_i$ conditioned on the query and previous states representation $\mathbf {Q}^{t-1}$. Then we sample one passage $p^t_{\tau }$ according to the predicted selection probability.



The first step starts with the original question $\mathbf {Q}^0$. A feed-forward network is used to project the concatenation of query encoding and selected passage encoding $\tilde{\mathbf {m}}^t_{p_{\tau }}$ back to the query space, and the new query $\mathbf {Q}^{t+1}$ is used to select the next passage.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Method ::: Passage Ranking Model ::: Reward via Distant Supervision</section-title>
<section-number>6</section-number>
<paragraphs>
We use policy gradient BIBREF21 to optimize our model. As we have no access to annotated reasoning chains during training, the reward comes from distant supervision. Specifically, we reward the Ranker if a selected passage appears as the corresponding part of a distant supervised chain in $\mathcal {C}$. The model receives immediate reward at each step of selection.

In this paper we only consider chains consist of $\le 3$ passages (2-hop and 3-hop chains). For the 2-hop cases, our model predicts a chain of two passages from the candidate set $\mathcal {C}$ in the form of $p_h\rightarrow e \rightarrow p_t$. Each candidate chain satisfies that $p_t$ contains the answer, while $p_h$ and $p_t$ contain a shared entity $e$. We call $p_h$ the head passage and $p_t$ the tail passage. Let $\mathcal {P}_{T}/\mathcal {P}_{H}$ denote the set of all tail/head passages from $\mathcal {C}$. Our model receives rewards $r_h, r_t$ according to its selections:

For the 3-hop cases, we need to select an additional intermediate passage $p_m$ between $p_h$ and $p_t$. If we reward any $p_m$ selection that appears in the middle of a chain in candidate chain set $\mathcal {C}$, the number of feasible options can be very large. Therefore, we make our model first select the head passage $p_h$ and the tail passage $p_t$ independently and then select $p_m$ conditioned on $(p_h,p_t)$. We further restrict that each path in $\mathcal {C}$ must have the head passage containing an entity from $q$. Then the selected $p_m$ is only rewarded if it appears in a chain in $\mathcal {C}$ that starts with $p_h$ and ends with $p_t$:
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Method ::: Cooperative Reasoner</section-title>
<section-number>7</section-number>
<paragraphs>
To alleviate the noise in the distant supervision signal $\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. Taking 2-hop as an example, we train the Ranker and Reasoner alternatively as a cooperative game:

Reasoner Step: Given the first passage $p_t$ selected by the trained Ranker, the Reasoner predicts the probability of each entity $e$ appearing in $p_t$. The Reasoner is trained with the cross-entropy loss:



Ranker Step: Given the Reasoner's top-1 predicted linking entity $e$, the reward for Ranker at the $2^{\textrm {nd}}$ step is defined as:





The extension to 3-hop cases is straightforward; the only difference is that the Reasoner reads both the selected $p_h$ and $p_t$ to output two entities. The Ranker receives one extra reward if the Reasoner picks the correct linking entity from $p_h$, so does $p_t$.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Experiments ::: Settings ::: Datasets</section-title>
<section-number>8</section-number>
<paragraphs>
We evaluate our path selection model on HotpotQA bridge type questions and on the MedHop dataset. In HotpotQA, the entities are pre-processed Wiki anchor link objects and in MedHop they are drug/protein database identifiers.

For HotpotQA, two supporting passages are provided along with each question. We ignore the support annotations during training and use them to create ground truth on development set: following BIBREF8, we determine the order of passages according to whether a passage contains the answer. We discard ambiguous instances.

For MedHop, there is no evidence annotated. Therefore we created a new evaluation dataset by manually annotating the correct paths for part of the development set: we first extract all candidate paths in form of passage triplets $(p_h, p_m, p_t)$, such that $p_h$ contains the query drug and $p_t$ contains the answer drug, and $p_h/p_m$ and $p_m/p_t$ are connected by shared proteins. We label a chain as positive if all the drug-protein or protein-protein interactions are described in the corresponding passages. Note that the positive paths are not unique for a question.

During training we select chains based on the full passage set $\mathcal {P}$; at inference time we extract the chains from the candidate set $\mathcal {C}$ (see Section SECREF2).


</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Experiments ::: Settings ::: Baselines and Evaluation Metric</section-title>
<section-number>9</section-number>
<paragraphs>
We compare our model with (1) random baseline, which randomly selects a candidate chain from the distant supervision chain set $\mathcal {C}$; and (2) distant supervised MatchLSTM, which uses the same base model as ours but scores and selects the passages independently. We use accuracy as our evaluation metric. As HotpotQA does not provide ground-truth linking entities, we only evaluate whether the supporting passages are fully recovered (yet our model still output the full chains). For MedHop we evaluate whether the whole predicted chain is correct. More details can be found in Appendix SECREF7. We use BIBREF24 as word embedding for HotpotQA, and BIBREF25 for MedHop.


</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Experiments ::: Results ::: HotpotQA</section-title>
<section-number>10</section-number>
<paragraphs>
We first evaluate on the 2-hop HotpotQA task. Our best performed model first selects the tail passage $p_t$ and then the head passage $p_h$, because the number of candidates of tail is smaller ($\sim $2 per question). Table TABREF21 shows the results. First, training a ranker with distant supervision performs significantly better than the random baseline, showing that the training process itself has a certain degree of denoising ability to distinguish the more informative signals from distant supervision labels. By introducing additional inductive bias of orders, the conditional selection model further improves with a large margin. Finally, our cooperative game gives the best performance, showing that a trained Reasoner has the ability of ignoring entity links that are irrelevant to the reasoning chain.

Table TABREF22 demonstrates the effect of selecting directions, together with the methods' recall on head passages and tail passages. The latter is evaluated on a subset of bridge-type questions in HotpotQA which has no ambiguous support annotations in passage orders; i.e., among the two human-labeled supporting passages, only one contains the answer and thus must be a tail. The results show that selecting tail first performs better. The cooperative game mainly improves the head selection.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Experiments ::: Results ::: MedHop</section-title>
<section-number>11</section-number>
<paragraphs>
Results in table TABREF21 show that recovering chains from MedHop is a much harder task: first, the large number of distant supervision chains in $\mathcal {C}$ introduce too much noise so the Distant Supervised Ranker improves only 3%; second, the dependent model leads to no improvement because $\mathcal {C}$ is strictly ordered given our data construction. Our cooperative game manages to remain effective and gives further improvement.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Conclusions</section-title>
<section-number>12</section-number>
<paragraphs>
In this paper we propose the problem of recovering reasoning chains in multi-hop QA from weak supervision signals. Our model adopts an cooperative game approach where a ranker and a reasoner cooperate to select the most confident chains. Experiments on the HotpotQA and MedHop benchmarks show the effectiveness of the proposed approach.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Details of MatchLSTMs for Passage Scoring and Reasoner ::: MatchLSTM for Passage Scoring</section-title>
<section-number>13</section-number>
<paragraphs>
Given the embeddings $\mathbf {Q} = \lbrace \vec{\mathbf {q}_0}, \vec{\mathbf {q}_1}, ..., \vec{\mathbf {q}_N}\rbrace $ of the question $q$, and $\mathbf {H}_i = \lbrace \vec{\mathbf {h}_{i,0}}, \vec{\mathbf {h}_{i,1}}, ..., \vec{\mathbf {h}_{i,M_i}}\rbrace $ of each passage $p_i \in P$, we use the MatchLSTM BIBREF20 to match $\mathbf {Q}$ and $\mathbf {H}_i$ as follows:

The final vector $\tilde{\mathbf {m}}_i$ represents the matching state between $q$ and $p_i$. All the $\tilde{\mathbf {m}}_i$s are then passed to a linear layer that outputs the ranking score of each passage. We apply softmax over the scores to get the probability of passage selection $P(p_i|q)$. We denote the above computation as $P(p_i|q)=\textrm {MatchLSTM}(\mathbf {H}_i, \mathbf {Q})$ for simplicity.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Details of MatchLSTMs for Passage Scoring and Reasoner ::: MatchLSTM for Reasoner</section-title>
<section-number>14</section-number>
<paragraphs>
Given the question embedding $\mathbf {Q}^r = \lbrace \vec{\mathbf {q}^r_0}, \vec{\mathbf {q}^r_1}, ..., \vec{\mathbf {q}^r_N}\rbrace $ and the input passage embedding $\mathbf {H}^r = \lbrace \vec{\mathbf {h}^r_{0}}, \vec{\mathbf {h}^r_{1}}, ..., \vec{\mathbf {h}^r_{M}}\rbrace $ of $p$, the Reasoner predicts the probability of each entity in the passage being the linking entity of the next passage in the chain. We use a reader model similar to BIBREF3 as our Reasoner network.

We first describe an attention sub-module. Given input sequence embedding $\mathbf {A} = \lbrace \vec{\mathbf {a}_0}, \vec{\mathbf {a}_1}, ..., \vec{\mathbf {a}_N}\rbrace $ and $\mathbf {B} = \lbrace \vec{\mathbf {b}_{0}}, \vec{\mathbf {b}_{1}}, ..., \vec{\mathbf {b}_{M}}\rbrace $, we define $\tilde{\mathcal {M}} = \text{Attention}(\mathbf {A}, \mathbf {B})$:

where FFN denotes a feed forward layer which projects the concatenated embedding back to the original space.

The Reasoner network consists of multiple attention layers, together with a bidirectional GRU encoder and skip connection.

For each token $e_k, k = 0, 1,..., M$ represented by $h^r_{p,k}$ at the corresponding location, we have:

where $g$ is the classification layer, softmax is applied across all entities to get the probability. We denote the computation above as $P^r(e_k| \mathbf {p}) = \textrm {MatchLSTM.Reader}(e_k, \mathbf {p})$ for simplicity.
</paragraphs>
</section>

---Paper Title: Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games---
<section>
<section-title>Definition of Chain Accuracy</section-title>
<section-number>15</section-number>
<paragraphs>
In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages).

In MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct.

The accuracy is defined as the ratio:


</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships</title>
<abstract>Many natural language questions require recognizing and reasoning with qualitative relationships (e.g., in science, economics, and medicine), but are challenging to answer with corpus-based methods. Qualitative modeling provides tools that support such reasoning, but the semantic parsing task of mapping questions into those models has formidable challenges. We present QuaRel, a dataset of diverse story questions involving qualitative relationships that characterize these challenges, and techniques that begin to address them. The dataset has 2771 questions relating 19 different types of quantities. For example,"Jenny observes that the robot vacuum cleaner moves slower on the living room carpet than on the bedroom carpet. Which carpet has more friction?"We contribute (1) a simple and flexible conceptual framework for representing these kinds of questions; (2) the QuaRel dataset, including logical forms, exemplifying the parsing challenges; and (3) two novel models for this task, built as extensions of type-constrained semantic parsing. The first of these models (called QuaSP+) significantly outperforms off-the-shelf tools on QuaRel. The second (QuaSP+Zero) demonstrates zero-shot capability, i.e., the ability to handle new qualitative relationships without requiring additional training data, something not possible with previous models. This work thus makes inroads into answering complex, qualitative questions that require reasoning, and scaling to new relationships at low cost. The dataset and models are available at http://data.allenai.org/quarel.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Many natural language tasks require recognizing and reasoning with qualitative relationships. For example, we may read about temperatures rising (climate science), a drug dose being increased (medicine), or the supply of goods being reduced (economics), and want to reason about the effects. Qualitative story problems, of the kind found in elementary exams (e.g., Figure FIGREF1 ), form a natural example of many of these linguistic and reasoning challenges, and is the target of this work.

Understanding and answering such questions is particularly challenging. Corpus-based methods perform poorly in this setting, as the questions ask about novel scenarios rather than facts that can be looked up. Similarly, word association methods struggle, as a single word change (e.g., “more” to “less”) can flip the answer. Rather, the task appears to require knowledge of the underlying qualitative relations (e.g., “more friction implies less speed”).

Qualitative modeling BIBREF0 , BIBREF1 , BIBREF2 provides a means for encoding and reasoning about such relationships. Relationships are expressed in a natural, qualitative way (e.g., if X increases, then so will Y), rather than requiring numeric equations, and inference allows complex questions to be answered. However, the semantic parsing task of mapping real world questions into these models is formidable and presents unique challenges. These challenges must be solved if natural questions involving qualitative relationships are to be reliably answered.

We make three contributions: (1) a simple and flexible conceptual framework for formally representing these kinds of questions, in particular ones that express qualitative comparisons between two scenarios; (2) a challenging new dataset (QuaRel), including logical forms, exemplifying the parsing challenges; and (3) two novel models that extend type-constrained semantic parsing to address these challenges.

Our first model, QuaSP+, addresses the problem of tracking different “worlds” in questions, resulting in significantly higher scores than with off-the-shelf tools (Section SECREF36 ). The second model, QuaSP+Zero, demonstrates zero-shot capability, i.e., the ability to handle new qualitative relationships on unseen properties, without requiring additional training data, something not possible with previous models (Section SECREF44 ). Together these contributions make inroads into answering complex, qualitative questions by linking language and reasoning, and offer a new dataset and models to spur further progress by the community.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Related Work</section-title>
<section-number>1</section-number>
<paragraphs>
There has been rapid progress in question-answering (QA), spanning a wide variety of tasks and phenomena, including factoid QA BIBREF3 , entailment BIBREF4 , sentiment BIBREF5 , and ellipsis and coreference BIBREF6 . Our contribution here is the first dataset specifically targeted at qualitative relationships, an important category of language that has been less explored. While questions requiring reasoning about qualitative relations sometimes appear in other datasets, e.g., BIBREF7 , our dataset specifically focuses on them so their challenges can be studied.

For answering such questions, we treat the problem as mapping language to a structured formalism (semantic parsing) where simple qualitative reasoning can occur. Semantic parsing has a long history BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , using datasets about geography BIBREF8 , travel booking BIBREF12 , factoid QA over knowledge bases BIBREF10 , Wikipedia tables BIBREF13 , and many more. Our contributions to this line of research are: a dataset that features phenomena under-represented in prior datasets, namely (1) highly diverse language describing open-domain qualitative problems, and (2) the need to reason over entities that have no explicit formal representation; and methods for adapting existing semantic parsers to address these phenomena.

For the target formalism itself, we draw on the extensive body of work on qualitative reasoning BIBREF0 , BIBREF1 , BIBREF2 to create a logical form language that can express the required qualitative knowledge, yet is sufficiently constrained that parsing into it is feasible, described in more detail in Section SECREF3 .

There has been some work connecting language with qualitative reasoning, although mainly focused on extracting qualitative models themselves from text rather than question interpretation, e.g., BIBREF14 , BIBREF15 . Recent work by BIBREF16 crouse2018learning also includes interpreting questions that require identifying qualitative processes in text, in constrast to our setting of interpreting NL story questions that involve qualitative comparisons.

Answering story problems has received attention in the domain of arithmetic, where simple algebra story questions (e.g., “Sue had 5 cookies, then gave 2 to Joe...”) are mapped to a system of equations, e.g., BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 . This task is loosely analogous to ours (we instead map to qualitative relations) except that in arithmetic the entities to relate are often identifiable (namely, the numbers). Our qualitative story questions lack this structure, adding an extra challenge.

The QuaRel dataset shares some structure with the Winograd Schema Challenge BIBREF21 , being 2-way multiple choice questions invoking both commonsense and coreference. However, they test different aspects of commonsense: Winograd uses coreference resolution to test commonsense understanding of scenarios, while QuaRel tests reasoning about qualitative relationships requiring tracking of coreferent “worlds.”

Finally, crowdsourcing datasets has become a driving force in AI, producing significant progress, e.g., BIBREF3 , BIBREF22 , BIBREF23 . However, for semantic parsing tasks, one obstacle has been the difficulty in crowdsourcing target logical forms for questions. Here, we show how those logical forms can be obtained indirectly from workers without training the workers in the formalism, loosely similar to BIBREF24 .
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Knowledge Representation </section-title>
<section-number>2</section-number>
<paragraphs>
We first describe our framework for representing questions and the knowledge to answer them. Our dataset, described later, includes logical forms expressed in this language.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Qualitative Background Knowledge</section-title>
<section-number>3</section-number>
<paragraphs>
We use a simple representation of qualitative relationships, leveraging prior work in qualitative reasoning BIBREF0 . Let INLINEFORM0 be the set of properties relevant to the question set's domain (e.g., smoothness, friction, speed). Let INLINEFORM1 be a set of qualitative values for property INLINEFORM2 (e.g., fast, slow). For the background knowledge about the domain itself (a qualitative model), following BIBREF0 Forbus1984QualitativePT, we use the following predicates: [vskip=1mm,leftmargin=5mm] q+(property1, property2)

q-(property1, property2) q+ denotes that property1 and property2 are qualitatively proportional, e.g., if property1 goes up, property2 will too, while q- denotes inverse proportionality, e.g., [vskip=1mm,leftmargin=5mm] # If friction goes up, speed goes down.

q-(friction, speed). We also introduce the predicate: [vskip=1mm,leftmargin=5mm] higher-than( INLINEFORM0 , INLINEFORM1 , property INLINEFORM2 ) where INLINEFORM3 , allowing an ordering of property values to be specified, e.g., higher-than(fast, slow, speed). For our purposes here, we simplify to use just two property values, low and high, for all properties. (The parser learns mappings from words to these values, described later).

Given these primitives, compact theories can be authored for a particular domain by choosing relevant properties INLINEFORM0 , and specifying qualitative relationships (q+,q-) and ordinal values (higher-than) for them. For example, a simple theory about friction is sketched graphically in Figure FIGREF3 . Our observation is that these theories are relatively small, simple, and easy to author. Rather, the primary challenge is in mapping the complex and varied language of questions into a form that interfaces with this representation.

This language can be extended to include additional primitives from qualitative modeling, e.g., i+(x,y) (“the rate of change of x is qualitatively proportional to y”). That is, the techniques we present are not specific to our particular qualitative modeling subset. The only requirement is that, given a set of absolute values or qualitative relationships from a question, the theory can compute an answer.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Representing Questions</section-title>
<section-number>4</section-number>
<paragraphs>
A key feature of our representation is the conceptualization of questions as describing events happening in two worlds, world1 and world2, that are being compared. That comparison may be between two different entities, or the same entity at different time points. E.g., in Figure FIGREF1 the two worlds being compared are the car on wood, and the car on carpet. The tags world1 and world2 denote these different situations, and semantic parsing (Section SECREF5 ) requires learning to correctly associate these tags with parts of the question describing those situations. This abstracts away irrelevant details of the worlds, while still keeping track of which world is which.

We define the following two predicates to express qualitative information in questions: [vskip=1mm,leftmargin=5mm] qrel(property, direction, world)

qval(property, value, world) where property ( INLINEFORM0 ) INLINEFORM1 P, value INLINEFORM2 INLINEFORM3 , direction INLINEFORM4 {higher, lower}, and world INLINEFORM5 {world1, world2}. qrel() denotes the relative assertion that property is higher/lower in world compared with the other world, which is left implicit, e.g., from Figure FIGREF1 : [vskip=1mm,leftmargin=5mm] # The car rolls further on wood.

qrel(distance, higher, world1) where world1 is a tag for the “car on wood” situation (hence world2 becomes a tag for the opposite “car on carpet” situation). qval() denotes that property has an absolute value in world, e.g., [vskip=1mm,leftmargin=5mm] # The car's speed is slow on carpet.

qval(speed, low, world2)
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Logical Forms for Questions</section-title>
<section-number>5</section-number>
<paragraphs>
Despite the wide variation in language, the space of logical forms (LFs) for the questions that we consider is relatively compact. In each question, the question body establishes a scenario and each answer option then probes an implication. We thus express a question's LF as a tuple: [vskip=1mm,leftmargin=5mm] (setup, answer-A, answer-B) where setup is the predicate(s) describing the scenario, and answer-* are the predicate(s) being queried for. If answer-A follows from setup, as inferred by the reasoner, then the answer is (A); similarly for (B). For readability we will write this as [vskip=1mm,leftmargin=5mm] setup INLINEFORM0 answer-A ; answer-B We consider two styles of LF, covering a large range of questions. The first is: [vskip=1mm,leftmargin=5mm] (1) qrel( INLINEFORM1 ) INLINEFORM2 

 qrel( INLINEFORM0 ) ; qrel( INLINEFORM1 ) which deals with relative values of properties between worlds, and applies when the question setup includes a comparative. An example of this is in Figure FIGREF1 . The second is: [vskip=1mm,leftmargin=5mm] (2) qval( INLINEFORM2 ), qval( INLINEFORM3 ) INLINEFORM4 

 qrel( INLINEFORM0 ) ; qrel( INLINEFORM1 ) which deals with absolute values of properties, and applies when the setup uses absolute terms instead of comparatives. An example is the first question in Figure FIGREF4 , shown simplified below, whose LF looks as follows (colors showing approximate correspondences): [vskip=1mm,leftmargin=5mm] # Does a bar stool orangeslide faster along the redbar surface with tealdecorative raised bumps or the magentasmooth wooden bluefloor? (A) redbar (B) bluefloor



qval(tealsmoothness, low, redworld1),

qval(magentasmoothness, high, blueworld2) INLINEFORM0 

 qrel(orangespeed, higher, redworld1) ;

 qrel(orangespeed, higher, blueworld2)
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Inference</section-title>
<section-number>6</section-number>
<paragraphs>
A small set of rules for qualitative reasoning connects these predicates together. For example, (in logic) if the value of P is higher in world1 than the value of P in world2 and q+(P,Q) then the value of Q will be higher in world1 than the value of Q in world2. Given a question's logical form, a qualitative model, and these rules, a Prolog-style inference engine determines which answer option follows from the premise.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>The QuaRel Dataset </section-title>
<section-number>7</section-number>
<paragraphs>
QuaRel is a crowdsourced dataset of 2771 multiple-choice story questions, including their logical forms. The size of the dataset is similar to several other datasets with annotated logical forms used for semantic parsing BIBREF8 , BIBREF25 , BIBREF24 . As the space of LFs is constrained, the dataset is sufficient for a rich exploration of this space.

We crowdsourced multiple-choice questions in two parts, encouraging workers to be imaginative and varied in their use of language. First, workers were given a seed qualitative relation q+/-( INLINEFORM0 ) in the domain, expressed in English (e.g., “If a surface has more friction, then an object will travel slower”), and asked to enter two objects, people, or situations to compare. They then created a question, guided by a large number of examples, and were encouraged to be imaginative and use their own words. The results are a remarkable variety of situations and phrasings (Figure FIGREF4 ).

Second, the LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions, without exposing workers to the underlying formalism. This is possible because of the constrained space of LFs. Referring to LF templates (1) and (2) earlier (Section SECREF13 ), these questions are as follows:

From this information, we can deduce the target LF ( INLINEFORM0 is the complement of INLINEFORM1 , INLINEFORM2 , we arbitrarily set INLINEFORM3 =world1, hence all other variables can be inferred). Three independent workers answer these follow-up questions to ensure reliable results.

We also had a human answer the questions in the dev partition (in principle, they should all be answerable). The human scored 96.4%, the few failures caused by occasional annotation errors or ambiguities in the question set itself, suggesting high fidelity of the content.

About half of the dataset are questions about friction, relating five different properties (friction, heat, distance, speed, smoothness). These questions form a meaningful, connected subset of the dataset which we denote QuaRel INLINEFORM0 . The remaining questions involve a wide variety of 14 additional properties and their relations, such as “exercise intensity vs. sweat” or “distance vs. brightness”.

Figure FIGREF4 shows typical examples of questions in QuaRel, and Table TABREF26 provides summary statistics. In particular, the vocabulary is highly varied (5226 unique words), given the dataset size. Figure FIGREF27 shows some examples of the varied phrases used to describe smoothness.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Baseline Systems </section-title>
<section-number>8</section-number>
<paragraphs>
We use four systems to evaluate the difficulty of this dataset. (We subsequently present two new models, extending the baseline neural semantic parser, in Sections SECREF36 and SECREF44 ). The first two are an information retrieval system and a word-association method, following the designs of BIBREF26 Clark2016CombiningRS. These are naive baselines that do not parse the question, but nevertheless may find some signal in a large corpus of text that helps guess the correct answer. The third is a CCG-style rule-based semantic parser written specifically for friction questions (the QuaRel INLINEFORM0 subset), but prior to data being collected. The last is a state-of-the-art neural semantic parser. We briefly describe each in turn.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Baseline Experiments</section-title>
<section-number>9</section-number>
<paragraphs>
We ran the above systems on the QuaRel dataset. QuaSP was trained on the training set, using the model with highest parse accuracy on the dev set (similarly BiLSTM used highest answer accuracy on the dev set) . The results are shown in Table TABREF34 . The 95% confidence interval is +/- 4% on the full test set. The human score is the sanity check on the dev set (Section SECREF4 ).

As Table TABREF34 shows, the QuaSP model performs better than other baseline approaches which are only slightly above random. QuaSP scores 56.1% (61.7% on the friction subset), indicating the challenges of this dataset.

For the rule-based system, we observe that it is unable to parse the majority (66%) of questions (hence scoring 0.5 for those questions, reflecting a random guess), due to the varied and unexpected vocabulary present in the dataset. For example, Figure FIGREF27 shows some of the ways that the notion of “smoother/rougher” is expressed in questions, many of which are not covered by the hand-written CCG grammar. This reflects the typical brittleness of hand-built systems.

For QuaSP, we also analyzed the parse accuracies, shown in Table TABREF35 , the score reflecting the percentage of times it produced exactly the right logical form. The random baseline for parse accuracy is near zero given the large space of logical forms, while the model parse accuracies are relatively high, much better than a random baseline.

Further analysis of the predicted LFs indicates that the neural model does well at predicting the properties ( INLINEFORM0 25% of errors on dev set), but struggles to predict the worlds in the LFs reliably ( INLINEFORM1 70% of errors on dev set). This helps explain why non-trivial parse accuracy does not necessarily translate into correspondingly higher answer accuracy: If only the world assignment is wrong, the answer will flip and give a score of zero, rather than the average 0.5.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>New Models </section-title>
<section-number>10</section-number>
<paragraphs>
We now present two new models, both extensions of the neural baseline QuaSP. The first, QuaSP+, addresses the leading cause of failure just described, namely the problem of identifying the two worlds being compared, and significantly outperforms all the baseline systems. The second, QuaSP+Zero, addresses the scaling problem, namely the costly requirement of needing many training examples each time a new qualitative property is introduced. It does this by instead using only a small amount of lexical information about the new property, thus achieving “zero shot” performance, i.e., handling properties unseen in the training examples BIBREF34 , a capability not present in the baseline systems. We present the models and results for each.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>QuaSP+: A Model Incorporating World Tracking</section-title>
<section-number>11</section-number>
<paragraphs>
We define the world tracking problem as identifying and tracking references to different “worlds” being compared in text, i.e., correctly mapping phrases to world identifiers, a critical aspect of the semantic parsing task. There are three reasons why this is challenging. First, unlike properties, the worlds being compared in questions are distinct in almost every question, and thus there is no obvious, learnable mapping from phrases to worlds. For example, while a property (like speed) has learnable ways to refer to it (“faster”, “moves rapidly”, “speeds”, “barely moves”), worlds are different in each question (e.g., “on a road”, “countertop”, “while cutting grass”) and thus learning to identify them is hard. Second, different phrases may be used to refer to the same world in the same question (see Figure FIGREF43 ), further complicating the task. Finally, even if the model could learn to identify worlds in other ways, e.g., by syntactic position in the question, there is the problem of selecting world1 or world2 consistently throughout the parse, so that the equivalent phrasings are assigned the same world.

This problem of mapping phrases to world identifiers is similar to the task of entity linking BIBREF35 . In prior semantic parsing work, entity linking is relatively straightforward: simple string-matching heuristics are often sufficient BIBREF36 , BIBREF37 , or an external entity linking system can be used BIBREF38 , BIBREF39 . In QuaRel, however, because the phrases denoting world1 and world2 are different in almost every question, and the word “world” is never used, such methods cannot be applied.

To address this, we have developed QuaSP+, a new model that extends QuaSP by adding an extra initial step to identify and delexicalize world references in the question. In this delexicalization process, potentially new linguistic descriptions of worlds are replaced by canonical tokens, creating the opportunity for the model to generalize across questions. For example, the world mentions in the question: [vskip=1mm,leftmargin=5mm] “A ball rolls further on wood than carpet because the (A) carpet is smoother (B) wood is smoother” are delexicalized to: [vskip=1mm,leftmargin=5mm] “A ball rolls further on World1 than World2 because the (A) World2 is smoother (B) World1 is smoother” This approach is analogous to BIBREF40 Herzig2018DecouplingSA, who delexicalized words to POS tags to avoid memorization. Similar delexicalized features have also been employed in Open Information Extraction BIBREF41 , so the Open IE system could learn a general model of how relations are expressed. In our case, however, delexicalizing to World1 and World2 is itself a significant challenge, because identifying phrases referring to worlds is substantially more complex than (say) identifying parts of speech.

To perform this delexicalization step, we use the world annotations included as part of the training dataset (Section SECREF4 ) to train a separate tagger to identify “world mentions” (text spans) in the question using BIO tags (BiLSTM encoder followed by a CRF). The spans are then sorted into World1 and World2 using the following algorithm:

If one span is a substring of another, they are are grouped together. Remaining spans are singleton groups.

The two groups containing the longest spans are labeled as the two worlds being compared.

Any additional spans are assigned to one of these two groups based on closest edit distance (or ignored if zero overlap).

The group appearing first in the question is labeled World1, the other World2.

The result is a question in which world mentions are canonicalized. The semantic parser QuaSP is then trained using these questions. We call the combined system (delexicalization plus semantic parser) QuaSP+.

The results for QuaSP+ are included in Table TABREF34 . Most importantly, QuaSP+ significantly outperforms the baselines by over 12% absolute. Similarly, the parse accuracies are significantly improved from 32.2% to 43.8% (Table TABREF35 ). This suggests that this delexicalization technique is an effective way of making progress on this dataset, and more generally on problems where multiple situations are being compared, a common characteristic of qualitative problems.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>QuaSP+Zero: A Model for the Zero-Shot Task</section-title>
<section-number>12</section-number>
<paragraphs>
While our delexicalization procedure demonstrates a way of addressing the world tracking problem, the approach still relies on annotated data; if we were to add new qualitative relations, new training data would be needed, which is a significant scalability obstacle. To address this, we define the zero-shot problem as being able to answer questions involving a new predicate p given training data only about other predicates P different from p. For example, if we add a new property (e.g., heat) to the qualitative model (e.g., adding q+(friction, heat); “more friction implies more heat”), we want to answer questions involving heat without creating new annotated training questions, and instead only use minimal extra information about the new property. A parser that achieved good zero-shot performance, i.e., worked well for new properties unseen at training time, would be a substantial advance, allowing a new qualitative model to link to questions with minimal effort.

QuaRel provides an environment in which methods for this zero-shot theory extension can be devised and evaluated. To do this, we consider the following experimental setting: All questions mentioning a particular property are removed, the parser is trained on the remainder, and then tested on those withheld questions, i.e., questions mentioning a property unseen in the training data.

We present and evaluate a model that we have developed for this, called QuaSP+Zero, that modifies the QuaSP+ parser as follows: During decoding, at points where the parser is selecting which property to include in the LF (e.g., Figure FIGREF31 ), it does not just consider the question tokens, but also the relationship between those tokens and the properties INLINEFORM0 used in the qualitative model. For example, a question token such as “longer” can act as a cue for (the property) length, even if unseen in the training data, because “longer” and a lexical form of length (e.g.,“length”) are similar. This approach follows the entity-linking approach used by BIBREF11 Krishnamurthy2017NeuralSP, where the similarity between question tokens and (words associated with) entities - called the entity linking score - help decide which entities to include in the LF during parsing. Here, we modify their entity linking score INLINEFORM1 , linking question tokens INLINEFORM2 and property “entities” INLINEFORM3 , to be: INLINEFORM4 

where INLINEFORM0 is a diagonal matrix connecting the embedding of the question token INLINEFORM1 and words INLINEFORM2 associated with the property INLINEFORM3 . For INLINEFORM4 , we provide a small list of words for each property (such as “speed”, “velocity”, and “fast” for the speed property), a small-cost requirement.

The results with QuaSP+Zero are in Table TABREF45 , shown in detail on the QuaRel INLINEFORM0 subset and (due to space constraints) summarized for the full QuaRel. We can measure overall performance of QuaSP+Zero by averaging each of the zero-shot test sets (weighted by the number of questions in each set), resulting in an overall parse accuracy of 38.9% and answer accuracy 61.0% on QuaRel INLINEFORM1 , and 25.7% (parse) and 59.5% (answer) on QuaRel, both significantly better than random. These initial results are encouraging, suggesting that it may be possible to parse into modified qualitative models that include new relations, with minimal annotation effort, significantly opening up qualitative reasoning methods for QA.
</paragraphs>
</section>

---Paper Title: QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships---
<section>
<section-title>Summary and Conclusion</section-title>
<section-number>13</section-number>
<paragraphs>
Our goal is to answer questions that involve qualitative relationships, an important genre of task that involves both language and knowledge, but also one that presents significant challenges for semantic parsing. To this end we have developed a simple and flexible formalism for representing these questions; constructed QuaRel, the first dataset of qualitative story questions that exemplifies these challenges; and presented two new models that adapt existing parsing techniques to this task. The first model, QuaSP+, illustrates how delexicalization can help with world tracking (identifying different “worlds” in questions), resulting in state-of-the-art performance on QuaRel. The second model, QuaSP+Zero, illustrates how zero-shot learning can be achieved (i.e., adding new qualitative relationships without requiring new training examples) by using an entity-linking approach applied to properties - a capability not present in previous models.

There are several directions in which this work can be expanded. First, quantitative property values (e.g., “10 mph”) are currently not handled well, as their mapping to “low” or “high” is context-dependent. Second, some questions do not fit our two question templates (Section SECREF13 ), e.g., where two property values are a single answer option (e.g., “....(A) one floor is smooth and the other floor is rough”). Finally, some questions include an additional level of indirection, requiring an inference step to map to qualitative relations. For example, “Which surface would be best for a race? (A) gravel (B) blacktop” requires the additional commonsense inference that “best for a race” implies “higher speed”.

Given the ubiquity of qualitative comparisons in natural text, recognizing and reasoning with qualitative relationships is likely to remain an important task for AI. This work makes inroads into this task, and contributes a dataset and models to encourage progress by others. The dataset and models are publicly available at http://data.allenai.org/quarel.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Multimodal Differential Network for Visual Question Generation</title>
<abstract>Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
To understand the progress towards multimedia vision and language understanding, a visual Turing test was proposed by BIBREF0 that was aimed at visual question answering BIBREF1 . Visual Dialog BIBREF2 is a natural extension for VQA. Current dialog systems as evaluated in BIBREF3 show that when trained between bots, AI-AI dialog systems show improvement, but that does not translate to actual improvement for Human-AI dialog. This is because, the questions generated by bots are not natural (human-like) and therefore does not translate to improved human dialog. Therefore it is imperative that improvement in the quality of questions will enable dialog agents to perform well in human interactions. Further, BIBREF4 show that unanswered questions can be used for improving VQA, Image captioning and Object Classification.

An interesting line of work in this respect is the work of BIBREF5 . Here the authors have proposed the challenging task of generating natural questions for an image. One aspect that is central to a question is the context that is relevant to generate it. However, this context changes for every image. As can be seen in Figure FIGREF1 , an image with a person on a skateboard would result in questions related to the event. Whereas for a little girl, the questions could be related to age rather than the action. How can one have widely varying context provided for generating questions? To solve this problem, we use the context obtained by considering exemplars, specifically we use the difference between relevant and irrelevant exemplars. We consider different contexts in the form of Location, Caption, and Part of Speech tags.

The human annotated questions are (b) for the first image and (a) for the second image.

Our method implicitly uses a differential context obtained through supporting and contrasting exemplars to obtain a differentiable embedding. This embedding is used by a question decoder to decode the appropriate question. As discussed further, we observe this implicit differential context to perform better than an explicit keyword based context. The difference between the two approaches is illustrated in Figure FIGREF2 . This also allows for better optimization as we can backpropagate through the whole network. We provide detailed empirical evidence to support our hypothesis. As seen in Figure FIGREF1 our method generates natural questions and improves over the state-of-the-art techniques for this problem.

To summarize, we propose a multimodal differential network to solve the task of visual question generation. Our contributions are: (1) A method to incorporate exemplars to learn differential embeddings that captures the subtle differences between supporting and contrasting examples and aid in generating natural questions. (2) We provide Multimodal differential embeddings, as image or text alone does not capture the whole context and we show that these embeddings outperform the ablations which incorporate cues such as only image, or tags or place information. (3) We provide a thorough comparison of the proposed network against state-of-the-art benchmarks along with a user study and statistical significance test.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Related Work</section-title>
<section-number>1</section-number>
<paragraphs>
Generating a natural and engaging question is an interesting and challenging task for a smart robot (like chat-bot). It is a step towards having a natural visual dialog instead of the widely prevalent visual question answering bots. Further, having the ability to ask natural questions based on different contexts is also useful for artificial agents that can interact with visually impaired people. While the task of generating question automatically is well studied in NLP community, it has been relatively less studied for image-related natural questions. This is still a difficult task BIBREF5 that has gained recent interest in the community.

Recently there have been many deep learning based approaches as well for solving the text-based question generation task such as BIBREF6 . Further, BIBREF7 have proposed a method to generate a factoid based question based on triplet set {subject, relation and object} to capture the structural representation of text and the corresponding generated question.

These methods, however, were limited to text-based question generation. There has been extensive work done in the Vision and Language domain for solving image captioning, paragraph generation, Visual Question Answering (VQA) and Visual Dialog. BIBREF8 , BIBREF9 , BIBREF10 proposed conventional machine learning methods for image description. BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 have generated descriptive sentences from images with the help of Deep Networks. There have been many works for solving Visual Dialog BIBREF19 , BIBREF20 , BIBREF2 , BIBREF21 , BIBREF22 . A variety of methods have been proposed by BIBREF23 , BIBREF24 , BIBREF1 , BIBREF25 , BIBREF26 , BIBREF27 for solving VQA task including attention-based methods BIBREF28 , BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 . However, Visual Question Generation (VQG) is a separate task which is of interest in its own right and has not been so well explored BIBREF5 . This is a vision based novel task aimed at generating natural and engaging question for an image. BIBREF35 proposed a method for continuously generating questions from an image and subsequently answering those questions. The works closely related to ours are that of BIBREF5 and BIBREF36 . In the former work, the authors used an encoder-decoder based framework whereas in the latter work, the authors extend it by using a variational autoencoder based sequential routine to obtain natural questions by performing sampling of the latent variable.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Approach</section-title>
<section-number>2</section-number>
<paragraphs>
In this section, we clarify the basis for our approach of using exemplars for question generation. To use exemplars for our method, we need to ensure that our exemplars can provide context and that our method generates valid exemplars.

We first analyze whether the exemplars are valid or not. We illustrate this in figure FIGREF3 . We used a pre-trained RESNET-101 BIBREF37 object classification network on the target, supporting and contrasting images. We observed that the supporting image and target image have quite similar probability scores. The contrasting exemplar image, on the other hand, has completely different probability scores.

Exemplars aim to provide appropriate context. To better understand the context, we experimented by analysing the questions generated through an exemplar. We observed that indeed a supporting exemplar could identify relevant tags (cows in Figure FIGREF3 ) for generating questions.

We improve use of exemplars by using a triplet network. This network ensures that the joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption and vice-versa. We empirically evaluated whether an explicit approach that uses the differential set of tags as a one-hot encoding improves the question generation, or the implicit embedding obtained based on the triplet network. We observed that the implicit multimodal differential network empirically provided better context for generating questions. Our understanding of this phenomenon is that both target and supporting exemplars generate similar questions whereas contrasting exemplars generate very different questions from the target question. The triplet network that enhances the joint embedding thus aids to improve the generation of target question. These are observed to be better than the explicitly obtained context tags as can be seen in Figure FIGREF2 . We now explain our method in detail.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Method</section-title>
<section-number>3</section-number>
<paragraphs>
The task in visual question generation (VQG) is to generate a natural language question INLINEFORM0 , for an image INLINEFORM1 . We consider a set of pre-generated context INLINEFORM2 from image INLINEFORM3 . We maximize the conditional probability of generated question given image and context as follows: DISPLAYFORM0 

where INLINEFORM0 is a vector for all possible parameters of our model. INLINEFORM1 is the ground truth question. The log probability for the question is calculated by using joint probability over INLINEFORM2 with the help of chain rule. For a particular question, the above term is obtained as: INLINEFORM3 

where INLINEFORM0 is length of the sequence, and INLINEFORM1 is the INLINEFORM2 word of the question. We have removed INLINEFORM3 for simplicity.

Our method is based on a sequence to sequence network BIBREF38 , BIBREF12 , BIBREF39 . The sequence to sequence network has a text sequence as input and output. In our method, we take an image as input and generate a natural question as output. The architecture for our model is shown in Figure FIGREF4 . Our model contains three main modules, (a) Representation Module that extracts multimodal features (b) Mixture Module that fuses the multimodal representation and (c) Decoder that generates question using an LSTM-based language model.

During inference, we sample a question word INLINEFORM0 from the softmax distribution and continue sampling until the end token or maximum length for the question is reached. We experimented with both sampling and argmax and found out that argmax works better. This result is provided in the supplementary material.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Multimodal Differential Network</section-title>
<section-number>4</section-number>
<paragraphs>
The proposed Multimodal Differential Network (MDN) consists of a representation module and a joint mixture module.

We used an efficient KNN-based approach (k-d tree) with Euclidean metric to obtain the exemplars. This is obtained through a coarse quantization of nearest neighbors of the training examples into 50 clusters, and selecting the nearest as supporting and farthest as the contrasting exemplars. We experimented with ITML based metric learning BIBREF40 for image features. Surprisingly, the KNN-based approach outperforms the latter one. We also tried random exemplars and different number of exemplars and found that INLINEFORM0 works best. We provide these results in the supplementary material.

We use a triplet network BIBREF41 , BIBREF42 in our representation module. We refereed a similar kind of work done in BIBREF34 for building our triplet network. The triplet network consists of three sub-parts: target, supporting, and contrasting networks. All three networks share the same parameters. Given an image INLINEFORM0 we obtain an embedding INLINEFORM1 using a CNN parameterized by a function INLINEFORM2 where INLINEFORM3 are the weights for the CNN. The caption INLINEFORM4 results in a caption embedding INLINEFORM5 through an LSTM parameterized by a function INLINEFORM6 where INLINEFORM7 are the weights for the LSTM. This is shown in part 1 of Figure FIGREF4 . Similarly we obtain image embeddings INLINEFORM8 & INLINEFORM9 and caption embeddings INLINEFORM10 & INLINEFORM11 . DISPLAYFORM0 

The Mixture module brings the image and caption embeddings to a joint feature embedding space. The input to the module is the embeddings obtained from the representation module. We have evaluated four different approaches for fusion viz., joint, element-wise addition, hadamard and attention method. Each of these variants receives image features INLINEFORM0 & the caption embedding INLINEFORM1 , and outputs a fixed dimensional feature vector INLINEFORM2 . The Joint method concatenates INLINEFORM3 & INLINEFORM4 and maps them to a fixed length feature vector INLINEFORM5 as follows: DISPLAYFORM0 

where INLINEFORM0 is the 4096-dimensional convolutional feature from the FC7 layer of pretrained VGG-19 Net BIBREF43 . INLINEFORM1 are the weights and INLINEFORM2 is the bias for different layers. INLINEFORM3 is the concatenation operator.

Similarly, We obtain context vectors INLINEFORM0 & INLINEFORM1 for the supporting and contrasting exemplars. Details for other fusion methods are present in supplementary.The aim of the triplet network BIBREF44 is to obtain context vectors that bring the supporting exemplar embeddings closer to the target embedding and vice-versa. This is obtained as follows: DISPLAYFORM0 

where INLINEFORM0 is the euclidean distance between two embeddings INLINEFORM1 and INLINEFORM2 . M is the training dataset that contains all set of possible triplets. INLINEFORM3 is the triplet loss function. This is decomposed into two terms, one that brings the supporting sample closer and one that pushes the contrasting sample further. This is given by DISPLAYFORM0 

Here INLINEFORM0 represent the euclidean distance between the target and supporting sample, and target and opposing sample respectively. The parameter INLINEFORM1 controls the separation margin between these and is obtained through validation data.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Decoder: Question Generator</section-title>
<section-number>5</section-number>
<paragraphs>
The role of decoder is to predict the probability for a question, given INLINEFORM0 . RNN provides a nice way to perform conditioning on previous state value using a fixed length hidden vector. The conditional probability of a question token at particular time step INLINEFORM1 is modeled using an LSTM as used in machine translation BIBREF38 . At time step INLINEFORM2 , the conditional probability is denoted by INLINEFORM3 , where INLINEFORM4 is the hidden state of the LSTM cell at time step INLINEFORM5 , which is conditioned on all the previously generated words INLINEFORM6 . The word with maximum probability in the probability distribution of the LSTM cell at step INLINEFORM7 is fed as an input to the LSTM cell at step INLINEFORM8 as shown in part 3 of Figure FIGREF4 . At INLINEFORM9 , we are feeding the output of the mixture module to LSTM. INLINEFORM10 are the predicted question tokens for the input image INLINEFORM11 . Here, we are using INLINEFORM12 and INLINEFORM13 as the special token START and STOP respectively. The softmax probability for the predicted question token at different time steps is given by the following equations where LSTM refers to the standard LSTM cell equations: INLINEFORM14 

Where INLINEFORM0 is the probability distribution over all question tokens. INLINEFORM1 is cross entropy loss.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Cost function</section-title>
<section-number>6</section-number>
<paragraphs>
Our objective is to minimize the total loss, that is the sum of cross entropy loss and triplet loss over all training examples. The total loss is: DISPLAYFORM0 

where INLINEFORM0 is the total number of samples, INLINEFORM1 is a constant, which controls both the loss. INLINEFORM2 is the triplet loss function EQREF13 . INLINEFORM3 is the cross entropy loss between the predicted and ground truth questions and is given by: INLINEFORM4 

where, INLINEFORM0 is the total number of question tokens, INLINEFORM1 is the ground truth label. The code for MDN-VQG model is provided .
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Variations of Proposed Method</section-title>
<section-number>7</section-number>
<paragraphs>
While, we advocate the use of multimodal differential network for generating embeddings that can be used by the decoder for generating questions, we also evaluate several variants of this architecture. These are as follows:

Tag Net: In this variant, we consider extracting the part-of-speech (POS) tags for the words present in the caption and obtaining a Tag embedding by considering different methods of combining the one-hot vectors. Further details and experimental results are present in the supplementary. This Tag embedding is then combined with the image embedding and provided to the decoder network.

Place Net: In this variant we explore obtaining embeddings based on the visual scene understanding. This is obtained using a pre-trained PlaceCNN BIBREF45 that is trained to classify 365 different types of scene categories. We then combine the activation map for the input image and the VGG-19 based place embedding to obtain the joint embedding used by the decoder.

Differential Image Network: Instead of using multimodal differential network for generating embeddings, we also evaluate differential image network for the same. In this case, the embedding does not include the caption but is based only on the image feature. We also experimented with using multiple exemplars and random exemplars.

Further details, pseudocode and results regarding these are present in the supplementary material.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Dataset</section-title>
<section-number>8</section-number>
<paragraphs>
We conduct our experiments on Visual Question Generation (VQG) dataset BIBREF5 , which contains human annotated questions based on images of MS-COCO dataset. This dataset was developed for generating natural and engaging questions based on common sense reasoning. We use VQG-COCO dataset for our experiments which contains a total of 2500 training images, 1250 validation images, and 1250 testing images. Each image in the dataset contains five natural questions and five ground truth captions. It is worth noting that the work of BIBREF36 also used the questions from VQA dataset BIBREF1 for training purpose, whereas the work by BIBREF5 uses only the VQG-COCO dataset. VQA-1.0 dataset is also built on images from MS-COCO dataset. It contains a total of 82783 images for training, 40504 for validation and 81434 for testing. Each image is associated with 3 questions. We used pretrained caption generation model BIBREF13 to extract captions for VQA dataset as the human annotated captions are not there in the dataset. We also get good results on the VQA dataset (as shown in Table TABREF26 ) which shows that our method doesn't necessitate the presence of ground truth captions. We train our model separately for VQG-COCO and VQA dataset.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Inference</section-title>
<section-number>9</section-number>
<paragraphs>
We made use of the 1250 validation images to tune the hyperparameters and are providing the results on test set of VQG-COCO dataset. During inference, We use the Representation module to find the embeddings for the image and ground truth caption without using the supporting and contrasting exemplars. The mixture module provides the joint representation of the target image and ground truth caption. Finally, the decoder takes in the joint features and generates the question. We also experimented with the captions generated by an Image-Captioning network BIBREF13 for VQG-COCO dataset and the result for that and training details are present in the supplementary material.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Experiments</section-title>
<section-number>10</section-number>
<paragraphs>
We evaluate our proposed MDN method in the following ways: First, we evaluate it against other variants described in section SECREF19 and SECREF10 . Second, we further compare our network with state-of-the-art methods for VQA 1.0 and VQG-COCO dataset. We perform a user study to gauge human opinion on naturalness of the generated question and analyze the word statistics in Figure FIGREF22 . This is an important test as humans are the best deciders of naturalness. We further consider the statistical significance for the various ablations as well as the state-of-the-art models. The quantitative evaluation is conducted using standard metrics like BLEU BIBREF46 , METEOR BIBREF47 , ROUGE BIBREF48 , CIDEr BIBREF49 . Although these metrics have not been shown to correlate with `naturalness' of the question these still provide a reasonable quantitative measure for comparison. Here we only provide the BLEU1 scores, but the remaining BLEU-n metric scores are present in the supplementary. We observe that the proposed MDN provides improved embeddings to the decoder. We believe that these embeddings capture instance specific differential information that helps in guiding the question generation. Details regarding the metrics are given in the supplementary material.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Ablation Analysis</section-title>
<section-number>11</section-number>
<paragraphs>
We considered different variations of our method mentioned in section SECREF19 and the various ways to obtain the joint multimodal embedding as described in section SECREF10 . The results for the VQG-COCO test set are given in table TABREF24 . In this table, every block provides the results for one of the variations of obtaining the embeddings and different ways of combining them. We observe that the Joint Method (JM) of combining the embeddings works the best in all cases except the Tag Embeddings. Among the ablations, the proposed MDN method works way better than the other variants in terms of BLEU, METEOR and ROUGE metrics by achieving an improvement of 6%, 12% and 18% in the scores respectively over the best other variant.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Baseline and State-of-the-Art</section-title>
<section-number>12</section-number>
<paragraphs>
The comparison of our method with various baselines and state-of-the-art methods is provided in table TABREF26 for VQA 1.0 and table TABREF27 for VQG-COCO dataset. The comparable baselines for our method are the image based and caption based models in which we use either only the image or the caption embedding and generate the question. In both the tables, the first block consists of the current state-of-the-art methods on that dataset and the second contains the baselines. We observe that for the VQA dataset we achieve an improvement of 8% in BLEU and 7% in METEOR metric scores over the baselines, whereas for VQG-COCO dataset this is 15% for both the metrics. We improve over the previous state-of-the-art BIBREF35 for VQA dataset by around 6% in BLEU score and 10% in METEOR score. In the VQG-COCO dataset, we improve over BIBREF5 by 3.7% and BIBREF36 by 3.5% in terms of METEOR scores.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Statistical Significance Analysis</section-title>
<section-number>13</section-number>
<paragraphs>
We have analysed Statistical Significance BIBREF50 of our MDN model for VQG for different variations of the mixture module mentioned in section SECREF10 and also against the state-of-the-art methods. The Critical Difference (CD) for Nemenyi BIBREF51 test depends upon the given INLINEFORM0 (confidence level, which is 0.05 in our case) for average ranks and N (number of tested datasets). If the difference in the rank of the two methods lies within CD, then they are not significantly different and vice-versa. Figure FIGREF29 visualizes the post-hoc analysis using the CD diagram. From the figure, it is clear that MDN-Joint works best and is statistically significantly different from the state-of-the-art methods.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Perceptual Realism</section-title>
<section-number>14</section-number>
<paragraphs>
A human is the best judge of naturalness of any question, We evaluated our proposed MDN method using a `Naturalness' Turing test BIBREF52 on 175 people. People were shown an image with 2 questions just as in figure FIGREF1 and were asked to rate the naturalness of both the questions on a scale of 1 to 5 where 1 means `Least Natural' and 5 is the `Most Natural'. We provided 175 people with 100 such images from the VQG-COCO validation dataset which has 1250 images. Figure FIGREF30 indicates the number of people who were fooled (rated the generated question more or equal to the ground truth question). For the 100 images, on an average 59.7% people were fooled in this experiment and this shows that our model is able to generate natural questions.
</paragraphs>
</section>

---Paper Title: Multimodal Differential Network for Visual Question Generation---
<section>
<section-title>Conclusion</section-title>
<section-number>15</section-number>
<paragraphs>
In this paper we have proposed a novel method for generating natural questions for an image. The approach relies on obtaining multimodal differential embeddings from image and its caption. We also provide ablation analysis and a detailed comparison with state-of-the-art methods, perform a user study to evaluate the naturalness of our generated questions and also ensure that the results are statistically significant. In future, we would like to analyse means of obtaining composite embeddings. We also aim to consider the generalisation of this approach to other vision and language tasks. Supplementary Material Section SECREF8 will provide details about training configuration for MDN, Section SECREF9 will explain the various Proposed Methods and we also provide a discussion in section regarding some important questions related to our method. We report BLEU1, BLEU2, BLEU3, BLEU4, METEOR, ROUGE and CIDER metric scores for VQG-COCO dataset. We present different experiments with Tag Net in which we explore the performance of various tags (Noun, Verb, and Question tags) and different ways of combining them to get the context vectors. Multimodal Differential Network [1] MDN INLINEFORM0 Finding Exemplars: INLINEFORM1 INLINEFORM2 Compute Triplet Embedding: INLINEFORM3 INLINEFORM4 Compute Triplet Fusion Embedding : INLINEFORM5 INLINEFORM6 INLINEFORM7 Compute Triplet Loss: INLINEFORM8 Compute Decode Question Sentence: INLINEFORM9 INLINEFORM10 —————————————————– Triplet Fusion INLINEFORM11 , INLINEFORM12 INLINEFORM13 :Image feature,14x14x512 INLINEFORM14 : Caption feature,1x512 Match Dimension: INLINEFORM15 ,196x512 INLINEFORM16 196x512 If flag==Joint Fusion: INLINEFORM17 INLINEFORM18 , [ INLINEFORM19 (MDN-Mul), INLINEFORM20 (MDN-Add)] If flag==Attention Fusion : INLINEFORM21 Semb INLINEFORM22 Dataset and Training Details Dataset We conduct our experiments on two types of dataset: VQA dataset BIBREF1 , which contains human annotated questions based on images on MS-COCO dataset. Second one is VQG-COCO dataset based on natural question BIBREF55 . VQA dataset VQA dataset BIBREF1 is built on complex images from MS-COCO dataset. It contains a total of 204721 images, out of which 82783 are for training, 40504 for validation and 81434 for testing. Each image in the MS-COCO dataset is associated with 3 questions and each question has 10 possible answers. So there are 248349 QA pair for training, 121512 QA pairs for validating and 244302 QA pairs for testing. We used pre-trained caption generation model BIBREF53 to extract captions for VQA dataset. VQG dataset The VQG-COCO dataset BIBREF55 , is developed for generating natural and engaging questions that are based on common sense reasoning. This dataset contains a total of 2500 training images, 1250 validation images and 1250 testing images. Each image in the dataset contains 5 natural questions. Training Configuration We have used RMSPROP optimizer to update the model parameter and configured hyper-parameter values to be as follows: INLINEFORM23 to train the classification network . In order to train a triplet model, we have used RMSPROP to optimize the triplet model model parameter and configure hyper-parameter values to be: INLINEFORM24 . We also used learning rate decay to decrease the learning rate on every epoch by a factor given by: INLINEFORM25 where values of a=1500 and b=1250 are set empirically. Ablation Analysis of Model While, we advocate the use of multimodal differential network (MDN) for generating embeddings that can be used by the decoder for generating questions, we also evaluate several variants of this architecture namely (a) Differential Image Network, (b) Tag net and (c) Place net. These are described in detail as follows: Differential Image Network For obtaining the exemplar image based context embedding, we propose a triplet network consist of three network, one is target net, supporting net and opposing net. All these three networks designed with convolution neural network and shared the same parameters. The weights of this network are learnt through end-to-end learning using a triplet loss. The aim is to obtain latent weight vectors that bring the supporting exemplar close to the target image and enhances the difference between opposing examples. More formally, given an image INLINEFORM26 we obtain an embedding INLINEFORM27 using a CNN that we parameterize through a function INLINEFORM28 where INLINEFORM29 are the weights of the CNN. This is illustrated in figure FIGREF43 . Tag net The tag net consists of two parts Context Extractor & Tag Embedding Net. This is illustrated in figure FIGREF45 . Extract Context: The first step is to extract the caption of the image using NeuralTalk2 BIBREF53 model. We find the part-of-speech(POS) tag present in the caption. POS taggers have been developed for two well known corpuses, the Brown Corpus and the Penn Treebanks. For our work, we are using the Brown Corpus tags. The tags are clustered into three category namely Noun tag, Verb tag and Question tags (What, Where, ...). Noun tag consists of all the noun & pronouns present in the caption sentence and similarly, verb tag consists of verb & adverbs present in the caption sentence. The question tags consists of the 7-well know question words i.e., why, how, what, when, where, who and which. Each tag token is represented as a one-hot vector of the dimension of vocabulary size. For generalization, we have considered 5 tokens from each category of the Tags. Tag Embedding Net: The embedding network consists of word embedding followed by temporal convolutions neural network followed by max-pooling network. In the first step, sparse high dimensional one-hot vector is transformed to dense low dimension vector using word embedding. After this, we apply temporal convolution on the word embedding vector. The uni-gram, bi-gram and tri-gram feature are computed by applying convolution filter of size 1, 2 and 3 respectability. Finally, we applied max-pooling on this to get a vector representation of the tags as shown figure FIGREF45 . We concatenated all the tag words followed by fully connected layer to get feature dimension of 512. We also explored joint networks based on concatenation of all the tags, on element-wise addition and element-wise multiplication of the tag vectors. However, we observed that convolution over max pooling and joint concatenation gives better performance based on CIDer score. INLINEFORM30 Where, T_CNN is Temporally Convolution Neural Network applied on word embedding vector with kernel size three. Place net Visual object and scene recognition plays a crucial role in the image. Here, places in the image are labeled with scene semantic categories BIBREF45 , comprise of large and diverse type of environment in the world, such as (amusement park, tower, swimming pool, shoe shop, cafeteria, rain-forest, conference center, fish pond, etc.). So we have used different type of scene semantic categories present in the image as a place based context to generate natural question. A place365 is a convolution neural network is modeled to classify 365 types of scene categories, which is trained on the place2 dataset consist of 1.8 million of scene images. We have used a pre-trained VGG16-places365 network to obtain place based context embedding feature for various type scene categories present in the image. The context feature INLINEFORM31 is obtained by: INLINEFORM32 Where, INLINEFORM33 is Place365_CNN. We have extracted INLINEFORM34 features of dimension 14x14x512 for attention model and FC8 features of dimension 365 for joint, addition and hadamard model of places365. Finally, we use a linear transformation to obtain a 512 dimensional vector. We explored using the CONV5 having feature dimension 14x14 512, FC7 having 4096 and FC8 having feature dimension of 365 of places365. Ablation Analysis Sampling Exemplar: KNN vs ITML Our method is aimed at using efficient exemplar-based retrieval techniques. We have experimented with various exemplar methods, such as ITML BIBREF40 based metric learning for image features and KNN based approaches. We observed KNN based approach (K-D tree) with Euclidean metric is a efficient method for finding exemplars. Also we observed that ITML is computationally expensive and also depends on the training procedure. The table provides the experimental result for Differential Image Network variant with k (number of exemplars) = 2 and Hadamard method: Question Generation approaches: Sampling vs Argmax We obtained the decoding using standard practice followed in the literature BIBREF38 . This method selects the argmax sentence. Also, we evaluated our method by sampling from the probability distributions and provide the results for our proposed MDN-Joint method for VQG dataset as follows: How are exemplars improving Embedding In Multimodel differential network, we use exemplars and train them using a triplet loss. It is known that using a triplet network, we can learn a representation that accentuates how the image is closer to a supporting exemplar as against the opposing exemplar BIBREF42 , BIBREF41 . The Joint embedding is obtained between the image and language representations. Therefore the improved representation helps in obtaining an improved context vector. Further we show that this also results in improving VQG. Are exemplars required? We had similar concerns and validated this point by using random exemplars for the nearest neighbor for MDN. (k=R in table TABREF35 ) In this case the method is similar to the baseline. This suggests that with random exemplar, the model learns to ignore the cue. Are captions necessary for our method? This is not actually necessary. In our method, we have used an existing image captioning method BIBREF13 to generate captions for images that did not have them. For VQG dataset, captions were available and we have used that, but, for VQA dataset captions were not available and we have generated captions while training. We provide detailed evidence with respect to caption-question pairs to ensure that we are generating novel questions. While the caption generates scene description, our proposed method generates semantically meaningful and novel questions. Examples for Figure 1 of main paper: First Image:- Caption- A young man skateboarding around little cones. Our Question- Is this a skateboard competition? Second Image:- Caption- A small child is standing on a pair of skis. Our Question:- How old is that little girl? Intuition behind Triplet Network: The intuition behind use of triplet networks is clear through this paper BIBREF41 that first advocated its use. The main idea is that when we learn distance functions that are “close” for similar and “far” from dissimilar representations, it is not clear that close and far are with respect to what measure. By incorporating a triplet we learn distance functions that learn that “A is more similar to B as compared to C”. Learning such measures allows us to bring target image-caption joint embeddings that are closer to supporting exemplars as compared to contrasting exemplars. Analysis of Network Analysis of Tag Context Tag is language based context. These tags are extracted from caption, except question-tags which is fixed as the 7 'Wh words' (What, Why, Where, Who, When, Which and How). We have experimented with Noun tag, Verb tag and 'Wh-word' tag as shown in tables. Also, we have experimented in each tag by varying the number of tags from 1 to 7. We combined different tags using 1D-convolution, concatenation, and addition of all the tags and observed that the concatenation mechanism gives better results. As we can see in the table TABREF33 that taking Nouns, Verbs and Wh-Words as context, we achieve significant improvement in the BLEU, METEOR and CIDEr scores from the basic models which only takes the image and the caption respectively. Taking Nouns generated from the captions and questions of the corresponding training example as context, we achieve an increase of 1.6% in Bleu Score and 2% in METEOR and 34.4% in CIDEr Score from the basic Image model. Similarly taking Verbs as context gives us an increase of 1.3% in Bleu Score and 2.1% in METEOR and 33.5% in CIDEr Score from the basic Image model. And the best result comes when we take 3 Wh-Words as context and apply the Hadamard Model with concatenating the 3 WH-words. Also in Table TABREF34 we have shown the results when we take more than one words as context. Here we show that for 3 words i.e 3 nouns, 3 verbs and 3 Wh-words, the Concatenation model performs the best. In this table the conv model is using 1D convolution to combine the tags and the joint model combine all the tags. Analysis of Context: Exemplars In Multimodel Differential Network and Differential Image Network, we use exemplar images(target, supporting and opposing image) to obtain the differential context. We have performed the experiment based on the single exemplar(K=1), which is one supporting and one opposing image along with target image, based on two exemplar(K=2), i.e. two supporting and two opposing image along with single target image. similarly we have performed experiment for K=3 and K=4 as shown in table- TABREF35 . Mixture Module: Other Variations Hadamard method uses element-wise multiplication whereas Addition method uses element-wise addition in place of the concatenation operator of the Joint method. The Hadamard method finds a correlation between image feature and caption feature vector while the Addition method learns a resultant vector. In the attention method, the output INLINEFORM35 is the weighted average of attention probability vector INLINEFORM36 and convolutional features INLINEFORM37 . The attention probability vector weights the contribution of each convolutional feature based on the caption vector. This attention method is similar to work stack attention method BIBREF54 . The attention mechanism is given by: DISPLAYFORM0 where INLINEFORM38 is the 14x14x512-dimensional convolution feature map from the fifth convolution layer of VGG-19 Net of image INLINEFORM39 and INLINEFORM40 is the caption context vector. The attention probability vector INLINEFORM41 is a 196-dimensional vector. INLINEFORM42 are the weights and INLINEFORM43 is the bias for different layers. We evaluate the different approaches and provide results for the same. Here INLINEFORM44 represents element-wise addition. Evaluation Metrics Our task is similar to encoder -decoder framework of machine translation. we have used same evaluation metric is used in machine translation. BLEU BIBREF46 is the first metric to find the correlation between generated question with ground truth question. BLEU score is used to measure the precision value, i.e That is how much words in the predicted question is appeared in reference question. BLEU-n score measures the n-gram precision for counting co-occurrence on reference sentences. we have evaluated BLEU score from n is 1 to 4. The mechanism of ROUGE-n BIBREF48 score is similar to BLEU-n,where as, it measures recall value instead of precision value in BLEU. That is how much words in the reference question is appeared in predicted question.Another version ROUGE metric is ROUGE-L, which measures longest common sub-sequence present in the generated question. METEOR BIBREF47 score is another useful evaluation metric to calculate the similarity between generated question with reference one by considering synonyms, stemming and paraphrases. the output of the METEOR score measure the word matches between predicted question and reference question. In VQG, it compute the word match score between predicted question with five reference question. CIDer BIBREF49 score is a consensus based evaluation metric. It measure human-likeness, that is the sentence is written by human or not. The consensus is measured, how often n-grams in the predicted question are appeared in the reference question. If the n-grams in the predicted question sentence is appeared more frequently in reference question then question is less informative and have low CIDer score. We provide our results using all these metrics and compare it with existing baselines. 
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Question Answering by Reasoning Across Documents with Graph Convolutional Networks</title>
<abstract>Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within-and crossdocument coreference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WIKIHOP (Welbl et al., 2018).</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
The long-standing goal of natural language understanding is the development of systems which can acquire knowledge from text collections. Fresh interest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD BIBREF1 and CNN/Daily Mail BIBREF2 , enabling end-to-end training of neural models BIBREF3 , BIBREF4 , BIBREF5 . These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence BIBREF6 . The last generation of large-scale reading comprehension datasets, such as a NarrativeQA BIBREF7 , TriviaQA BIBREF8 , and RACE BIBREF9 , have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance.

Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The WikiHop dataset BIBREF0 was explicitly created to facilitate the development of systems dealing with these scenarios. Each example in WikiHop consists of a collection of documents, a query and a set of candidate answers (Figure 1 ). Though there is no guarantee that a question cannot be answered by relying just on a single sentence, the authors ensure that it is answerable using a chain of reasoning crossing document boundaries.

Though an important practical problem, the multi-hop setting has so far received little attention. The methods reported by BIBREF0 approach the task by merely concatenating all documents into a single long text and training a standard RNN-based reading comprehension model, namely, BiDAF BIBREF3 and FastQA BIBREF6 . Document concatenation in this setting is also used in Weaver BIBREF10 and MHPGM BIBREF11 . The only published paper which goes beyond concatenation is due to BIBREF12 , where they augment RNNs with jump-links corresponding to co-reference edges. Though these edges provide a structural bias, the RNN states are still tasked with passing the information across the document and performing multi-hop reasoning.

Instead, we frame question answering as an inference problem on a graph representing the document collection. Nodes in this graph correspond to named entities in a document whereas edges encode relations between them (e.g., cross- and within-document coreference links or simply co-occurrence in a document). We assume that reasoning chains can be captured by propagating local contextual information along edges in this graph using a graph convolutional network (GCN) BIBREF13 .

The multi-document setting imposes scalability challenges. In realistic scenarios, a system needs to learn to answer a query for a given collection (e.g., Wikipedia or a domain-specific set of documents). In such scenarios one cannot afford to run expensive document encoders (e.g., RNN or transformer-like self-attention BIBREF14 ), unless the computation can be preprocessed both at train and test time. Even if (similarly to WikiHop creators) one considers a coarse-to-fine approach, where a set of potentially relevant documents is provided, re-encoding them in a query-specific way remains the bottleneck. In contrast to other proposed methods (e.g., BIBREF12 , BIBREF10 , BIBREF3 ), we avoid training expensive document encoders.

In our approach, only a small query encoder, the GCN layers and a simple feed-forward answer selection component are learned. Instead of training RNN encoders, we use contextualized embeddings (ELMo) to obtain initial (local) representations of nodes. This implies that only a lightweight computation has to be performed online, both at train and test time, whereas the rest is preprocessed. Even in the somewhat contrived WikiHop setting, where fairly small sets of candidates are provided, the model is at least 5 times faster to train than BiDAF. Interestingly, when we substitute ELMo with simple pre-trained word embeddings, Entity-GCN still performs on par with many techniques that use expensive question-aware recurrent document encoders.

Despite not using recurrent document encoders, the full Entity-GCN model achieves over 2% improvement over the best previously-published results. As our model is efficient, we also reported results of an ensemble which brings further 3.6% of improvement and only 3% below the human performance reported by BIBREF0 . Our contributions can be summarized as follows:
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Method</section-title>
<section-number>1</section-number>
<paragraphs>
In this section we explain our method. We first introduce the dataset we focus on, WikiHop by BIBREF0 , as well as the task abstraction. We then present the building blocks that make up our Entity-GCN model, namely, an entity graph used to relate mentions to entities within and across documents, a document encoder used to obtain representations of mentions in context, and a relational graph convolutional network that propagates information through the entity graph.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Dataset and task abstraction</section-title>
<section-number>2</section-number>
<paragraphs>
The WikiHop dataset comprises of tuples $\langle q, S_q, C_q, a^\star \rangle $ where: $q$ is a query/question, $S_q$ is a set of supporting documents, $C_q$ is a set of candidate answers (all of which are entities mentioned in $S_q$ ), and $a^\star \in C_q$ is the entity that correctly answers the question. WikiHop is assembled assuming that there exists a corpus and a knowledge base (KB) related to each other. The KB contains triples $\langle s, r, o \rangle $ where $s$ is a subject entity, $o$ an object entity, and $r$ a unidirectional relation between them. BIBREF0 used Wikipedia as corpus and Wikidata BIBREF15 as KB. The KB is only used for constructing WikiHop: BIBREF0 retrieved the supporting documents $q$0 from the corpus looking at mentions of subject and object entities in the text. Note that the set $q$1 (not the KB) is provided to the QA system, and not all of the supporting documents are relevant for the query but some of them act as distractors. Queries, on the other hand, are not expressed in natural language, but instead consist of tuples $q$2 where the object entity is unknown and it has to be inferred by reading the support documents. Therefore, answering a query corresponds to finding the entity $q$3 that is the object of a tuple in the KB with subject $q$4 and relation $q$5 among the provided set of candidate answers $q$6 .

The goal is to learn a model that can identify the correct answer $a^\star $ from the set of supporting documents $S_q$ . To that end, we exploit the available supervision to train a neural network that computes scores for candidates in $C_q$ . We estimate the parameters of the architecture by maximizing the likelihood of observations. For prediction, we then output the candidate that achieves the highest probability. In the following, we present our model discussing the design decisions that enable multi-step reasoning and an efficient computation.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Reasoning on an entity graph</section-title>
<section-number>3</section-number>
<paragraphs>
In an offline step, we organize the content of each training instance in a graph connecting mentions of candidate answers within and across supporting documents. For a given query $q = \langle s, r, ? \rangle $ , we identify mentions in $S_q$ of the entities in $C_q \cup \lbrace s\rbrace $ and create one node per mention. This process is based on the following heuristic:

we consider mentions spans in $S_q$ exactly matching an element of $C_q \cup \lbrace s\rbrace $ . Admittedly, this is a rather simple strategy which may suffer from low recall.

we use predictions from a coreference resolution system to add mentions of elements in $C_q \cup \lbrace s\rbrace $ beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end coreference resolution by BIBREF16 .

we discard mentions which are ambiguously resolved to multiple coreference chains; this may sacrifice recall, but avoids propagating ambiguity.

To each node $v_i$ , we associate a continuous annotation $\mathbf {x}_i \in \mathbb {R}^D$ which represents an entity in the context where it was mentioned (details in Section "Node annotations" ). We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges—these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges). Note that MATCH edges when connecting mentions in the same document are mostly included in the set of edges predicted by the coreference system. Having the two types of edges lets us distinguish between less reliable edges provided by the coreference system and more reliable (but also more sparse) edges given by the exact-match heuristic. We treat these three types of connections as three different types of relations. See Figure 2 for an illustration. In addition to that, and to prevent having disconnected graphs, we add a fourth type of relation (COMPLEMENT edge) between any two nodes that are not connected with any of the other relations. We can think of these edges as those in the complement set of the entity graph with respect to a fully connected graph.

Our model then approaches multi-step reasoning by transforming node representations (Section "Node annotations" for details) with a differentiable message passing algorithm that propagates information through the entity graph. The algorithm is parameterized by a graph convolutional network (GCN) BIBREF13 , in particular, we employ relational-GCNs BIBREF17 , an extended version that accommodates edges of different types. In Section "Entity relational graph convolutional network" we describe the propagation rule.

Each step of the algorithm (also referred to as a hop) updates all node representations in parallel. In particular, a node is updated as a function of messages from its direct neighbours, and a message is possibly specific to a certain relation. At the end of the first step, every node is aware of every other node it connects directly to. Besides, the neighbourhood of a node may include mentions of the same entity as well as others (e.g., same-document relation), and these mentions may have occurred in different documents. Taking this idea recursively, each further step of the algorithm allows a node to indirectly interact with nodes already known to their neighbours. After $L$ layers of R-GCN, information has been propagated through paths connecting up to $L+1$ nodes.

We start with node representations $\lbrace \mathbf {h}_i^{(0)}\rbrace _{i=1}^N$ , and transform them by applying $L$ layers of R-GCN obtaining $\lbrace \mathbf {h}_i^{(L)}\rbrace _{i=1}^N$ . Together with a representation $\mathbf {q}$ of the query, we define a distribution over candidate answers and we train maximizing the likelihood of observations. The probability of selecting a candidate $c \in C_q$ as an answer is then 

$$ 
P(c|q, C_q, S_q) \propto \exp \left(\max _{i \in \mathcal {M}_c} f_o([\mathbf {q}, \mathbf {h}^{(L)}_i]) \right)\;,$$   (Eq. 16) 

where $f_o$ is a parameterized affine transformation, and $\mathcal {M}_c$ is the set of node indices such that $i\in \mathcal {M}_c$ only if node $v_i$ is a mention of $c$ . The $\max $ operator in Equation 16 is necessary to select the node with highest predicted probability since a candidate answer is realized in multiple locations via different nodes.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Node annotations</section-title>
<section-number>4</section-number>
<paragraphs>
Keeping in mind we want an efficient model, we encode words in supporting documents and in the query using only a pre-trained model for contextualized word representations rather than training our own encoder. Specifically, we use ELMo BIBREF20 , a pre-trained bi-directional language model that relies on character-based input representation. ELMo representations, differently from other pre-trained word-based models (e.g., word2vec BIBREF21 or GloVe BIBREF22 ), are contextualized since each token representation depends on the entire text excerpt (i.e., the whole sentence).

We choose not to fine tune nor propagate gradients through the ELMo architecture, as it would have defied the goal of not having specialized RNN encoders. In the experiments, we will also ablate the use of ELMo showing how our model behaves using non-contextualized word representations (we use GloVe).

ELMo encodings are used to produce a set of representations $\lbrace \mathbf {x}_i\rbrace _{i=1}^N$ , where $\mathbf {x}_i \in \mathbb {R}^D$ denotes the $i$ th candidate mention in context. Note that these representations do not depend on the query yet and no trainable model was used to process the documents so far, that is, we use ELMo as a fixed pre-trained encoder. Therefore, we can pre-compute representation of mentions once and store them for later use.

ELMo encodings are used to produce a query representation $\mathbf {q} \in \mathbb {R}^K$ as well. Here, $\mathbf {q}$ is a concatenation of the final outputs from a bidirectional RNN layer trained to re-encode ELMo representations of words in the query. The vector $\mathbf {q}$ is used to compute a query-dependent representation of mentions $\lbrace \mathbf { \hat{x}}_i\rbrace _{i=1}^N$ as well as to compute a probability distribution over candidates (as in Equation 16 ). Query-dependent mention encodings $\mathbf {\hat{x}}_i = f_x(\mathbf {q}, \mathbf {x}_i)$ are generated by a trainable function $f_x$ which is parameterized by a feed-forward neural network.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Entity relational graph convolutional network</section-title>
<section-number>5</section-number>
<paragraphs>
Our model uses a gated version of the original R-GCN propagation rule. At the first layer, all hidden node representation are initialized with the query-aware encodings $\mathbf {h}_i^{(0)} = \mathbf {\hat{x}}_i$ . Then, at each layer $0\le \ell \le L$ , the update message $\mathbf {u}_i^{(\ell )}$ to the $i$ th node is a sum of a transformation $f_s$ of the current node representation $\mathbf {h}^{(\ell )}_i$ and transformations of its neighbours: 

$$\mathbf {u}^{(\ell )}_i = f_s(\mathbf {h}^{(\ell )}_i) + \frac{1}{|\mathcal {N}_i|} \sum _{j \in \mathcal {N}_i} \sum _{r \in \mathcal {R}_{ij}} f_r(\mathbf {h}_j^{(\ell )})\;,$$   (Eq. 22) 

where $\mathcal {N}_i$ is the set of indices of nodes neighbouring the $i$ th node, $\mathcal {R}_{ij}$ is the set of edge annotations between $i$ and $j$ , and $f_r$ is a parametrized function specific to an edge type $r\in \mathcal {R}$ . Recall the available relations from Section "Ablation study" , namely, $\mathcal {R} =\lbrace $ DOC-BASED, MATCH, COREF, COMPLEMENT $\rbrace $ .

A gating mechanism regulates how much of the update message propagates to the next step. This provides the model a way to prevent completely overwriting past information. Indeed, if all necessary information to answer a question is present at a layer which is not the last, then the model should learn to stop using neighbouring information for the next steps. Gate levels are computed as 

$$\mathbf {a}^{(\ell )}_i = \sigma \left( f_a\left([\mathbf {u}^{(\ell )}_i, \mathbf {h}^{(\ell )}_i ]\right) \right) \;,$$   (Eq. 23) 

where $\sigma (\cdot )$ is the sigmoid function and $f_a$ a parametrized transformation. Ultimately, the updated representation is a gated combination of the previous representation and a non-linear transformation of the update message: 

$$\mathbf {h}^{(\ell + 1)}_i = \phi (\mathbf {u}^{(\ell )}_i) \odot \mathbf {a}^{(\ell )}_i + \mathbf {h}^{(\ell )}_i \odot (1 - \mathbf {a}^{(\ell )}_i ) \;,$$   (Eq. 24) 

where $\phi (\cdot )$ is any nonlinear function (we used $\tanh $ ) and $\odot $ stands for element-wise multiplication. All transformations $f_*$ are affine and they are not layer-dependent (since we would like to use as few parameters as possible to decrease model complexity promoting efficiency and scalability).
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Experiments</section-title>
<section-number>6</section-number>
<paragraphs>
In this section, we compare our method against recent work as well as preforming an ablation study using the WikiHop dataset BIBREF0 . See Appendix "Implementation and experiments details" in the supplementary material for a description of the hyper-parameters of our model and training details.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Comparison</section-title>
<section-number>7</section-number>
<paragraphs>
In this experiment, we compare our Enitity-GCN against recent prior work on the same task. We present test and development results (when present) for both versions of the dataset in Table 2 . From BIBREF0 , we list an oracle based on human performance as well as two standard reading comprehension models, namely BiDAF BIBREF3 and FastQA BIBREF6 . We also compare against Coref-GRU BIBREF12 , MHPGM BIBREF11 , and Weaver BIBREF10 . Additionally, we include results of MHQA-GRN BIBREF23 , from a recent arXiv preprint describing concurrent work. They jointly train graph neural networks and recurrent encoders. We report single runs of our two best single models and an ensemble one on the unmasked test set (recall that the test set is not publicly available and the task organizers only report unmasked results) as well as both versions of the validation set.

Entity-GCN (best single model without coreference edges) outperforms all previous work by over 2% points. We additionally re-ran BiDAF baseline to compare training time: when using a single Titan X GPU, BiDAF and Entity-GCN process 12.5 and 57.8 document sets per second, respectively. Note that BIBREF0 had to use BiDAF with very small state dimensionalities (20), and smaller batch size due to the scalability issues (both memory and computation costs). We compare applying the same reductions. Eventually, we also report an ensemble of 5 independently trained models. All models are trained on the same dataset splits with different weight initializations. The ensemble prediction is obtained as $\arg \max \limits _c \prod \limits _{i=1}^5 P_i(c|q, C_q, S_q)$ from each model.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Ablation study</section-title>
<section-number>8</section-number>
<paragraphs>
To help determine the sources of improvements, we perform an ablation study using the publicly available validation set (see Table 3 ). We perform two groups of ablation, one on the embedding layer, to study the effect of ELMo, and one on the edges, to study how different relations affect the overall model performance.

We argue that ELMo is crucial, since we do not rely on any other context encoder. However, it is interesting to explore how our R-GCN performs without it. Therefore, in this experiment, we replace the deep contextualized embeddings of both the query and the nodes with GloVe BIBREF22 vectors (insensitive to context). Since we do not have any component in our model that processes the documents, we expect a drop in performance. In other words, in this ablation our model tries to answer questions without reading the context at all. For example, in Figure 1 , our model would be aware that “Stockholm” and “Sweden” appear in the same document but any context words, including the ones encoding relations (e.g., “is the capital of”) will be hidden. Besides, in the masked case all mentions become `unknown' tokens with GloVe and therefore the predictions are equivalent to a random guess. Once the strong pre-trained encoder is out of the way, we also ablate the use of our R-GCN component, thus completely depriving the model from inductive biases that aim at multi-hop reasoning.

The first important observation is that replacing ELMo by GloVe (GloVe with R-GCN in Table 3 ) still yields a competitive system that ranks far above baselines from BIBREF0 and even above the Coref-GRU of BIBREF12 , in terms of accuracy on (unmasked) validation set. The second important observation is that if we then remove R-GCN (GloVe w/o R-GCN in Table 3 ), we lose 8.0 points. That is, the R-GCN component pushes the model to perform above Coref-GRU still without accessing context, but rather by updating mention representations based on their relation to other ones. These results highlight the impact of our R-GCN component.

In this experiment we investigate the effect of the different relations available in the entity graph and processed by the R-GCN module. We start off by testing our stronger encoder (i.e., ELMo) in absence of edges connecting mentions in the supporting documents (i.e., using only self-loops – No R-GCN in Table 3 ). The results suggest that WikipHop genuinely requires multihop inference, as our best model is 6.1% and 8.4% more accurate than this local model, in unmasked and masked settings, respectively. However, it also shows that ELMo representations capture predictive context features, without being explicitly trained for the task. It confirms that our goal of getting away with training expensive document encoders is a realistic one.

We then inspect our model's effectiveness in making use of the structure encoded in the graph. We start naively by fully-connecting all nodes within and across documents without distinguishing edges by type (No relation types in Table 3 ). We observe only marginal improvements with respect to ELMo alone (No R-GCN in Table 3 ) in both the unmasked and masked setting suggesting that a GCN operating over a naive entity graph would not add much to this task and a more informative graph construction and/or a more sophisticated parameterization is indeed needed.

Next, we ablate each type of relations independently, that is, we either remove connections of mentions that co-occur in the same document (DOC-BASED), connections between mentions matching exactly (MATCH), or edges predicted by the coreference system (COREF). The first thing to note is that the model makes better use of DOC-BASED connections than MATCH or COREF connections. This is mostly because i) the majority of the connections are indeed between mentions in the same document, and ii) without connecting mentions within the same document we remove important information since the model is unaware they appear closely in the document. Secondly, we notice that coreference links and complement edges seem to play a more marginal role. Though it may be surprising for coreference edges, recall that the MATCH heuristic already captures the easiest coreference cases, and for the rest the out-of-domain coreference system may not be reliable. Still, modelling all these different relations together gives our Entity-GCN a clear advantage. This is our best system evaluating on the development. Since Entity-GCN seems to gain little advantage using the coreference system, we report test results both with and without using it. Surprisingly, with coreference, we observe performance degradation on the test set. It is likely that the test documents are harder for the coreference system.

We do perform one last ablation, namely, we replace our heuristic for assigning edges and their labels by a model component that predicts them. The last row of Table 3 (Induced edges) shows model performance when edges are not predetermined but predicted. For this experiment, we use a bilinear function $f_e(\mathbf {\hat{x}}_i, \mathbf {\hat{x}}_j) = \sigma \left( \mathbf {\hat{x}}^\top _i \mathbf {W}_e \mathbf {\hat{x}}_j \right)$ that predicts the importance of a single edge connecting two nodes $i,j$ using the query-dependent representation of mentions (see Section "Node annotations" ). The performance drops below `No R-GCN' suggesting that it cannot learn these dependencies on its own.

Most results are stronger for the masked settings even though we do not apply the coreference resolution system in this setting due to masking. It is not surprising as coreferred mentions are labeled with the same identifier in the masked version, even if their original surface forms did not match ( BIBREF0 used Wikipedia links for masking). Indeed, in the masked version, an entity is always referred to via the same unique surface form (e.g., MASK1) within and across documents. In the unmasked setting, on the other hand, mentions to an entity may differ (e.g., “US” vs “United States”) and they might not be retrieved by the coreference system we are employing, making the task harder for all models. Therefore, as we rely mostly on exact matching when constructing our graph for the masked case, we are more effective in recovering coreference links on the masked rather than unmasked version.

In Figure 3 , we show how the model performance goes when the input graph is large. In particular, how Entity-GCN performs as the number of candidate answers or the number of nodes increases.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Error analysis</section-title>
<section-number>9</section-number>
<paragraphs>
In this section we provide an error analysis for our best single model predictions. First of all, we look at which type of questions our model performs well or poorly. There are more than 150 query types in the validation set but we filtered the three with the best and with the worst accuracy that have at least 50 supporting documents and at least 5 candidates. We show results in Table 4 . We observe that questions regarding places (birth and death) are considered harder for Entity-GCN. We then inspect samples where our model fails while assigning highest likelihood and noticed two principal sources of failure i) a mismatch between what is written in Wikipedia and what is annotated in Wikidata, and ii) a different degree of granularity (e.g., born in “London” vs “UK” could be considered both correct by a human but not when measuring accuracy). See Table 6 in the supplement material for some reported samples.

Secondly, we study how the model performance degrades when the input graph is large. In particular, we observe a negative Pearson's correlation (-0.687) between accuracy and the number of candidate answers. However, the performance does not decrease steeply. The distribution of the number of candidates in the dataset peaks at 5 and has an average of approximately 20. Therefore, the model does not see many samples where there are a large number of candidate entities during training. Differently, we notice that as the number of nodes in the graph increases, the model performance drops but more gently (negative but closer to zero Pearson's correlation). This is important as document sets can be large in practical applications. See Figure 3 in the supplemental material for plots.

In Table 6 , we report three samples from WikiHop development set where out Entity-GCN fails. In particular, we show two instances where our model presents high confidence on the answer, and one where is not. We commented these samples explaining why our model might fail in these cases.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Related work</section-title>
<section-number>10</section-number>
<paragraphs>
In previous work, BiDAF BIBREF3 , FastQA BIBREF6 , Coref-GRU BIBREF12 , MHPGM BIBREF11 , and Weaver / Jenga BIBREF10 have been applied to multi-document question answering. The first two mainly focus on single document QA and BIBREF0 adapted both of them to work with WikiHop. They process each instance of the dataset by concatenating all $d \in S_q$ in a random order adding document separator tokens. They trained using the first answer mention in the concatenated document and evaluating exact match at test time. Coref-GRU, similarly to us, encodes relations between entity mentions in the document. Instead of using graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep co-encoding model that uses several alternating bi-LSTMs to process the concatenated documents and the query.

Graph neural networks have been shown successful on a number of NLP tasks BIBREF24 , BIBREF25 , BIBREF26 , including those involving document level modeling BIBREF27 . They have also been applied in the context of asking questions about knowledge contained in a knowledge base BIBREF28 . In schlichtkrull2017modeling, GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by BIBREF23 are the first to study graph neural networks in the context of multi-document QA. Besides differences in the architecture, BIBREF23 propose to train a combination of a graph recurrent network and an RNN encoder. We do not train any RNN document encoders in this work.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Conclusion</section-title>
<section-number>11</section-number>
<paragraphs>
We designed a graph neural network that operates over a compact graph representation of a set of documents where nodes are mentions to entities and edges signal relations such as within and cross-document coreference. The model learns to answer questions by gathering evidence from different documents via a differentiable message passing algorithm that updates node representations based on their neighbourhood. Our model outperforms published results where ablations show substantial evidence in favour of multi-step reasoning. Moreover, we make the model fast by using pre-trained (contextual) embeddings.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Acknowledgments</section-title>
<section-number>12</section-number>
<paragraphs>
We would like to thank Johannes Welbl for helping to test our system on WikiHop. This project is supported by SAP Innovation Center Network, ERC Starting Grant BroadSem (678254) and the Dutch Organization for Scientific Research (NWO) VIDI 639.022.518. Wilker Aziz is supported by the Dutch Organisation for Scientific Research (NWO) VICI Grant nr. 277-89-002.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Architecture</section-title>
<section-number>13</section-number>
<paragraphs>
See table 5 for an outline of Entity-GCN architectural detail. Here the computational steps

ELMo embeddings are a concatenation of three 1024-dimensional vectors resulting in 3072-dimensional input vectors $\lbrace \mathbf {x}_i\rbrace _{i=1}^N$ .

For the query representation $\mathbf {q}$ , we apply 2 bi-LSTM layers of 256 and 128 hidden units to its ELMo vectors. The concatenation of the forward and backward states results in a 256-dimensional question representation.

ELMo embeddings of candidates are projected to 256-dimensional vectors, concatenated to the $\mathbf {q}$ , and further transformed with a two layers MLP of 1024 and 512 hidden units in 512-dimensional query aware entity representations $\lbrace \mathbf {\hat{x}}_i\rbrace _{i=1}^N \in \mathbb {R}^{512}$ .

All transformations $f_*$ in R-GCN-layers are affine and they do maintain the input and output dimensionality of node representations the same (512-dimensional).

Eventually, a 2-layers MLP with [256, 128] hidden units takes the concatenation between $\lbrace \mathbf {h}_i^{(L)}\rbrace _{i=1}^N$ and $\mathbf {q}$ to predict the probability that a candidate node $v_i$ may be the answer to the query $q$ (see Equation 16 ).

During preliminary trials, we experimented with different numbers of R-GCN-layers (in the range 1-7). We observed that with WikiHop, for $L \ge 3$ models reach essentially the same performance, but more layers increase the time required to train them. Besides, we observed that the gating mechanism learns to keep more and more information from the past at each layer making unnecessary to have more layers than required.
</paragraphs>
</section>

---Paper Title: Question Answering by Reasoning Across Documents with Graph Convolutional Networks---
<section>
<section-title>Training details</section-title>
<section-number>14</section-number>
<paragraphs>
We train our models with a batch size of 32 for at most 20 epochs using the Adam optimizer BIBREF29 with $\beta _1=0.9$ , $\beta _2=0.999$ and a learning rate of $10^{-4}$ . To help against overfitting, we employ dropout (drop rate $\in {0, 0.1, 0.15, 0.2, 0.25}$ ) BIBREF30 and early-stopping on validation accuracy. We report the best results of each experiment based on accuracy on validation set.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>TutorialVQA: Question Answering Dataset for Tutorial Videos</title>
<abstract>Despite the number of currently available datasets on video question answering, there still remains a need for a dataset involving multi-step and non-factoid answers. Moreover, relying on video transcripts remains an under-explored topic. To adequately address this, We propose a new question answering task on instructional videos, because of their verbose and narrative nature. While previous studies on video question answering have focused on generating a short text as an answer, given a question and video clip, our task aims to identify a span of a video segment as an answer which contains instructional details with various granularities. This work focuses on screencast tutorial videos pertaining to an image editing program. We introduce a dataset, TutorialVQA, consisting of about 6,000manually collected triples of (video, question, answer span). We also provide experimental results with several baselines algorithms using the video transcripts. The results indicate that the task is challenging and call for the investigation of new algorithms.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Video is the fastest growing medium to create and deliver information today. Consequentially, videos have been increasingly used as main data sources in many question answering problems BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF2, BIBREF5. These previous studies have mostly focused on factoid questions, each of which can be answered in a few words or phrases generated by understanding multimodal contents in a short video clip.

However, this problem definition of video question answering causes some practical limitations for the following reasons. First, factoid questions are just a small part of what people actually want to ask on video contents. Especially if a short video is given to users, most fragmentary facts within the scope of previous tasks can be easily perceived by themselves even before asking questions. Thus, video question answering is expected to provide answers to more complicated non-factoid questions beyond the simple facts. For example, those questions could be the ones asking about a how procedure as shown in Fig. FIGREF5, and the answers should contain all necessary steps to complete the task.

Accordingly, the answer format needs to also be improved towards more flexible ways than multiple choice BIBREF1, BIBREF2 or fill-in-the-blank questions BIBREF3, BIBREF4. Although open-ended video question answering BIBREF0, BIBREF2, BIBREF5 has been explored, it still aims to generate just a short word or phrase-level answer, which is not enough to cover various granularities of non-factoid question answering.

The other issue is that most videos with sufficient amount of information, which are likely to be asked, have much longer lengths than the video clips in the existing datasets. Therefore, the most relevant part of a whole video needs to be determined prior to each answer generation in practice. However, this localization task has been out of scope for previous studies.

In this work, we propose a new question answering problem for non-factoid questions on instructional videos. According to the nature of the media created for educational purposes, we assume that many answers already exist within the given video contents. Under this assumption, we formulate the problem as a localization task to specify the span of a video segment as the direct answer to a given video and a question, as illustrated in Figure FIGREF1.

The remainder of this paper is structured as follows. Section SECREF3 introduces TutorialVQA dataset as a case study of our proposed problem. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software. Section SECREF4 presents the baseline models and their experiment details on the sentence-level prediction and video segment retrieval tasks on our dataset. Then, we discuss the experimental results in Section SECREF5 and conclude the paper in Section SECREF6.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>Related Work</section-title>
<section-number>1</section-number>
<paragraphs>
Most relevant to our proposed work is the reading comprehension task, which is a question answering task involving a piece of text such as a paragraph or article. Such datasets for the reading comprehension task, such as SQuAD BIBREF6 based on Wikipedia, TriviaQA BIBREF7 constructed from trivia questions with answer evidence from Wikipedia, or those from Hermann et al. based on CNN and Daily Mail articles BIBREF8 are factoid-based, meaning the answers typically involve a single entity. Differing from video transcripts, the structures of these data sources, namely paragraphs from Wikipedia and news sources, are typically straightforward since they are meant to be read. In contrast, video transcripts originate from spoken dialogue, which can verbose, unstructured, and disconnected. Furthermore, the answers in instructional video transcripts can be longer, spanning multiple sentences if the process is multi-step or even fragmented into multiple segments throughout the video.

Visual corpora in particular have proven extremely valuable to visual questions-answering tasks BIBREF9, the most similar being MovieQA BIBREF1 and VideoQA BIBREF0. Similar to how our data is generated from video tutorials, the MovieQA and VideoQA corpus is generated from movie scripts and news transcipts, respectively. MovieQA's answers have a shorter span than the answers collected in our corpus, because questions and answer pairs were generated after each paragraph in a movie's plot synopsis BIBREF1. The MovieQA dataset also contains specific annotated answers with incorrect examples for each question. In the VideoQA dataset, questions focus on a single entity, contrary to our instructional video dataset. Although not necessarily a visual question-answering task, the work proposed by BIBREF10 involved answering questions over transcript data. Contrary to our work, Gupta's dataset is not publically available and their examples only showcase factoid-style questions involving single entity answers.

BIBREF11 focus on aligning a set of instructions to a video of someone carrying out those instructions. In their task, they use the video transcript to represent the video, which they later augment with a visual cue detector on food entities. Their task focuses on procedure-based cooking videos, and contrary to our task is primarily a text alignment task. In our task we aim to answer questions-using the transcripts-on instructional-style videos, in which the answer can involve steps not mentioned in the question.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>TutorialVQA Dataset</section-title>
<section-number>2</section-number>
<paragraphs>
In this section, we introduce the TutorialVQA dataset and describe the data collection process .
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>TutorialVQA Dataset ::: Overview</section-title>
<section-number>3</section-number>
<paragraphs>
Our dataset consists of 76 tutorial videos pertaining to an image editing software. All of the videos include spoken instructions which are transcribed and manually segmented into multiple segments. Specifically, we asked the annotators to manually divide each video into multiple segments such that each of the segments can serve as an answer to any question. For example, Fig. FIGREF1 shows example segments marked in red (each which are a complete unit as an answer span). Each sentence is associated with the starting and ending time-stamps, which can be used to access the relevant visual information.

The dataset contains 6,195 non-factoid QA pairs, where the answers are the segments that were manually annotated. Fig. FIGREF5 shows an example of the annotations. video_id can be used to retrieve the video information such as meta information and the transcripts. answer_start and answer_end denote the starting and ending sentence indexes of the answer span. Table. TABREF4 shows the statistics of our dataset, with each answer segment having on average about 6 sentences, showing that our answers are more verbose than those in previous factoid QA tasks.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>TutorialVQA Dataset ::: Basis</section-title>
<section-number>4</section-number>
<paragraphs>
We chose videos pertaining to an image editing software because of the complexity and variety of tasks involved. In these videos, a narrator is communicating an overall goal by utilizing an example. For example, in FIGREF1 the video pertains to combining multiple layers into one image. However, throughout the videos multiple subtasks are achieved, such as the opening of multiple images, the masking of images, and the placement of two images side-by-side. These subtasks involve multiple steps and are of interest to us in segmenting the videos. Each segment can be seen as a subtask within a larger video dictating an example. We thus chose these videos because of the amount of procedural information stored in each video for which the user may ask. Though there is only one domain, each video corresponds to a different overall goal.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>TutorialVQA Dataset ::: Data Collection</section-title>
<section-number>5</section-number>
<paragraphs>
We downloaded 76 videos from a tutorial website about an image editing program . Each video is pre-processed to provide the transcripts and the time-stamp information for each sentence in the transcript. We then used Amazon Mechanical Turk to collect the question-answer pairs . One naive way of collecting the data is to prepare a question list and then, for each question, ask the workers to find the relevant parts in the video. However, this approach is not feasible and error-prone because the videos are typically long and finding a relevant part from a long video is difficult. Doing so might also cause us to miss questions which were relevant to the video segment. Instead, we took a reversed approach. First, for each video, we manually identified the sentence spans that can serve as answers. These candidates are of various granularity and may overlap. The segments are also complete in that they encompass the beginning and end of a task. In total, we identified 408 segments from the 76 videos. Second we asked AMT workers to provide question annotations for the videos.

Our AMT experiment consisted of two parts. In the first part, we presented the workers with the video content of a segment. For each segment, we asked workers to generate questions that can be answered by the presented segment. We did not limit the number of questions a worker can input to a corresponding segment and encouraged them to input a diverse set of questions which the span can answer. Along with the questions, the workers were also required to provide a justification as to why they made their questions. We manually checked this justification to filter out the questions with poor quality by removing those questions which were unrelated to the video. One initial challenge worth mentioning is that at first some workers input questions they had about the video and not questions which the video could answer. This was solved by providing them with an unrelated example. The second part of the question collection framework consisted of a paraphrasing task. In this task we presented workers with the questions generated by the first task and asked them to write the questions differently while keeping the semantics the same. In this way, we expanded our question dataset. After filtering out the questions with low quality, we collected a total of 6,195 questions.

It is important to note the differences between our data collection process and the the query generation process employed in the Search and Hyperlinking Task at MediaEval BIBREF12. In the Search and Hyperlinking Task, 30 users were tasked to first browse the collection of videos, select interesting segments with start and end times, and then asked to conjecture questions that they would use on a search query to find the interesting video segments. This was done in order to emulate their thought-proces mechanism. While the nature of their task involves queries relating to the overall videos themselves, hence coming from a video's interestingness, our task involves users already being given a video and formulating questions where the answers themselves come from within a video. By presenting the same video segment to many users, we maintain a consistent set of video segments and extend the possibility to generate a diverse set of question for the same segment.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>TutorialVQA Dataset ::: Dataset Details</section-title>
<section-number>6</section-number>
<paragraphs>
Table TABREF12 presents some extracted sample questions from our dataset. The first column corresponds to an AMT generated question, while the second column corresponds to the video ID where the segment can be found. As can be seen in the first two rows, multiple types of questions can be answered within the same video (but different segments). The last two rows display questions which belong to the same segment but correspond to different properties of the same entity, 'crop tool'. Here we observe different types of questions, such as "why", "how", "what", and "where", and can see why the answers may involve multiple steps. Some questions that the worked paraphrased were in the "yes/no" style, however our answer segments then provide an explanation to these questions.

Each answer segment was extracted from an image editing tutorial video that involved multiple steps and procedures to produce a final image, which can partially be seen in FIGREF1. The average number of sentences per video was approximately 52, with the maximum number of sentences contained in a video being 187. The sub-tasks in the tutorial include segments (and thus answers) on editing parts of images, instructions on using certain tools, possible actions that can be performed on an image, and identifying the locations of tools and features, with the shortest and longest segment having a span of 1 and 37 sentences respectively, demonstrating the heterogeneity of the answer spans.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>Baselines</section-title>
<section-number>7</section-number>
<paragraphs>
Our video question answering task is novel and to our knowledge, no model has been designed specifically for this task. As a first step towards solving this problem, we evaluated the performance of state-of-the-art models developed for other QA tasks, including a sentence-level prediction task and two segment retrieval tasks. In this section, we report their results on the TutorialVQA dataset.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>Baselines ::: Baseline1: Sentence-level prediction</section-title>
<section-number>8</section-number>
<paragraphs>
Given a transcript (a sequence of sentences) and a question, Baseline1 predicts (starting sentence index, ending sentence index). The model is based on RaSor BIBREF13, which has been developed for the SQuAD QA task BIBREF6. RaSor concatenates the embedding vectors of the starting and the ending words to represent a span. Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model.

Model. The model takes two inputs, a transcript, $\lbrace s_1, s_2, ... s_n\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript.

where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence. Then, the model combines each pair of sentence embeddings ($p_i$, $p_j$) to generate a span embedding.

where [$\cdot $,$\cdot $] indicates the concatenation. Finally, we use a one-layer feed forward network to compute a score between each span and a question.

In training, we use cross-entropy as an objective function. In testing, the span with the highest score is picked as an answer.

Metrics. We use tolerance accuracy BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer – a difference of a few seconds would not matter much to the user.

Specifically, the predicted span is counted as correct if $|pred_{start} - gt_{start}| + |pred_{end} - gt_{end}| <=$ $k$, where $pred_{start/end}$ and $gt_{start/end}$ indicate the indices of the predicted and ground-truth starting and ending sentences, respectively. We then measure the percentage of correctly predicted questions among the entire test questions.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>Baselines ::: Baseline2: Segment retrieval</section-title>
<section-number>9</section-number>
<paragraphs>
We also considered a simpler task by casting our problem as a retrieval task. Specifically, in addition to a plain transcript, we also provided the model with the segmentation information which was created during the data collection phrase (See Section. SECREF3). Note that each segments corresponds to a candidate answer. Then, the task is to pick the best segment for given a query. This task is easier than Baseline1's task in that the segmentation information is provided to the model. Unlike Baseline1, however, it is unable to return an answer span at various granularities. Baseline2 is based on the attentive LSTM BIBREF17, which has been developed for the InsuranceQA task. The right diagram in Fig. FIGREF15 illustrates the Baseline2 model.

Model. The two inputs, $s$ and $q$ represent the segment text and a question. The model first encodes the two inputs.

$h^s$ is then re-weighted using attention weights.

where $\odot $ denotes the element-wise multiplication operation. The final score is computed using a one-layer feed-forward network.

During training, the model requires negative samples. For each positive example, (question, ground-truth segment), all the other segments in the same transcript are used as negative samples. Cross entropy is used as an objective function.

Metrics. We used accuracy and MRR (Mean Reciprocal Ranking) as metrics. The accuracy is

We split the ground-truth dataset to train/dev/test into the ratio of 6/2/2. The resulting size is 3,718 (train), 1,238 (dev) and 1,239 qa pairs (test).
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>Baselines ::: Baseline3: Pipeline Segment retrieval</section-title>
<section-number>10</section-number>
<paragraphs>
We construct a pipelined approach through another segment retrieval task, calculating the cosine similarities between the segment and question embeddings. In this task however, we want to test the accuracy of retrieving the segments given that we first retrieve the correct video from our 76 videos. First, we generate the TF-IDF embeddings for the whole video transcripts and questions. The next step involves retrieving the videos which have the lowest cosine distance between the video transcripts and question. We then filter and store the top ten videos, reducing the number of computations required in the next step. Finally, we calculate the cosine distances between the question and the segments which belong to the filtered top 10 videos, marking it as correct if found in these videos. While the task is less computationally expensive than the previous baseline, we do not learn the segment representations, as this task is a simple retrieval task based on TF-IDF embeddings.

Model. The first two inputs are are the question, q, and video transcript, v, encoded by their TF-IDF vectors: BIBREF18:

We then filter the top 10 video transcripts(out of 76) with the minimum cosine distance, and further compute the TF-IDF vectors for their segments, Stop10n, where n = 10. We repeat the process for the corresponding segments:

selecting the segment with the minimal cosine distance distance to the query.

Metrics. To evaluate our pipeline approach we use overall accuracy after filtering and accuracy given that the segment is in the top 10 videos. While the first metric is similar to SECREF17, the second can indicate if initially searching on the video space can be used to improve our selection:
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>Baselines ::: Results</section-title>
<section-number>11</section-number>
<paragraphs>
Tables TABREF20, TABREF21, TABREF22 show the results. First, the tables show that the two first baselines under-perform for our task. Even with a tolerance window of 6, Baseline1 merely achieves an accuracy of .14. Baseline2, despite being a simpler task, has only an accuracy of .23. Second, while we originally hypothesized that the segment selection task should be easier than the sentence prediction task, Table TABREF21 shows that the task is also challenging. One possible reason is that the segments contained within the same transcript have similar contents, due to the composition of the overall task in each video, and differentiating among them may require a more sophisticated model than just using a sequence model for segment representation. Table TABREF22 shows the accuracy of retrieving the correct segment, for baseline both overall and given that the video selected is within the top 10 videos. While the overall accuracy is only .16, by reducing the search space to 10 relevant videos our accuracy increases to 0.6385. In future iterations, it may then be useful to find better approaches in filtering large paragraphs of text before predicting the correct segment.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>Discussion and Future Work</section-title>
<section-number>12</section-number>
<paragraphs>
We performed an error analysis on Baseline1's results. We first observe that, in 92% of the errors, the predicted span and the ground-truth overlap. Furthermore, in 56% of the errors, the predicted spans are a subset or superset of the ground-truth spans. This indicates that the model finds the rough answer regions but fails to locate the precise boundaries. To address this issue, we plan on exploring the Pointer-network BIBREF19, which finds an answer span by selecting the boundary sentences. Unlike Baseline1 which avoids an explicit segmentation step, the Pointer-network can explicitly model which sentences are likely to be a boundary sentence. Moreover, the search space of the spans in the Pointer-network is $2n$ where $n$ is the number of sentences, because it selects only two boundary sentences. Note that the search space of Baseline1 is $n^2$. A much smaller search space might improve the accuracy by making the model consider fewer candidates.

In future work, we also plan to use multi-modal information. While our baselines only used the transcript, complementing the narratives with the visual information may improve the performance, similarly to the text alignment task in BIBREF11.
</paragraphs>
</section>

---Paper Title: TutorialVQA: Question Answering Dataset for Tutorial Videos---
<section>
<section-title>Conclusion</section-title>
<section-number>13</section-number>
<paragraphs>
We have described the collection, analysis, and baseline results of TutorialVQA, a new type of dataset used to find answer spans in tutorial videos. Our data collection method for question-answer pairs on instructional video can be further adopted to other domains where the answers involve multiple steps and are part of an overall goal, such as cooking or educational videos. We have shown that current baseline models for finding the answer spans are not sufficient for achieving high accuracy and hope that by releasing this new dataset and task, more appropriate question answering models can be developed for question answering on instructional videos.
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>Open Information Extraction from Question-Answer Pairs</title>
<abstract>Open Information Extraction (OpenIE) extracts meaningful structured tuples from free-form text. Most previous work on OpenIE considers extracting data from one sentence at a time. We describe NeurON, a system for extracting tuples from question-answer pairs. Since real questions and answers often contain precisely the information that users care about, such information is particularly desirable to extend a knowledge base with. NeurON addresses several challenges. First, an answer text is often hard to understand without knowing the question, and second, relevant information can span multiple sentences. To address these, NeurON formulates extraction as a multi-source sequence-to-sequence learning task, wherein it combines distributed representations of a question and an answer to generate knowledge facts. We describe experiments on two real-world datasets that demonstrate that NeurON can find a significant number of new and interesting facts to extend a knowledge base compared to state-of-the-art OpenIE methods.</abstract>
<sections>
<section>
<section-title>Credits</section-title>
<section-number>0</section-number>
<paragraphs>
This document has been adapted from the instructions for earlier ACL and NAACL proceedings, including those for ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu, NAACL 2018 by Margaret Michell and Stephanie Lukin, 2017/2018 (NA)ACL bibtex suggestions from Jason Eisner, ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell, ACL 2012 by Maggie Li and Michael White, those from ACL 2010 by Jing-Shing Chang and Philipp Koehn, those for ACL 2008 by JohannaD. Moore, Simone Teufel, James Allan, and Sadaoki Furui, those for ACL 2005 by Hwee Tou Ng and Kemal Oflazer, those for ACL 2002 by Eugene Charniak and Dekang Lin, and earlier ACL and EACL formats. Those versions were written by several people, including John Chen, Henry S. Thompson and Donald Walker. Additional elements were taken from the formatting instructions of the International Joint Conference on Artificial Intelligence and the Conference on Computer Vision and Pattern Recognition.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Introduction</section-title>
<section-number>1</section-number>
<paragraphs>
The following instructions are directed to authors of papers submitted to NAACL-HLT 2019 or accepted for publication in its proceedings. All authors are required to adhere to these specifications. Authors are required to provide a Portable Document Format (PDF) version of their papers. The proceedings are designed for printing on A4 paper.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>General Instructions</section-title>
<section-number>2</section-number>
<paragraphs>
Manuscripts must be in two-column format. Exceptions to the two-column format include the title, authors' names and complete addresses, which must be centered at the top of the first page, and any full-width figures or tables (see the guidelines in Subsection "The First Page" ). Type single-spaced. Start all pages directly under the top margin. See the guidelines later regarding formatting the first page. The manuscript should be printed single-sided and its length should not exceed the maximum page limit described in Section "Length of Submission" . Pages are numbered for initial submission. However, do not number the pages in the camera-ready version.

By uncommenting \aclfinalcopy at the top of this document, it will compile to produce an example of the camera-ready formatting; by leaving it commented out, the document will be anonymized for initial submission. When you first create your submission on softconf, please fill in your submitted paper ID where *** appears in the \def\aclpaperid{***} definition at the top.

The review process is double-blind, so do not include any author information (names, addresses) when submitting a paper for review. However, you should maintain space for names and addresses so that they will fit in the final (accepted) version. The NAACL-HLT 2019 style will create a titlebox space of 2.5in for you when \aclfinalcopy is commented out.

The author list for submissions should include all (and only) individuals who made substantial contributions to the work presented. Each author listed on a submission to NAACL-HLT 2019 will be notified of submissions, revisions and the final decision. No authors may be added to or removed from submissions to NAACL-HLT 2019 after the submission deadline.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>The Ruler</section-title>
<section-number>3</section-number>
<paragraphs>
The NAACL-HLT 2019 style defines a printed ruler which should be presented in the version submitted for review. The ruler is provided in order that reviewers may comment on particular lines in the paper without circumlocution. If you are preparing a document without the provided style files, please arrange for an equivalent ruler to appear on the final output pages. The presence or absence of the ruler should not change the appearance of any other content on the page. The camera ready copy should not contain a ruler. ( users may uncomment the \aclfinalcopy command in the document preamble.)

Reviewers: note that the ruler measurements do not align well with lines in the paper – this turns out to be very difficult to do well when the paper contains many figures and equations, and, when done, looks ugly. In most cases one would expect that the approximate location will be adequate, although you can also use fractional references (e.g., the first paragraph on this page ends at mark $108.5$ ).
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Electronically-available resources</section-title>
<section-number>4</section-number>
<paragraphs>
NAACL-HLT provides this description in 2e (naaclhlt2019.tex) and PDF format (naaclhlt2019.pdf), along with the 2e style file used to format it (naaclhlt2019.sty) and an ACL bibliography style (acl_natbib.bst) and example bibliography (naaclhlt2019.bib). These files are all available at http://naacl2019.org/downloads/ naaclhlt2019-latex.zip. We strongly recommend the use of these style files, which have been appropriately tailored for the NAACL-HLT 2019 proceedings.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Format of Electronic Manuscript</section-title>
<section-number>5</section-number>
<paragraphs>
For the production of the electronic manuscript you must use Adobe's Portable Document Format (PDF). PDF files are usually produced from using the pdflatex command. If your version of produces Postscript files, you can convert these into PDF using ps2pdf or dvipdf. On Windows, you can also use Adobe Distiller to generate PDF.

Please make sure that your PDF file includes all the necessary fonts (especially tree diagrams, symbols, and fonts with Asian characters). When you print or create the PDF file, there is usually an option in your printer setup to include none, all or just non-standard fonts. Please make sure that you select the option of including ALL the fonts. Before sending it, test your PDF by printing it from a computer different from the one where it was created. Moreover, some word processors may generate very large PDF files, where each page is rendered as an image. Such images may reproduce poorly. In this case, try alternative ways to obtain the PDF. One way on some systems is to install a driver for a postscript printer, send your document to the printer specifying “Output to a file”, then convert the file to PDF.

It is of utmost importance to specify the A4 format (21 cm x 29.7 cm) when formatting the paper. When working with dvips, for instance, one should specify -t a4. Or using the command \special{papersize=210mm,297mm} in the latex preamble (directly below the \usepackage commands). Then using dvipdf and/or pdflatex which would make it easier for some.

Print-outs of the PDF file on A4 paper should be identical to the hardcopy version. If you cannot meet the above requirements about the production of your electronic submission, please contact the publication chairs as soon as possible.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Layout</section-title>
<section-number>6</section-number>
<paragraphs>
Format manuscripts two columns to a page, in the manner these instructions are formatted. The exact dimensions for a page on A4 paper are:

Left and right margins: 2.5 cm

Top margin: 2.5 cm

Bottom margin: 2.5 cm

Column width: 7.7 cm

Column height: 24.7 cm

Gap between columns: 0.6 cm

Papers should not be submitted on any other paper size. If you cannot meet the above requirements about the production of your electronic submission, please contact the publication chairs above as soon as possible.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Fonts</section-title>
<section-number>7</section-number>
<paragraphs>
For reasons of uniformity, Adobe's Times Roman font should be used. In 2e this is accomplished by putting

\usepackage{times}

\usepackage{latexsym}

in the preamble. If Times Roman is unavailable, use Computer Modern Roman (2e's default). Note that the latter is about 10% less dense than Adobe's Times Roman font.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>The First Page</section-title>
<section-number>8</section-number>
<paragraphs>
Center the title, author's name(s) and affiliation(s) across both columns. Do not use footnotes for affiliations. Do not include the paper ID number assigned during the submission process. Use the two-column format only when you begin the abstract.

Title: Place the title centered at the top of the first page, in a 15-point bold font. (For a complete guide to font sizes and styles, see Table 1 ) Long titles should be typed on two lines without a blank line intervening. Approximately, put the title at 2.5 cm from the top of the page, followed by a blank line, then the author's names(s), and the affiliation on the following line. Do not use only initials for given names (middle initials are allowed). Do not format surnames in all capitals (e.g., use “Mitchell” not “MITCHELL”). Do not format title and section headings in all capitals as well except for proper names (such as “BLEU”) that are conventionally in all capitals. The affiliation should contain the author's complete address, and if possible, an electronic mail address. Start the body of the first page 7.5 cm from the top of the page.

The title, author names and addresses should be completely identical to those entered to the electronical paper submission website in order to maintain the consistency of author information among all publications of the conference. If they are different, the publication chairs may resolve the difference without consulting with you; so it is in your own interest to double-check that the information is consistent.

Abstract: Type the abstract at the beginning of the first column. The width of the abstract text should be smaller than the width of the columns for the text in the body of the paper by about 0.6 cm on each side. Center the word Abstract in a 12 point bold font above the body of the abstract. The abstract should be a concise summary of the general thesis and conclusions of the paper. It should be no longer than 200 words. The abstract text should be in 10 point font.

Text: Begin typing the main body of the text immediately after the abstract, observing the two-column format as shown in the present document. Do not include page numbers.

Indent: Indent when starting a new paragraph, about 0.4 cm. Use 11 points for text and subsection headings, 12 points for section headings and 15 points for the title.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Sections</section-title>
<section-number>9</section-number>
<paragraphs>
Headings: Type and label section and subsection headings in the style shown on the present document. Use numbered sections (Arabic numerals) in order to facilitate cross references. Number subsections with the section number and the subsection number separated by a dot, in Arabic numerals. Do not number subsubsections.

Citations: Citations within the text appear in parentheses as BIBREF0 or, if the author's name appears in the text itself, as Gusfield Gusfield:97. Using the provided style, the former is accomplished using \cite and the latter with \shortcite or \newcite. Collapse multiple citations as in BIBREF0 , BIBREF1 ; this is accomplished with the provided style using commas within the \cite command, e.g., \cite{Gusfield:97,Aho:72}. Append lowercase letters to the year in cases of ambiguities. Treat double authors as in BIBREF1 , but write as in BIBREF2 when more than two authors are involved. Collapse multiple citations as in BIBREF0 , BIBREF1 . Also refrain from using full citations as sentence constituents.

We suggest that instead of

“ BIBREF0 showed that ...”

you use

“Gusfield Gusfield:97 showed that ...”

If you are using the provided and Bib style files, you can use the command \citet (cite in text) to get “author (year)” citations.

If the Bib file contains DOI fields, the paper title in the references section will appear as a hyperlink to the DOI, using the hyperref package. To disable the hyperref package, load the style file with the nohyperref option:

\usepackage[nohyperref]{naaclhlt2019}

Digital Object Identifiers: As part of our work to make ACL materials more widely used and cited outside of our discipline, ACL has registered as a CrossRef member, as a registrant of Digital Object Identifiers (DOIs), the standard for registering permanent URNs for referencing scholarly materials. As of 2017, we are requiring all camera-ready references to contain the appropriate DOIs (or as a second resort, the hyperlinked ACL Anthology Identifier) to all cited works. Thus, please ensure that you use Bib records that contain DOI or URLs for any of the ACL materials that you reference. Appropriate records should be found for most materials in the current ACL Anthology at http://aclanthology.info/.

As examples, we cite BIBREF3 to show you how papers with a DOI will appear in the bibliography. We cite BIBREF4 to show how papers without a DOI but with an ACL Anthology Identifier will appear in the bibliography.

As reviewing will be double-blind, the submitted version of the papers should not include the authors' names and affiliations. Furthermore, self-references that reveal the author's identity, e.g.,

“We previously showed BIBREF0 ...”

should be avoided. Instead, use citations such as

“ BIBREF0 Gusfield:97 previously showed ... ”

Any preliminary non-archival versions of submitted papers should be listed in the submission form but not in the review version of the paper. NAACL-HLT 2019 reviewers are generally aware that authors may present preliminary versions of their work in other venues, but will not be provided the list of previous presentations from the submission form.

Please do not use anonymous citations and do not include when submitting your papers. Papers that do not conform to these requirements may be rejected without review.

References: Gather the full set of references together under the heading References; place the section before any Appendices. Arrange the references alphabetically by first author, rather than by order of occurrence in the text. By using a .bib file, as in this template, this will be automatically handled for you. See the \bibliography commands near the end for more.

Provide as complete a citation as possible, using a consistent format, such as the one for Computational Linguistics or the one in the Publication Manual of the American Psychological Association BIBREF5 . Use of full names for authors rather than initials is preferred. A list of abbreviations for common computer science journals can be found in the ACM Computing Reviews BIBREF6 .

The and Bib style files provided roughly fit the American Psychological Association format, allowing regular citations, short citations and multiple citations as described above.

Example citing an arxiv paper: BIBREF7 .

Example article in journal citation: BIBREF8 .

Example article in proceedings, with location: BIBREF9 .

Example article in proceedings, without location: BIBREF10 .

See corresponding .bib file for further details.

Submissions should accurately reference prior and related work, including code and data. If a piece of prior work appeared in multiple venues, the version that appeared in a refereed, archival venue should be referenced. If multiple versions of a piece of prior work exist, the one used by the authors should be referenced. Authors should not rely on automated citation indices to provide accurate references for prior and related work.

Appendices: Appendices, if any, directly follow the text and the references (but see above). Letter them in sequence and provide an informative title: Appendix A. Title of Appendix.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Footnotes</section-title>
<section-number>10</section-number>
<paragraphs>
Footnotes: Put footnotes at the bottom of the page and use 9 point font. They may be numbered or referred to by asterisks or other symbols. Footnotes should be separated from the text by a line.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Graphics</section-title>
<section-number>11</section-number>
<paragraphs>
Illustrations: Place figures, tables, and photographs in the paper near where they are first discussed, rather than at the end, if possible. Wide illustrations may run across both columns. Color illustrations are discouraged, unless you have verified that they will be understandable when printed in black ink.

Captions: Provide a caption for every illustration; number each one sequentially in the form: “Figure 1. Caption of the Figure.” “Table 1. Caption of the Table.” Type the captions of the figures and tables below the body, using 10 point text. Captions should be placed below illustrations. Captions that are one line are centered (see Table 1 ). Captions longer than one line are left-aligned (see Table 2 ). Do not overwrite the default caption sizes. The naaclhlt2019.sty file is compatible with the caption and subcaption packages; do not add optional arguments.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Accessibility</section-title>
<section-number>12</section-number>
<paragraphs>
In an effort to accommodate people who are color-blind (as well as those printing to paper), grayscale readability for all accepted papers will be encouraged. Color is not forbidden, but authors should ensure that tables and figures do not rely solely on color to convey critical distinctions. A simple criterion: All curves and points in your figures should be clearly distinguishable without color.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Translation of non-English Terms</section-title>
<section-number>13</section-number>
<paragraphs>
It is also advised to supplement non-English characters and terms with appropriate transliterations and/or translations since not all readers understand all such characters and terms. Inline transliteration or translation can be represented in the order of: original-form transliteration “translation”.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Length of Submission</section-title>
<section-number>14</section-number>
<paragraphs>
The NAACL-HLT 2019 main conference accepts submissions of long papers and short papers. Long papers may consist of up to eight (8) pages of content plus unlimited pages for references. Upon acceptance, final versions of long papers will be given one additional page – up to nine (9) pages of content plus unlimited pages for references – so that reviewers' comments can be taken into account. Short papers may consist of up to four (4) pages of content, plus unlimited pages for references. Upon acceptance, short papers will be given five (5) pages in the proceedings and unlimited pages for references. For both long and short papers, all illustrations and tables that are part of the main text must be accommodated within these page limits, observing the formatting instructions given in the present document. Papers that do not conform to the specified length and formatting requirements are subject to be rejected without review.

NAACL-HLT 2019 does encourage the submission of additional material that is relevant to the reviewers but not an integral part of the paper. There are two such types of material: appendices, which can be read, and non-readable supplementary materials, often data or code. Do not include this additional material in the same document as your main paper. Additional material must be submitted as one or more separate files, and must adhere to the same anonymity guidelines as the main paper. The paper must be self-contained: it is optional for reviewers to look at the supplementary material. Papers should not refer, for further detail, to documents, code or data resources that are not available to the reviewers. Refer to Appendix "Appendices" and Appendix "Supplemental Material" for further information.

Workshop chairs may have different rules for allowed length and whether supplemental material is welcome. As always, the respective call for papers is the authoritative source.
</paragraphs>
</section>

---Paper Title: Open Information Extraction from Question-Answer Pairs---
<section>
<section-title>Acknowledgments</section-title>
<section-number>15</section-number>
<paragraphs>
The acknowledgments should go immediately before the references. Do not number the acknowledgments section. Do not include this section when submitting your paper for review.

Preparing References:

Include your own bib file like this: \bibliographystyle{acl_natbib} \begin{thebibliography}{40} 

Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. 2015. Leveraging linguistic structure for open domain information extraction. In Proc. ACL '15/IJCNLP '15, pages 344–354.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proc. ICLR '15.

Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proc. IJCAI '07, pages 2670–2676.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proc. EMNLP '13, pages 1533–1544.

Nikita Bhutani, HV Jagadish, and Dragomir Radev. 2016. Nested propositions in open information extraction. In Proc. EMNLP '16, pages 55–64.

Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Proc. NIPS '13, pages 2787–2795.

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation. In Proc. EMNLP '14, pages 1724–1734.

Lei Cui, Furu Wei, and Ming Zhou. 2018. Neural open information extraction. In Proc. ACL '18, pages 407–413.

Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. arXiv preprint arXiv:1809.02922.

Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proc. EMNLP '11, pages 1535–1545.

Ben Hixon, Peter Clark, and Hannaneh Hajishirzi. 2015. Learning knowledge graphs for question answering through conversational dialog. In Proc. NAACL-HLT '15, pages 851–861.

Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging. arXiv preprint arXiv:1508.01991.

Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. In Proc. ACL '17, pages 963–973.

Prachi Jain, Shikhar Murty, Mausam, and Soumen Chakrabarti. 2018. Mitigating the effect of out-of-vocabulary entity pairs in matrix factorization for KB inference. In Proc. IJCAI '18, pages 4122–4129.

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. OpenNMT: Open-source toolkit for neural machine translation. In Proc. ACL '17 (System Demonstrations), pages 67–72.

Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. 2015a. Multi-task sequence to sequence learning. In Proc. ICLR '16.

Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015b. Effective approaches to attention-based neural machine translation. In Proc. EMNLP '15, pages 1412–1421.

Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using $t$ -SNE. Journal of Machine Learning Research, 9(Nov):2579–2605.

Mausam, Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, et al. 2012. Open language learning for information extraction. In Proc. EMNLP '12, pages 523–534.

Julian McAuley and Alex Yang. 2016. Addressing complex and subjective product-related queries with customer reviews. In Proc. WWW '16, pages 625–635.

Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. 2016. Holographic embeddings of knowledge graphs. In Proc. AAAI '16, pages 1955–1961.

Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proc. EMNLP '14, pages 1532–1543.

Nils Reimers and Iryna Gurevych. 2017. Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging. In Proc. EMNLP '17, pages 338–348.

Subhashree S and P Sreenivasa Kumar. 2018. Enriching domain ontologies using question-answer datasets. In Proc. CoDS-COMAD '18, pages 329–332.

Swarnadeep Saha, Harinder Pal, et al. 2017. Bootstrapping for numerical open ie. In Proc. ACL '17, pages 317–323.

Denis Savenkov, Wei-Lwun Lu, Jeff Dalton, and Eugene Agichtein. 2015. Relation extraction from community generated question-answer pairs. In Proc. NAACL-HLT '15, pages 96–102.

Gabriel Stanovsky and Ido Dagan. 2016. Creating a large benchmark for open information extraction. In Proc. EMNLP '16.

Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, and Ido Dagan. 2018. Supervised open information extraction. In Proc. ACL '18, pages 885–895.

Antonio Toral and Víctor M. Sánchez-Cartagena. 2017. A multifaceted evaluation of neural versus phrase-based machine translation for 9 language directions. In Proc. EACL '17, pages 1063–1073.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Proc. NIPS '15, pages 2692–2700.

Mengting Wan and Julian McAuley. 2016. Modeling ambiguity, subjectivity, and diverging viewpoints in opinion question answering systems. In Proc. ICDM '16, pages 489–498.

Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724–2743.

Zeqiu Wu, Xiang Ren, Frank F. Xu, Ji Li, and Jiawei Han. 2018. Indirect supervision for relation extraction using question-answer pairs. In Proc. WSDM '18, pages 646–654.

Chunyang Xiao, Marc Dymetman, and Claire Gardent. 2016. Sequence-based structured prediction for semantic parsing. In Proc. ACL '16, pages 1341–1350.

Caiming Xiong, Victor Zhong, and Richard Socher. 2017. Dynamic coattention networks for question answering. In Proc. ICLR '17.

Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland. 2007. TextRunner: Open information extraction on the web. In Proc. NAACL-HLT '07 (Demonstrations), pages 25–26.

Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. In Proc. ACL '17, pages 440–450.

Biao Zhang, Deyi Xiong, and Jinsong Su. 2016. Cseq2seq: Cyclic sequence-to-sequence learning. arXiv preprint arXiv:1607.08725.

Yaoyuan Zhang, Zhenxu Ye, Yansong Feng, Dongyan Zhao, and Rui Yan. 2017. A constrained sequence-to-sequence neural model for sentence simplification. arXiv preprint arXiv:1704.02312.

Barret Zoph and Kevin Knight. 2016. Multi-source neural translation. In Proc. NAACL-HLT '16, pages 30–34.

|

where naaclhlt2019 corresponds to a naaclhlt2019.bib file. Appendices Appendices are material that can be read, and include lemmas, formulas, proofs, and tables that are not critical to the reading and understanding of the paper. Appendices should be uploaded as supplementary material when submitting the paper for review. Upon acceptance, the appendices come after the references, as shown here. Use \appendix before any appendix section to switch the section numbering over to letters. Supplemental Material Submissions may include non-readable supplementary material used in the work and described in the paper. Any accompanying software and/or data should include licenses and documentation of research review as appropriate. Supplementary material may report preprocessing decisions, model parameters, and other details necessary for the replication of the experiments reported in the paper. Seemingly small preprocessing decisions can sometimes make a large difference in performance, so it is crucial to record such decisions to precisely characterize state-of-the-art methods. Nonetheless, supplementary material should be supplementary (rather than central) to the paper. Submissions that misuse the supplementary material may be rejected without review. Supplementary material may include explanations or details of proofs or derivations that do not fit into the paper, lists of features or feature templates, sample inputs and outputs for a system, pseudo-code or source code, and data. (Source code and data should be separate uploads, rather than part of the paper). The paper should not rely on the supplementary material: while the paper may refer to and cite the supplementary material and the supplementary material will be available to the reviewers, they will not be asked to review the supplementary material. 
</paragraphs>
</section>

</sections>

</paper>


<paper>
<title>VQABQ: Visual Question Answering by Basic Questions</title>
<abstract>Taking an image and question as the input of our method, it can output the text-based answer of the query question about the given image, so called Visual Question Answering (VQA). There are two main modules in our algorithm. Given a natural language question about an image, the first module takes the question as input and then outputs the basic questions of the main given question. The second module takes the main question, image and these basic questions as input and then outputs the text-based answer of the main question. We formulate the basic questions generation problem as a LASSO optimization problem, and also propose a criterion about how to exploit these basic questions to help answer main question. Our method is evaluated on the challenging VQA dataset and yields state-of-the-art accuracy, 60.34% in open-ended task.</abstract>
<sections>
<section>
<section-title>Introduction</section-title>
<section-number>0</section-number>
<paragraphs>
Visual Question Answering (VQA) is a challenging and young research field, which can help machines achieve one of the ultimate goals in computer vision, holistic scene understanding BIBREF1 . VQA is a computer vision task: a system is given an arbitrary text-based question about an image, and then it should output the text-based answer of the given question about the image. The given question may contain many sub-problems in computer vision, e.g.,

Besides, in our real life there are a lot of more complicated questions that can be queried. So, in some sense, VQA can be considered as an important basic research problem in computer vision. From the above sub-problems in computer vision, we can discover that if we want to do holistic scene understanding in one step, it is probably too difficult. So, we try to divide the holistic scene understanding-task into many sub-tasks in computer vision. The task-dividing concept inspires us to do Visual Question Answering by Basic Questions (VQABQ), illustrated by Figure 1 . That means, in VQA, we can divide the query question into some basic questions, and then exploit these basic questions to help us answer the main query question. Since 2014, there has been a lot of progress in designing systems with the VQA ability BIBREF2 , BIBREF0 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . Regarding these works, we can consider most of them as visual-attention VQA works because most of them do much effort on dealing with the image part but not the text part. However, recently there are some works BIBREF7 , BIBREF8 that try to do more effort on the question part. In BIBREF8 , authors proposed a Question Representation Update (QRU) mechanism to update the original query question to increase the accuracy of the VQA algorithm. Typically, VQA is a strongly image-question dependent issue, so we should pay equal attention to both the image and question, not only one of them. In reality, when people have an image and a given question about the image, we usually notice the keywords of the question and then try to focus on some parts of the image related to question to give the answer. So, paying equal attention to both parts is a more reasonable way to do VQA. In BIBREF7 , the authors proposed a Co-Attention mechanism, jointly utilizing information about visual and question attention, for VQA and achieved the state-of-the-art accuracy.

The Co-Attention mechanism inspires us to build part of our VQABQ model, illustrated by Figure 2 . In the VQABQ model, there are two main modules, the basic question generation module (Module 1) and co-attention visual question answering module (Module 2). We take the query question, called the main question (MQ), encoded by Skip-Thought Vectors BIBREF9 , as the input of Module 1. In the Module 1, we encode all of the questions, also by Skip-Thought Vectors, from the training and validation sets of VQA BIBREF0 dataset as a 4800 by 215623 dimension basic question (BQ) matrix, and then solve the LASSO optimization problem, with MQ, to find the 3 BQ of MQ. These BQ are the output of Module 1. Moreover, we take the MQ, BQ and the given image as the input of Module 2, the VQA module with co-attention mechanism, and then it can output the final answer of MQ. We claim that the BQ can help Module 2 get the correct answer to increase the VQA accuracy. In this work, our main contributions are summarized below:

The rest of this paper is organized as the following. We first talk about the motivation about this work in Section 2. In Section 3, we review the related work, and then Section 4 shortly introduces the proposed VQABQ dataset. We discuss the detailed methodology in Section 5. Finally, the experimental results are demonstrated in Section 6.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Motivations</section-title>
<section-number>1</section-number>
<paragraphs>
The following two important reasons motivate us to do Visual Question Answering by Basic Questions (VQABQ). First, recently most of VQA works only emphasize more on the image part, the visual features, but put less effort on the question part, the text features. However, image and question features both are important for VQA. If we only focus on one of them, we probably cannot get the good performance of VQA in the near future. Therefore, we should put our effort more on both of them at the same time. In BIBREF7 , they proposed a novel co-attention mechanism that jointly performs image-guided question attention and question-guided image attention for VQA. BIBREF7 also proposed a hierarchical architecture to represent the question, and construct image-question co-attention maps at the word level, phrase level and question level. Then, these co-attended features are combined with word level, phrase level and question level recursively for predicting the final answer of the query question based on the input image. BIBREF8 is also a recent work focusing on the text-based question part, text feature. In BIBREF8 , they presented a reasoning network to update the question representation iteratively after the question interacts with image content each time. Both of BIBREF7 , BIBREF8 yield better performance than previous works by doing more effort on the question part.

Secondly, in our life , when people try to solve a difficult problem, they usually try to divide this problem into some small basic problems which are usually easier than the original problem. So, why don't we apply this dividing concept to the input question of VQA ? If we can divide the input main question into some basic questions, then it will help the current VQA algorithm achieve higher probability to get the correct answer of the main question.

Thus, our goal in this paper is trying to generate the basic questions of the input question and then exploit these questions with the given image to help the VQA algorithm get the correct answer of the input question. Note that we can consider the generated basic questions as the extra useful information to VQA algorithm.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Related Work</section-title>
<section-number>2</section-number>
<paragraphs>
Recently, there are many papers BIBREF0 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 have proposed methods to solve the VQA issue. Our method involves in different areas in machine learning, natural language processing (NLP) and computer vision. The following, we discuss recent works related to our approach for solving VQA problem.

Sequence modeling by Recurrent Neural Networks.

Recurrent Neural Networks (RNN) can handle the sequences of flexible length. Long Short Term Memory (LSTM) BIBREF17 is a particular variant of RNN and in natural language tasks, such as machine translation BIBREF18 , BIBREF19 , LSTM is a successful application. In BIBREF14 , the authors exploit RNN and Convolutional Neural Network (CNN) to build a question generation algorithm, but the generated question sometimes has invalid grammar. The input in BIBREF3 is the concatenation of each word embedding with the same feature vector of image. BIBREF6 encodes the input question sentence by LSTM and join the image feature to the final output. BIBREF13 groups the neighbouring word and image features by doing convolution. In BIBREF20 , the question is encoded by Gated Recurrent Unit (GRU) BIBREF21 similar to LSTM and the authors also introduce a dynamic parameter layer in CNN whose weights are adaptively predicted by the encoded question feature.

Sentence encoding.

In order to analyze the relationship among words, phrases and sentences, several works, such as BIBREF22 , BIBREF9 , BIBREF23 , proposed methods about how to map text into vector space. After we have the vector representation of text, we can exploit the vector analysis skill to analyze the relationship among text. BIBREF22 , BIBREF23 try to map words to vector space, and if the words share common contexts in the corpus, their encoded vectors will close to each other in the vector space. In BIBREF9 , the authors propose a framework of encoder-decoder models, called skip-thoughts. In this model, the authors exploit an RNN encoder with GRU activations BIBREF21 and an RNN decoder with a conditional GRU BIBREF21 . Because skip-thoughts model emphasizes more on whole sentence encoding, in our work, we encode the whole question sentences into vector space by skip-thoughts model and use these skip-thought vectors to do further analysis of question sentences.

Image captioning.

In some sense, VQA is related to image captioning BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 . BIBREF27 uses a language model to combine a set of possible words detected in several regions of the image and generate image description. In BIBREF26 , the authors use CNN to extract the high-level image features and considered them as the first input of the recurrent network to generate the caption of image. BIBREF24 proposes an algorithm to generate one word at a time by paying attention to local image regions related to the currently predicted word. In BIBREF25 , the deep neural network can learn to embed language and visual information into a common multi-modal space. However, the current image captioning algorithms only can generate the rough description of image and there is no so called proper metric to evaluate the quality of image caption , even though BLEU BIBREF28 can be used to evaluate the image caption.

Attention-based VQA.

There are several VQA models have ability to focus on specific image regions related to the input question by integrating the image attention mechanism BIBREF10 , BIBREF11 , BIBREF29 , BIBREF8 . In BIBREF8 , in the pooling step, the authors exploit an image attention mechanism to help determine the relevance between original questions and updated ones. Before BIBREF7 , no work applied language attention mechanism to VQA, but the researchers in NLP they had modeled language attention. In BIBREF7 , the authors propose a co-attention mechanism that jointly performs language attention and image attention. Because both question and image information are important in VQA, in our work we introduce co-attention mechanism into our VQABQ model.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Basic Question Dataset</section-title>
<section-number>3</section-number>
<paragraphs>
We propose a new dataset, called Basic Question Dataset (BQD), generated by our basic question generation algorithm. BQD is the first basic question dataset. Regarding the BQD, the dataset format is $\lbrace Image,~MQ,~3~(BQ + corresponding~similarity~score)\rbrace $ . All of our images are from the testing images of MS COCO dataset BIBREF30 , the MQ, main questions, are from the testing questions of VQA, open-ended, dataset BIBREF0 , the BQ, basic questions, are from the training and validation questions of VQA, open-ended, dataset BIBREF0 , and the corresponding similarity score of BQ is generated by our basic question generation method, referring to Section 5. Moreover, we also take the multiple-choice questions in VQA dataset BIBREF0 to do the same thing as above. Note that we remove the repeated questions in the VQA dataset, so the total number of questions is slightly less than VQA dataset BIBREF0 . In BQD, we have 81434 images, 244302 MQ and 732906 (BQ + corresponding similarity score). At the same time, we also exploit BQD to do VQA and achieve the competitive accuracy compared to state-of-the-art.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Methodology</section-title>
<section-number>4</section-number>
<paragraphs>
In Section 5, we mainly discuss how to encode questions and generate BQ and why we exploit the Co-Attention Mechanism VQA algorithm BIBREF7 to answer the query question. The overall architecture of our VQABQ model can be referred to Figure 2 . The model has two main parts, Module 1 and Module 2. Regarding Module 1, it takes the encoded MQ as input and uses the matrix of the encoded BQ to output the BQ of query question. Then, the Module 2 is a VQA algorithm with the Co-Attention Mechanism BIBREF7 , and it takes the output of Module 1, MQ, and the given image as input and then outputs the final answer of MQ. The detailed architecture of Module 1 can be referred to Figure 2 .
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Question encoding</section-title>
<section-number>5</section-number>
<paragraphs>
There are many popular text encoders, such as Word2Vec BIBREF23 , GloVe BIBREF22 and Skip-Thoughts BIBREF9 . In these encoders, Skip-Thoughts not only can focus on the word-to-word meaning but also the whole sentence semantic meaning. So, we choose Skip-Thoughts to be our question encoding method. In Skip-Thoughts model, it uses an RNN encoder with GRU BIBREF21 activations, and then we use this encoder to map an English sentence into a vector. Regarding GRU, it has been shown to perform as well as LSTM BIBREF17 on the sequence modeling applications but being conceptually simpler because GRU units only have 2 gates and do not need the use of a cell.

Question encoder. Let $w_{i}^{1},...,w_{i}^{N}$ be the words in question $s_{i}$ and N is the total number of words in $s_{i}$ . Note that $w_{i}^{t}$ denotes the $t$ -th word for $s_{i}$ and $\mathbf {x}_{i}^t$ denotes its word embedding. The question encoder at each time step generates a hidden state $\mathbf {h}_{i}^{t}$ . It can be considered as the representation of the sequence $w_{i}^{1},..., w_{i}^{t}$ . So, the hidden state $\mathbf {h}_{i}^{N}$ can represent the whole question. For convenience, here we drop the index $s_{i}$0 and iterate the following sequential equations to encode a question: 

$$\mathbf {r}^{t}~=~\sigma (\mathbf {U}_{r}\mathbf {h}^{t-1}+\mathbf {W}_{r}\mathbf {x}^{t})$$   (Eq. 12) 

$$\mathbf {z}^{t}~=~\sigma (\mathbf {U}_{z}\mathbf {h}^{t-1}+\mathbf {W}_{z}\mathbf {x}^{t})$$   (Eq. 13) 

, where $\mathbf {U}_{r}$ , $\mathbf {U}_{z}$ , $\mathbf {W}_{r}$ , $\mathbf {W}_{z}$ , $\mathbf {U}$ and $\mathbf {W}$ are the matrices of weight parameters. $\bar{\mathbf {h}}^{t}$ is the state update at time step $t$ , $\mathbf {r}^{t}$ is the reset gate, $\odot $ denotes an element-wise product and $\mathbf {U}_{z}$0 is the update gate. These two update gates take the values between zero and one.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Problem Formulation</section-title>
<section-number>6</section-number>
<paragraphs>
Our idea is the BQ generation for MQ and, at the same time, we only want the minimum number of BQ to represent the MQ, so modeling our problem as $LASSO$ optimization problem is an appropriate way: 

$$\min _{\mathbf {x}}~\frac{1}{2}\left\Vert  A\mathbf {x}-\mathbf {b} \right\Vert _{2}^{2}+\lambda \left\Vert  \mathbf {x} \right\Vert _{1}$$   (Eq. 17) 

, where $A$ is the matrix of encoded BQ, $\mathbf {b}$ is the encode MQ and $\lambda $ is a parameter of the regularization term.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Basic Question Generation</section-title>
<section-number>7</section-number>
<paragraphs>
We now describe how to generate the BQ of a query question, illustrated by Figure 2 . Note that the following we only describe the open-ended question case because the multiple-choice case is same as open-ended one. According to Section 5.2, we can encode the all questions from the training and validation questions of VQA dataset BIBREF0 by Skip-Thought Vectors, and then we have the matrix of these encoded basic questions. Each column of the matrix is the vector representation, 4800 by 1 dimensions, of a basic question and we have 215623 columns. That is, the dimension of BQ matrix, called $A$ , is 4800 by 215623. Also, we encode the query question as a column vector, 4800 by 1 dimensions, by Skip-Thought Vectors, called $\mathbf {b}$ . Now, we can solve the $LASSO$ optimization problem, mentioned in Section 5.3, to get the solution, $\mathbf {x}$ . Here, we consider the elements, in solution vector $\mathbf {x}$ , as the weights of the corresponding BQ in BQ matrix, $A$ . The first element of $\mathbf {x}$ corresponds to the first column, i.e. the first BQ, of $A$ . Then, we rank the all weights in $\mathbf {x}$ and pick up the top 3 large weights with corresponding BQ to be the BQ of the query question. Intuitively, because BQ are important to MQ, the weights of BQ also can be considered as importance scores and the BQ with larger weight means more important to MQ. Finally, we find the BQ of all 142093 testing questions from VQA dataset and collect them together, with the format $\lbrace Image,~MQ,~3~(BQ + corresponding~ similarity~score)\rbrace $ , as the BQD in Section 4.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Basic Question Concatenation</section-title>
<section-number>8</section-number>
<paragraphs>
In this section, we propose a criterion to use these BQ. In BQD, each MQ has three corresponding BQ with scores. We can have the following format, $\lbrace MQ,(BQ1,~score1),(BQ2,~score2),(BQ3,~score3)\rbrace $ , and these scores are all between 0 and 1 with the following order, 

$$score1\ge score2\ge score3$$   (Eq. 20) 

and we define 3 thresholds, $s1$ , $s2$ and $s3$ . Also, we compute the following 3 averages ( $avg$ ) and 3 standard deviations ( $std$ ) to $score1$ , $score2/score1$ and $score3/score2$ , respectively, and then use $avg \pm std$ , referring to Table 3 , to be the initial guess of proper thresholds. The BQ utilization process can be explained as Table 1 . The detailed discussion about BQ concatenation algorithm is described in the Section 6.4.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Co-Attention Mechanism</section-title>
<section-number>9</section-number>
<paragraphs>
There are two types of Co-Attention Mechanism BIBREF7 , Parallel and Alternating. In our VQABQ model, we only use the VQA algorithm with Alternating Co-Attention Mechanism to be our VQA module, referring to Figure 2 , because, in BIBREF7 , Alternating Co-Attention Mechanism VQA module can get the higher accuracy than the Parallel one. Moreover, we want to compare with the VQA method, Alternating one, with higher accuracy in BIBREF7 . In Alternating Co-Attention Mechanism, it sequentially alternates between generating question and image attention. That is, this mechanism consists of three main steps:

First, the input question is summarized into a single vector $\mathbf {q}$ .

Second, attend to the given image depended on $\mathbf {q}$ .

Third, attend to the question depended on the attended image feature.

We can define $\hat{\mathbf {x}}$ is an attention operator, which is a function of $\mathbf {X}$ and $\mathbf {g}$ . This operator takes the question (or image) feature $\mathbf {X}$ and attention guider $\mathbf {g}$ derived from image (or question) as inputs, and then outputs the attended question (or image) vector. We can explain the above operation as the following steps: 

$$\mathbf {H}~=~\rm {tanh}(\mathbf {W}_{x}\mathbf {X}+(\mathbf {W}_{g}g)\mathbf {1}^{T})$$   (Eq. 26) 

$$\mathbf {a}^{x}~=~\rm {softmax}(\mathbf {w}_{hx}^{T}\mathbf {H})$$   (Eq. 27) 

, where $\mathbf {a}^{x}$ is the attention weight of feature $\mathbf {X}$ , $\mathbf {1}$ is a vector whose elements are all equal to 1, and $\mathbf {W}_{g}$ , $\mathbf {W}_{x}$ and $\mathbf {w}_{hx}$ are matrices of parameters.

Concretely, at the first step of Alternating Co-Attention Mechanism, $\mathbf {g}$ is 0 and $\mathbf {X} = \mathbf {Q}$ . Then, at the second step, $\mathbf {X} = \mathbf {V}$ where $\mathbf {V}$ is the image features and the guider, $\mathbf {g}$ , is intermediate attended question feature, $\hat{s}$ , which is from the first step. At the final step, it uses the attended image feature, $\hat{v}$ , as the guider to attend the question again. That is, $\mathbf {X} = \mathbf {Q}$ and $\mathbf {g} = \hat{v}$ .
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Experiment</section-title>
<section-number>10</section-number>
<paragraphs>
In Section 6, we describe the details of our implementation and discuss the experiment results about the proposed method.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Datasets</section-title>
<section-number>11</section-number>
<paragraphs>
We conduct our experiments on VQA BIBREF0 dataset. VQA dataset is based on the MS COCO dataset BIBREF30 and it contains the largest number of questions. There are questions, 248349 for training, 121512 for validation and 244302 for testing. In the VQA dataset, each question is associated with 10 answers annotated by different people from Amazon Mechanical Turk (AMT). About 98% of answers do not exceed 3 words and 90% of answers have single words. Note that we only test our method on the open-ended case in VQA dataset because it has the most open-ended questions among the all available dataset and we also think open-ended task is closer to the real situation than multiple-choice one.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Setup</section-title>
<section-number>12</section-number>
<paragraphs>
In order to prove our claim that BQ can help accuracy and compare with the state-of-the-art VQA method BIBREF7 , so, in our Module 2, we use the same setting, dataset and source code mentioned in BIBREF7 . Then, the Module 1 in VQABQ model, is our basic question generation module. In other words, in our model ,the only difference compared to BIBREF7 is our Module 1, illustrated by Figure 2 .
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Evaluation Metrics</section-title>
<section-number>13</section-number>
<paragraphs>
VQA dataset provides multiple-choice and open-ended task for evaluation. Regarding open-ended task, the answer can be any phrase or word. However, in multiple-choice task, an answer should be chosen from 18 candidate answers. For both cases, answers are evaluated by accuracy which can reflect human consensus. The accuracy is given by the following: 

$$Accuracy_{_{VQA}}=\frac{1}{N}\sum _{i=1}^{N}\min \left\lbrace  \frac{\sum _{t\in T_{i}}\mathbb {I}[a_{i}=t]}{3},1 \right\rbrace $$   (Eq. 36) 

, where $N$ is the total number of examples, $\mathbb {I}[\cdot ]$ denotes an indicator function, $a_{i}$ is the predicted answer and $T_{i}$ is an answer set of the $i^{th}$ example. That is, a predicted answer is considered as a correct one if at least 3 annotators agree with it, and the score depends on the total number of agreements when the predicted answer is not correct.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Results and Analysis</section-title>
<section-number>14</section-number>
<paragraphs>
Here, we describe our final results and analysis by the following parts:

Does Basic Question Help Accuracy ?

The answer is yes. Here we only discuss the open-ended case. In our experiment, we use the $avg\pm std$ , referring to Table 3 , to be the initial guess of proper thresholds of s1, s2 and s3, in Table 1 . We discover that when s1 = 0.43, s2 = 0.82 and s3 = 0.53, we can get the better utilization of BQ. The threshold, s1 = 0.43, can be consider as 43% of testing questions from VQA dataset which cannot find the basic question, from the training and validation sets of VQA dataset, and only 57% of testing questions can find the basic questions. Note that we combine the training and validation sets of VQA dataset to be our basic question dataset. Regarding s2 = 0.82, that means 82% of those 57% testing questions, i.e. 46.74%, only can find 1 basic question, and 18% of those 57% testing questions, i.e. 10.26%, can find at least 2 basic questions. Furthermore, s3 = 0.53 means that 53% of those 10.26% testing question, i.e. around 5.44%, only can find 2 basic questions, and 47% of those 10.26% testing question, i.e. around 4.82%, can find 3 basic questions. The above detail can be referred to Table 2 .

Accordingly to the Table 2 , 43% of testing questions from VQA dataset cannot find the proper basic questions from VQA training and validation datasets, and there are some failed examples about this case in Table 6 . We also discover that a lot of questions in VQA training and validation datasets are almost the same. This issue reduces the diversity of basic question dataset. Although we only have 57% of testing questions can benefit from the basic questions, our method still can improve the state-of-the-art accuracy BIBREF7 from 60.32% to 60.34%, referring to Table 4 and 5 . Then, we have 142093 testing questions, so that means the number of correctly answering questions of our method is more than state-of-the-art method 28 questions. In other words, if we have well enough basic question dataset, we can increase accuracy more, especially in the counting-type question, referring to Table 4 and 5 . Because the Co-Attention Mechanism is good at localizing, the counting-type question is improved more than others. So, based on our experiment, we can conclude that basic question can help accuracy obviously.

Comparison with State-of-the-art.

Recently, BIBREF7 proposed the Co-Attention Mechanism in VQA and got the state-of-the-art accuracy. However, when we use their code and the same setup mentioned in their paper to re-run the experiment, we cannot get the same accuracy reported in their work. The re-run results are presented in Table 5 . So, under the fair conditions, our method is competitive compared to the state-of-the-art.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Conclusion and Future Work</section-title>
<section-number>15</section-number>
<paragraphs>
In this paper, we propose a VQABQ model for visual question answering. The VQABQ model has two main modules, Basic Question Generation Module and Co-Attention VQA Module. The former one can generate the basic questions for the query question, and the latter one can take the image , basic and query question as input and then output the text-based answer of the query question. According to the Section 6.4, because the basic question dataset generated from VQA dataset is not well enough, we only have the 57% of all testing questions can benefit from the basic questions. However, we still can increase 28 correctly answering questions compared to the state-of-the-art. We believe that if our basic question dataset is well enough, the increment of accuracy will be much more.

According to the previous state-of-the-art methods in VQA, they all got the highest accuracy in the Yes/No-type question. So, how to effectively only exploit the Yes/No-type basic questions to do VQA will be an interesting work, illustrated by Figure 3 . Also, how to generate other specific type of basic questions based on the query question and how to do better combination of visual and textual features in order to decrease the semantic inconsistency? The above future works will be our next research focus.
</paragraphs>
</section>

---Paper Title: VQABQ: Visual Question Answering by Basic Questions---
<section>
<section-title>Acknowledgements</section-title>
<section-number>16</section-number>
<paragraphs>
This work is supported by competitive research funding from King Abdullah University of Science and Technology (KAUST). Also, we would like to acknowledge Fabian Caba, Humam Alwassel and Adel Bibi. They always can provide us helpful discussion about this work.
</paragraphs>
</section>

</sections>

</paper>
